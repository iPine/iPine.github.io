<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[SQL面试题]]></title>
    <url>%2F2019-08-14%2F</url>
    <content type="text"><![CDATA[关于SQL的几个面试题或练习题，在解题的时候，同时考虑用pandas来解，所以做个总结。 题目一 题目描述 用户日志表log，有用户编号cid，时间dt两个字段，查找每个用户成为新增用户的时间 HiveSQL解 思路： 对每个用户的出现时间进行排名 从排名中挑选出每个用户的排名为1的时间，即首次出现时间，也就是成为新增用户的时间 12345678select a.cid, a.dtfrom (select cid, dt, row_number() over(partition by cid order by dt) as rank from log)awhere a.rank=1; Python解 现在需要新增加一列is_new，判断当天该用户是否是新用户，若是，则对应is_new的值设为1，否则设为0。 思路： 统计每个用户出现的最早时间（成为新用户时间），并用dict(zip())构造一个 用户ID：最早时间 字典 将原df_log每个cid的dt与字典的dt比较，若相同就把相应行的is_new设为1，否则为0 具体实现： 构造log数据框 123cid=[1,2,2,3,4,3,5]dt=['2019-08-01','2019-08-01','2019-08-02','2019-08-02','2019-08-03','2019-08-04','2019-08-01']df_log=pd.DataFrame(&#123;'cid':cid,'dt':dt&#125;) log数据框： 统计每个用户成为新增用户时间，按cid分组，求最早时间 1minTime = df_log.groupby('cid').dt.min().tolist() 统计独立用户 1cid = set(df_log['cid'].tolist()) 构造字典：用户: 成为新增用户的时间 1cid_mintime_dict = dict(zip(cid,minTime)) 将df_log每行的dt与字典的dt比较，若相同，就把新增的is_new列设置为1，否则为0 12345678def match(df): if df['dt']==cid_mintime_dict[df['cid']]: return 1 else: return 0df_log['is_new']=df_log.apply(lambda x:match(x),axis=1)df_log 结果： 题目二 题目描述 有两张表，t1和t2；t1有用户编号uid和页面浏览量pageviews两个字段；t2有用户编号uid和用户所在组别groupid两个字段；查询每组用户的总阅读量 两张表为： HiveSQL解 说明这是列转行问题，用函数lateral view explode拆分字段的多个值。 思路： 先把t2表的groupid拆分开，将得到的结果重命名 以新表为主，左连接t1表，对groupid分组，统计每组pageviews的和 12345678select b.groupid, sum(b.pageviews) as total_pageviewsfrom ((select t2.uid as uid,t3.groupid as groupid from t2 lateral view explode(groupid) t3 as groupid)a left join t1 on a.uid=t1.uid)bgroup by b.groupid; Python解 思路： 先将groupid列按属性值拆分成多行 合并两张表，对合并后的表按照groupid进行分组，统计pageviews的和 具体实现： 说明：data_t1表示t1的数据框， data_t2表示t2的数据框 先拆分groupid，并重命名为group 1newGroup = data_t2['groupid'].str.split(',', expand=True).stack().reset_index(level=1,drop=True).rename('group') newGroup的结果为： 把newGroup合并到data_t2上，得到新的data_t2 12data_t2 = data_t2.drop('groupid', axis=1).join(newGroup)data_t2.head() 结果为： 合并新的data_t2和data_t1，以data_t2为主 12df_combined = pd.merge(data_t1,data_t2,on='uid',how='right')df_combined.head() 结果为： 按照group分组，统计总浏览量 1df_combined.groupby('group')['pageviews'].sum() 最终结果： 题目三 题目描述 浏览量表V，有用户编号cid，时间dt，浏览量visits字段； 选出连续3天或以上浏览量大于10的用户。 HiveSQL解 巧妙利用排序，把日期进行从小到大排序，用日期(天)本身减去日期的排名，若日期所在行的差值相同说明这些日期是连续的。 思路： 子查询从浏览量大于10的数据中选出用户编号cid、日期(天)与日期排名差值diff 外层查询选择cid，按照用户编号cid和diff分组，having过滤出计数大于等于3的 123456789select a.cid, a.difffrom (select cid, (day(dt)-row_number() over(order by dt)) as diff from V where visits&gt;10)agroup by a.cid,a.diffhaving count(1)&gt;=3]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>data_analysis</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬猫眼TOP100电影之正则表达式]]></title>
    <url>%2F2019-07-07%2F</url>
    <content type="text"><![CDATA[背景 之前写过一个爬虫，爬取比特币官网的交易数据，用的是网页解析库(Python的BeautifulSoup)解析页面内容。最近学习了基础的正则表达式，所以实践一波用正则解析HTML，爬取猫眼榜单上TOP100榜的电影，包括电影排名、电影名、主演、上映时间以及评分信息。 爬虫基础回顾 基本流程 发起请求。模拟浏览器发送一个request，请求可以包含额外的头信息，等待服务器响应。 说明： 在浏览器输入地址，回车，就是一个请求。请求方式主要有get和post两种类型；请求的URL，统一资源定位符，网页的一张图片，一个视频或者是文档都由URL确定；请求头，包含Cookies,host，User-Agent等信息；请求体，额外携带的一些信息，如表单提交的表单数据。 获取响应内容。正常情况下服务器返回一个response，其中包含要爬取的页面内容信息，形式可以是HTML，Json或者图片视频类的二进制数据。 说明： 服务器根据请求，返回数据到浏览器显示，就是一个响应。响应状态，有多种，200是OK、301重定向、404找不到、502服务器错误等；响应头，包含内容类型、内容长度、服务器信息等；响应体，主要是包含请求的资源内容，如图片、视频等。 解析内容。对HTML可用正则表达式或者网页解析库解析，对Json可以转为Json对象解析，对二进制数据，做保存或者进一步处理。 说明： 解析方式有多种，对于解析库的选择可根据爬取的网页具体情况具体选择。常见的解析方式除了正则外，还有一些解析库，如BeautifulSoup、XPath、PyQuery等。 保存数据。形式很多，可存文本，特定格式的文件，也可以保存至数据库。 注：有时爬取到的内容都是HTML，CSS和JavaScript代码，想要的数据不在抓取的内容中，这种动态网页的情况，待之后遇到再记录。 准备工作 分析要爬取电影的页面信息，获取到数据来源（正确的页面链接） 如下图所示，猫眼排行榜TOP100榜的页面信息： 在要获取的电影信息上面，【鼠标右击】，点击【检查】，进入页面分析模式，切到【Network】，点击【Name】下的链接，在【Response】下确认页面结构中是否出现所需信息，若没出现就换下一个链接，直到找到正确的链接，然后切到【Headers】，找到该页面的链接，请求方式等信息。 定位要获取的信息具体在页面结构中的哪个位置 鼠标放在要找元素上方【右击】，点击【检查】，切到【Element】，即可定位到它所在的标签，发现要获取的每条数据都在&lt;dd&gt;&lt;/dd&gt;标签内。 分析不同页面的URL之间的关系 点击页面翻页按钮，分析页面URL之间的变化关系，发现URL只有最后的offset在改变，并且每次递增10，排行榜第一页offset为0，第二页offset为10，以此类推。 代码 先导入所需模块 123import requestsimport reimport pandas as pd 面向对象编程，定义一个爬虫类，里面的每一个类方法就是爬虫的4个基本流程之一。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253class MYSpider(object): ''' 定义一个爬虫类 ''' def __init__(self,base_url,url_headers): self.base_url = base_url self.headers = url_headers def get_onePage(self,start_num): ''' 获取第一页内容 ''' url = self.base_url.format(start_num) response = requests.get(url=url,headers=self.headers) if response.status_code == 200: return response.content.decode('utf-8') else: return None def parse_onePage(self,html): ''' 解析数据 ''' #正则获取排名、电影名、主演、上映时间、评分 #用re的compile()模块创建模式对象 pattern = re.compile('&lt;dd&gt;.*?board-index.*?&gt;(\d+).*?movie-item-info.*?&gt;.*?&lt;a' +'.*?title="(.*?)".*?star.*?&gt;\s+主演：(.*?)\s+&lt;/p&gt;.*?releasetime.*?&gt;上映时间：(.*?)&lt;/p&gt;' +'.*?integer.*?&gt;(\d+\.)&lt;/i&gt;.*?fraction.*?&gt;(\d+)&lt;/i&gt;.*?&lt;/dd&gt;',re.S) result = re.findall(pattern,html) #print(result) return result def save_data(self,data): ''' 保存数据 ''' rank=[];name=[];star=[];time=[];score= [] for value in data: rank.append(value[0]) name.append(value[1]) star.append(value[2]) time.append(value[3].strip()[:10]) score.append(value[4]+value[5]) rank_col = pd.Series(rank) name_col = pd.Series(name) star_col = pd.Series(star) time_col = pd.Series(time) score_col = pd.Series(score) movie_data = pd.concat([rank_col,name_col,star_col,time_col,score_col],axis=1) #将数据通过pandas的to_csv方法写入到csv文件 movie_data.to_csv('./moviedata.csv',mode='a',index=False,header=False,encoding='gbk') 调用爬虫类的方法，完成10个页面的请求、解析和保存工作。这里每个页面URL之间的关系通过前面的准备工作分析得出，即offset每次递增10。 请求头headers的内容是相对固定的，是模拟不同的浏览器对服务器发起请求。 12345678if __name__ == "__main__": base_url = 'https://maoyan.com/board/4?offset=&#123;&#125;' headers = &#123;"User-Agent":'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36(KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36'&#125; myspider = MYSpider(base_url,headers) for i in range(0,100,10): html = myspider.get_onePage(i) result_data = myspider.parse_onePage(html) myspider.save_data(result_data) 得到的数据存储在moviedata.csv文件中，查看爬取结果，如下：]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>data_analysis</tag>
        <tag>regex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推论统计实践之验证斯特鲁普效应]]></title>
    <url>%2F2019-06-02%2F</url>
    <content type="text"><![CDATA[背景 斯特鲁普效应是以美国心理学家约翰·里德利·斯特鲁普的名字命名的一种心理现象，在心理学中，该效应是对任务反应时间的干扰。当词的信息（词义）与写词色彩不一致时，便会出现心理紧张与自动反应之间的矛盾，于是造成参与者的反应时间延长。一个典型的例子–交互式斯特鲁普效应实验，可以很好地证明这种现象，这将在下面实验中详细描述。 实验 参与者会看到一个单词列表，每个单词都用墨水的颜色显示。参与者的任务是大声说出单词所用的颜色。 任务有两个条件: 一致词条件：显示的单词与其所用颜色一致 不一致词条件：显示的单词与其所用颜色不一致 在每一种条件下，列表中单词数量相同，测量参与者说出单词所用颜色的时间。每个参与者将经历2轮，1轮一致词条件，1轮不一致词条件，记录每个条件下的用时。 根据斯特鲁普效应，说出一致词条件下单词所用颜色所需时间较短。 分析 利用推论统计学的知识对“斯特鲁普效应”进行验证。 描述统计分析 报告关于数据集的一些描述性统计信息。例如，集中趋势测量和可变性测量。 12345import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as sns%matplotlib inline 123filepath = 'stroop_effect_data.csv'df = pd.read_csv(filepath)df.head() 数据说明，第一列Congruent表明是在一致词条件下，参与者的反应时间（以秒为单位），第二列Incongruent表明是在不一致词条件下，参与者的反应时间。 1df.describe() 123# 一致词条件下的样本均值 congruent_mean = df['Congruent'].mean()congruent_mean 14.315800000000001 123# 不一致词条件下的样本均值 incongruent_mean = df['Incongruent'].mean()incongruent_mean 23.13928 123# 一致词条件下的样本标准差congruent_std = df['Congruent'].std()congruent_std 3.973978516549882 123# 不一致词条件下的样本标准差incongruent_std = df['Incongruent'].std()incongruent_std 6.292920013263583 123# 两个条件的平均时间差mean_diff = congruent_mean - incongruent_meanmean_diff -8.823479999999998 123# 两个条件的时间差的标准差std_diff = np.std(df['Congruent'] - df['Incongruent'])std_diff 5.270743538591116 从以上输出结果中可知道，25个参与者在一致性词条件和不一致性词条件下执行任务的平均用时分别是xcˉ=14.3158s\bar{x_{c}}=14.3158sxc​ˉ​=14.3158s和 xiˉ=23.1393s\bar{x_{i}}=23.1393sxi​ˉ​=23.1393s。两种情况的样本标准差分别为sc=3.9740s_{c}=3.9740sc​=3.9740和si=6.2929s_{i}=6.2929si​=6.2929。两个平均时间的差值为xcˉ−xiˉ=−8.8235s\bar{x_{c}}- \bar{x_{i}}=-8.8235sxc​ˉ​−xi​ˉ​=−8.8235s，两种情况下参与者用时差值的标准差为sd=5.2707s_{d}=5.2707sd​=5.2707。 下面将数据进行可视化，比较两个条件下样本数据的分布。 123sns.set_style("darkgrid")sns.boxplot(data=df[['Congruent', 'Incongruent']], orient="v",width=0.4,palette=sns.color_palette("BuGn"));plt.ylabel("Time"); 从图中可以看出，在不一致词条件下的任务反应时间似乎比在一致词条件下的任务反应时间长，这与之前报告的描述统计结果一致。此外，不一致条件组存在三个异常值，因而该组样本数据的分布很可能是正偏态。 推论统计分析 假设检验 1、问题 Q1：研究问题的自变量是什么？因变量是什么？ 自变量是斯特鲁普任务的条件（单词含义与颜色是否一致），因变量是参与者完成任务的反应时间（以秒为单位）。实验要考察的是两个条件任务（自变量）对反应时间（因变量）的影响。 Q2：两个假设是什么？ 我们想要考察任务类型对参与者反应时间的影响大小，并评估我们样本中的差异是否因为总体存在显著差异。 μc\mu_{c}μc​ : 一致条件下，总体反应时间均值 μi\mu_{i}μi​ : 不一致条件下，总体反应时间均值 H0H_{0}H0​：在一致条件和不一致条件下，参与者反应时间总体没有显著差异 →\rightarrow→ μc=μi\mu_{c} = \mu_{i}μc​=μi​ H1H_{1}H1​：与一致条件相比，在不一致条件下，参与者反应时间总体更长 →\rightarrow→ μc&lt;μi\mu_{c} &lt; \mu_{i}μc​&lt;μi​ Q3：检验类型是什么？ 因为实验的参与者是同一组人，被分配了两个条件，所以这里应该是相关配对检验。相关配对检验只关注每对相关数据的差值，从而避免得到的结论受到参与者间正常反应时间独立性的影响。在只关注差值集的情况下，样本集处理后只有一组（差值集）。下面对样本数据进行处理，从而得到差值集。 12df['differences'] = df['Congruent'] - df['Incongruent']df.head() Q4：抽样分布类型是什么？ 样本大小为25，小于30，所以需要进一步判断抽样分布是否满足T分布。通过可视化差值列数据查看其分布情况。 12345#查看数据集分布sns.set_style("darkgrid")sns.distplot(df['differences'],color='green')plt.title('Distribution of the difference of reaction time under two conditions')plt.show() 通过观察上图，发现两个条件下的反应时间差值的分布呈近似正态分布，满足t分布的使用条件，所以我们使用相关配对t检验。 Q5：检验方向什么？ 因为备选假设是斯特鲁普效应确实存在，根据Stroop Effect的定义，颜色和文字不同的情况下，人们的反应时间会变长（μc&lt;μi\mu_{c} &lt; \mu_{i}μc​&lt;μi​），所以我们使用单尾检验中的左尾检验。 2、证据 在零假设成立的条件下，我们得到样本平均值（或者更极端值）的概率p为多少？ 12345678910from scipy import stats'''ttest_rel：相关配对检验返回的第1个值t是假设检验计算出的（t值），第2个值p是双尾检验的p值'''t,p_twoTail = stats.ttest_rel(df['Congruent'],df['Incongruent'])print('t值=',t,'\n双尾检验的p值=',p_twoTail) t值= -8.201128966874277 双尾检验的p值= 2.027895273895288e-08 123# 通过双尾检验的p值得到单尾检验的p值p_oneTail=p_twoTail/2print('单尾检验的p值=',p_oneTail) 单尾检验的p值= 1.013947636947644e-08 3、判断标准 显著水平为5% 1alpha = 0.05 4、得出结论 12345#决策if(t&lt;0 and p_oneTail&lt; alpha): print('拒绝零假设，有统计显著。即接受备选假设，说明斯特鲁普效应存在')else: print('接受备选假设，没有统计显著，也就是斯特鲁普效应不存在') 拒绝零假设，有统计显著性。即接受备选假设，说明斯特鲁普效应存在。 5、假设检验报告 相关配对检验 t(24)=−8.2011t(24)=-8.2011t(24)=−8.2011， p=1.0139e−08p=1.0139e-08p=1.0139e−08 (α=5%)(\alpha=5\%)(α=5%)， 单尾(左尾)检验 统计上存在显著差异，拒绝零假设，从而验证斯特鲁普效应存在。 置信区间 t检验的自由度 df=n−1=25−1=24df=n-1=25-1=24df=n−1=25−1=24，通过置信水平95%和自由度查表得到对应的t值 1234567891011t_ci= 2.064# 时间差的平均值: mean_diff# 使用scipy包的stats模块计算时间差的标准误差se = stats.sem(df['differences'])# 置信区间的上限a = mean_diff - t_ci * se# 置信区间的下限b = mean_diff + t_ci * se 1print('两个平均值差的置信区间，95%置信水平 CI=[&#123;:.6&#125;,&#123;:.5&#125;]'.format(a,b)) 两个平均值差的置信区间，95%置信水平 CI=[-11.0441,-6.6029] 效应量 统计显著说明，两个条件下参与者的反应时间有差异；但差异的大小，效果是否显著通过效应量来衡量。 123456789#根据零假设，可知时间差对应的总体平均值是0pop_mean=0# 时间差的样本平均值: mean_diff# 时间差的样本标准差：std_diffd=(mean_diff - pop_mean) / std_diffprint('d=',d) d= -1.674048440300045 分析结果总结 描述统计分析 第一组样本数据：25个参与者在字体含义和字体颜色一致情况下，平均用时是 xcˉ=14.3158s\bar{x_{c}}=14.3158sxc​ˉ​=14.3158s，标准差是 sc=3.9740s_{c}=3.9740sc​=3.9740 。 第二组样本数据：25个参与者在字体含义和字体颜色不一致情况下，平均用时是 xiˉ=23.1393s\bar{x_{i}}=23.1393sxi​ˉ​=23.1393s，标准差是 si=6.2929s_{i}=6.2929si​=6.2929 。 两种情况下参与者的平均用时差值为 xcˉ−xiˉ=−8.8235s\bar{x_{c}}- \bar{x_{i}}=-8.8235sxc​ˉ​−xi​ˉ​=−8.8235s，两种情况下参与者用时差值的标准差为 sd=5.2707s_{d}=5.2707sd​=5.2707。 推论统计分析 假设检验 相关配对检验 t(24)=−8.2011t(24)=-8.2011t(24)=−8.2011， p=1.0139e−08p=1.0139e-08p=1.0139e−08 (α=5%)(\alpha=5\%)(α=5%)， 单尾(左尾)检验 统计上存在显著差异，拒绝零假设，从而验证斯特鲁普效应存在。 置信区间 两个平均值差的置信区间，95%置信水平 CI=[-11.0441,-6.6029] 效应量 d= -1.6740 参考 斯特鲁普效应-维基百科 交互式斯特鲁普效应-文本和颜色实验]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>data_analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[淘宝婴儿用品销售数据销量分析之报告]]></title>
    <url>%2F2019-05-24%2F</url>
    <content type="text"><![CDATA[一、分析背景与目的 背景 拿到的数据是阿里天池里关于淘宝和天猫用户购买婴儿用品的真实数据，包括用户购买婴儿用品的交易记录以及婴儿的基本信息。原始的数据集应该有超过900万条，但目前只能采集到它的样本，其中交易记录数据包含29971条，婴儿信息数据包括953条。交易记录数据是从2012年7月2日-2015年2月5日，2年半左右的时间跨度。数据维度的详细介绍如下： 交易记录数据，包含7个字段： user_id: 用户编号 auction_id: 购买行为编号 cat_id：商品编号 cat1：商品属于的类别 property：商品具有的属性 buy_mount：购买数量 day：购买时间 婴儿信息数据，包含3个字段： user_id：用户编号 birthday：婴儿出生日期 gender：婴儿性别（0 男性；1 女性） 目的 目的是通过这个数据集，分析一些实际的业务问题。比如： 最受用户欢迎的产品是哪些？ 一年中，用户更喜欢在什么月份购买商品？ 哪个品牌的婴儿用品更受用户喜爱？ 对于不同品牌商品，用户购买时间和频率是什么样的？ 不同品牌的用户画像如何？… 具体地，拟通过分析解决以下几个业务问题： 最畅销的10个商品 总的月均购买次数，各个月的平均购买次数，不同年份的月均购买次数 每类商品的月均购买次数，月均销量 每类商品的月均销量趋势，月均购买次数趋势 每类商品的用户画像，具体表现为性别和年龄段的分布情况 二、分析思路与分析过程 数据清洗 首先对数据集进行清洗，包括： 选择子集，对于要分析的业务指标，不需要用到交易记录数据中的auction_id，property字段，选择将这两个字段删除。 列名规范，为了便于查看和理解，将所有列名改为中文名。 缺失值查看，两个数据集都不存在缺失字段。 数据类型转换，查看数据类型，发现都是int64类型。这对于销售时间和出生日期字段来说，并不合理，所以将这两个字段转换为日期类型。 数据排序，为了便于查看时间跨度，分别按照销售时间和出生日期进行升序排序，并将排序后的索引值重新改为0-N的顺序。 异常值处理， 查看婴儿性别字段，发现除了0（男），1（女）之外，还存在2的情况，这应该是用户不愿意透露婴儿的性别信息，这部分值只有26个，删除这部分数据后，将0和1分别替换成男和女。 总体分析思路 具体分析过程 指标一 根据商品编号进行分组，统计不同商品的销量 根据销量排序找出销量前10的商品 按照分析思路计算并绘制柱状图，得到结果如下： 可以看到这段时间内销量第一的是商品编号为50018831的商品，其销量高达12657件；销量第二的商品50011993仅卖出3609件，两者差距非常大，前者是后者的3.5倍多。此外，除去销量第一的商品外，剩下的9个商品，销量差别不大，对这些商品商家可以多进行广告投放的营销活动。 指标二 指标二包含了三个小指标，以月为维度分析整体月均购买次数，各个月的平均购买次数，以及以年为维度分析月均购买次数； 整体月均购买次数，这个指标是一个值，指的是在2年半的时间跨度里平均每个月的购买次数。由总的消费次数除以总的月份数得到，在计算总消费次数时注意同一天内，同一个用户发生的所有消费都算作一次消费，所以需要根据用户编号和销售时间两个字段作去重处理。 由于整体月均购买次数只是一个具体的数字，不能反映出用户的购物习惯，为了分析用户更喜欢在哪些月份消费，进一步计算用户在不同月份的平均购买次数。计算方式为各个月的总购买次数除以月份出现的年份数，注意不同月份出现的年份数是不一样的，因为采集的数据是从2012年的7月到2015年的2月，所以有的月份出现了3次有的月份只出现了2次。 将整体月均购买次数、各月的平均购买次数可视化，结果如图所示： 发现在5、11、12这三个月的月均购买次数远超过整体月均购买次数，说明用户更喜爱在这三个时间段购买婴儿用品，那么商家在这些时段可备好货源，保持营销。2、7、8这三个月，用户的购买力度不强，商家可以着重在这些时段进行更多的产品推广或者促销活动。 再通过以年为维度对比不同年份的月均购买次数，观察随着时间的推移，用户的购买力度的变化情况。计算方式为一年的总购买次数除以对应的月份数，得到结果如下： 2012年至2014年，月均购买次数呈上升趋势，2014年到达高峰，2015年又降下来。对于2014年的购买高峰，如果没有商家方的内部原因，那么也许是2014年婴儿出生率有提高，或者是其他外部影响因素。 指标三 接下来开始细分到每个商品种类，分析每个类别中商品的月均购买次数和商品的月均销量。各类商品的月均销量由各类商品的销售总量除以总的月份数得到，先按商品种类字段分组，求出各类别商品的销售总量，月份数在指标一中已计算，根据公式得到各类商品的月均销量；同理可计算各类商品的月均购买次数。 最终绘制成柱状图，进行对比分析： 从上图中看到，总共6个商品种类，只有种类122650008它的销量和购买次数基本持平；剩下的5类都是销量大于购买次数，即用户购买时会一次买多件。购买次数最多的种类是50008168，销售数量最多的却是种类28，该种类的销量是其购买次数的4倍多，说明用户每次购买该种类的婴儿用品都会囤货。 此外，商品种类50014815也比较特别，购买次数排第3位，但是它的销量超过了购买次数最多的50008168。所以商品种类28和50014815是6种商品里最畅销的。 对于那些购买次数多，但销量不是特别高的商品种类，说明用户粘性强，那么商家可以适当做一些广告，刺激用户的消费，提高总销量； 对于购买次数少，但是热销的商品种类，商家可进行引流和促销。 下面通过指标四来分析每类商品具体是在哪些月份更畅销。 指标四 为了进一步分析不同类的商品究竟在哪些月份更畅销，将每种商品种类在一年12个月中的月均购买次数和月均销量分别可视化出来。计算这两个小指标时，首先按商品种类和销售月份分组得到月总销量（月总购买次数），然后统计月总销量（月总购买次数）是由几个年份组成，最后计算月均销量（月均购买次数）。 从这幅图可以看到，商品种类50008168（红色）的月均购买次数最多，与指标三吻合；并且可以很清楚看到这个种类的商品在不同月份的平均购买次数波动很大，在2、7月是低谷，而在5、11月是高峰，远远多于其他种类；观察下边的图，发现这个这个种类的商品在9月份销量更高，对于购买次数处于高峰的5、11月销量反而不算多。其他5类商品的平均购买次数趋势相对平稳，低谷都出现在2月份，高峰都出现在11月份。 从这幅图可以看到，所有种类的商品的销量低谷都在2月，与上图结果一致；商品种类28（橙色）是销量最多的，它分别在5、7、12月出现了销售高峰；而另一商品种类50014815（紫色）11月份的销量激增，这说明用户特别喜爱在双十一活动时对它进行囤货。其余3类商品，相对于平均购买次数，它们的平均销量趋势非常平稳。 2月份一般都是春节期间，所以用户的消费欲望会有所减缓；对于不同种类商品，用户的囤货行为会出现在不同的月份，商家可以提前做好预估，并在高峰来临前做进一步的宣传；至于购买次数和购买量相对平缓的月份，商家可通过广告、红包福利等措施刺激用户消费。 指标五 最后对各类商品的用户画像进行分析。需要先将交易数据表和婴儿信息数据表合并，并且以婴儿信息数据表为主。先分析购买每类商品的婴儿性别分布情况，根据商品种类和性别字段对数据进行分组，统计每种商品的用户性别分布，计算每类商品的男女用户比例，最后可视化： 除了商品种类50022520对女孩的销售比例高于男孩外，其余5类商品的用户对象都是男孩居多，尤其是种类38和50014815的商品，对男孩的销售比例占据了70%以上。 再分析购买每类商品的婴儿年龄段分布情况。先计算婴儿的年龄，并将年龄分成0岁、1岁、2岁、3岁、4岁、5岁、5岁以上7个阶段，然后统计不同类商品在不同年龄段的销售情况，计算每类商品的用户年龄段比例，最后可视化： 种类为50022520（灰棕色）的商品，其用户百分之80%以上都是0岁的婴儿，即还未出生，父母就为其购买了相关用品。种类为50014815（紫色）的商品，其用户大部分由0岁和1岁两个年龄段组成，占据百分之70%以上；种类38（绿色）的商品，其用户大部分由1岁和2岁两个年龄段组成，占据80%左右；种类122650008（蓝色）的商品，其用户除了1岁和2岁的，还有3岁的孩子。剩余两个种类的商品，都是随着孩子年龄增大，销售比例逐渐降低。 通过对不同种类商品的用户进行画像分析，进行精准营销。对不同年龄和不同性别的孩子，推销不同种类的商品。 四、总结 以上分析涉及了用户消费趋势分析、商品对比分析和用户画像分析。这些分析都是小数据量下的关于业务指标的描述性统计分析，目的是为精准营销。数据中未包含用户的行为类型，所以没有进行PV、DAU、留存与转化等网站运营指标的分析。利用用户购买的商品信息来预测用户年龄段，以及利用用户信息预测购买何种商品种类的分析，可以参见前一篇文章，这里不给出预测结果。本案例的目的是通过分析现实场景数据，锻炼业务思维，加强用Python处理数据和进行可视化的代码能力，在以后有更多业务经验后再做出更深入的分析。]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>data_analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[淘宝婴儿用品销售数据分析之预测]]></title>
    <url>%2F2019-05-19%2F</url>
    <content type="text"><![CDATA[之前对淘宝婴儿用品销售数据集做了基本的业务指标探索性分析。这里还是以这个数据集为例，进行后续的相关性探索。通过分析两个问题，熟悉机器学习算法建模流程。 两个预测问题： 根据孩子的信息(年龄、性别等)预测用户会购买什么样的商品； 根据父母的购买行为预测孩子的年龄。 分析流程： 提出问题 理解数据 包括：采集数据，导入数据，查看数据集信息，理解字段含义，查看是否有缺失值 预处理/数据清洗 包括：选择子集、列名重命名、缺失值处理、类型转换、重复值处理、异常值处理、数据排序等 特征提取 包括：对不同类型的数据做特征提取，如，数值型直接使用；分类型可能需要进行one-hot编码；字符串型可能需要做一些提取工作等 特征选择/特征降维 特征选择的好坏，会直接影响模型的预测性能。这个过程涉及到特征工程的知识，相关学习可以参考以下两篇博文： 机器学习之特征工程 使用sklearn做特征工程 建立模型 选择合适的机器学习算法，用训练集和算法得到机器学习模型，并用测试集来评估得到的模型。 字段含义： user_id：用户id auction_id：购买行为编号 cat_id：商品种类ID cat1：商品属于哪个类别 property：商品属性 buy_mount：购买数量 day：购买时间 birthday：出生日期 gender：性别（0 男性；1 女性） 问题1：根据孩子的信息预测用户会购买什么样的商品 一、提出问题 问题一：根据孩子的信息(年龄、性别等)预测用户会购买什么样的商品。 首先确定特征和标签是什么，特征是年龄age和性别gender，标签是预测出商品类别cat1。 二、理解数据 下载并导入数据后，查看数据集信息： 1234fileName1 = './(sample)sam_tianchi_mum_baby_trade_history.csv'fileName2 = './(sample)sam_tianchi_mum_baby.csv'trade_data = pd.read_csv(fileName1)baby_data = pd.read_csv(fileName2) 1trade_data.info() 1baby_data.info() 三、数据预处理 以婴儿信息表为主，合并两个数据集。 12combined_data = pd.merge(trade_data,baby_data,on='user_id',how='right')combined_data.head() 查看合并后的数据集信息： 1combined_data.info() 处理重复数据 发现有3个user_id重复出现，删除： 12combined_data.drop_duplicates(subset='user_id',inplace=True)combined_data.info() 处理缺失值 发现property商品属性这个字段有2条缺失。可以看到每个商品的属性都有多个，以逗号连接。这里采取删除的处理方式： 12combined_data.dropna(inplace=True)combined_data.info() 现在所有字段都不为空，总共951条数据。 数据类型转换 将购买商品日期和婴儿出生日期转换为时间格式： 12combined_data['day'] = combined_data['day'].astype('str')combined_data['birthday'] = combined_data['birthday'].astype('str') 12combined_data['day'] = pd.to_datetime(combined_data['day'],format='%Y-%m-%d')combined_data['birthday'] = pd.to_datetime(combined_data['birthday'],format='%Y-%m-%d') 处理异常值 性别gender字段除了0（男），1（女）之外，还存在2的情况，这应该是用户不愿意透露婴儿的性别信息，这部分值只有26个，故删除这部分数据： 1combined_data['gender'].value_counts() 12combined_data = combined_data[combined_data['gender']&lt;2]combined_data['gender'].value_counts() 到目前为止，总共925条数据。 四、特征提取 计算婴儿年龄，并进行分段，增加age列和age_group列 增加年龄列，年龄的计算通过购买日期的年份减去出生日期的年份得到： 12combined_data['age'] = (combined_data['day'].dt.year - combined_data['birthday'].dt.year)combined_data['age'].describe() 发现年龄有异常大的值28岁，将其删去： 12combined_data = combined_data[combined_data['age'] &lt; 28]combined_data['age'].describe() 这里只有一个异常大值，所以最终剩下924条数据。然后删除销售时间day字段和出生日期birthday字段： 1combined_data.drop(['day','birthday'],axis=1,inplace=True) 将年龄进行分组，总共分为5个年龄组，增加年龄组列： 1234567bin_labels = ['G0','G1','G2','G3','G4'] #年龄段标签bin_edges = [-3,0,1,3,6,12]combined_data['age_group'] = pd.cut(combined_data['age'], bin_edges, labels=bin_labels)combined_data['age_group'] = combined_data['age_group'].astype('str')# 确认所有年龄都被分到正确区间combined_data['age_group'].isnull().sum() 对年龄组列进行one-hot编码，并将创建的虚拟变量添加到数据集： 12ageDf = pd.get_dummies(combined_data['age_group'])combined_data = pd.concat([combined_data,ageDf],axis=1) 查看数据集是否得到预期效果： 1combined_data.head(3) 如下图所示： 五、特征选择 该问题是根据孩子的信息(年龄、性别等)预测用户会购买什么样的商品。 那么选择的特征为： 特征：年龄区间age_group或者年龄age，性别gender 标签：商品种类cat1 特征以选择年龄区间age_group和性别gender为例： 123# 特征选择train_features_2 = pd.concat([combined_data['gender'],ageDf],axis=1)train_labels_2 = combined_data['cat1'] 六、构建模型 拆分数据集为训练集和测试集 训练集用于训练模型，测试集用于验证模型： 1234from sklearn.model_selection import train_test_splittrain_X_2, test_X_2, train_y_2, test_y_2 = train_test_split(train_features_2,train_labels_2,test_size=.2)#输出数据集大小print ('原始数据集特征：',train_features_2.shape, '训练数据集特征：',train_X_2.shape , '测试数据集特征：',test_X_2.shape)print ('原始数据集标签：',train_labels_2.shape, '训练数据集标签：',train_y_2.shape , '测试数据集标签：',test_y_2.shape) 打印结果为： 建立模型，训练和验证 这里选择梯度增强算法： 1234# 梯度增强 from sklearn.ensemble import GradientBoostingClassifierfrom sklearn.metrics import accuracy_scoremodel = GradientBoostingClassifier() 训练模型，并用测试集验证： 123model.fit(train_X_2, train_y_2)test_pred = model.predict(test_X_2)accuracy_score(test_pred,test_y_2) 问题2：根据父母的购买行为预测孩子的年龄 首先确定特征和标签是什么，现在能非常确定的是预测标签是年龄组age_group；而对于特征则是不确定有哪些。 与问题1的相同之处 理解数据、数据预处理步骤与问题1相同。 与问题1的不同之处 在第四步特征提取中，增添了对商品属性property的处理，因为商品属性也是购买行为的特征之一。 商品属性property字段是字符型，每个商品具有多个大类别属性以及大类别属性下的具体属性。每对属性数字，表示大类别属性下的具体属性，即大类别属性 : 具体属性；每对属性数字用;隔开，如下图红框中所示。 处理方式： 先按分号划分字符串，得到每个商品具有的所有属性对； 再按冒号划分每个属性对，保留每个商品含有的具体属性。 首先按属性值将property拆分成多行，并将新的列命名为new_property： 123new_prop = combined_data['property'].str.split(';',expand=True).stack().reset_index(level=1,drop=True).rename('new_property')combined_data = combined_data.drop('property',axis=1).join(new_prop)combined_data.head() 如图所示： 然后，再拆分每个属性对，提取出其中的具体属性；为此定义一个函数： 123456# 定义函数，提取具体属性def getTitle(property): str1=property.split( ':' )[1] #strip() 方法用于移除字符串头尾指定的字符（默认为空格） str2=str1.strip() return str2 调用函数，提取具体属性，并将提取后的特征连接到原数据集上： 12345#存放提取后的特征propDf = pd.DataFrame()#map函数：对Series每个数据应用自定义的函数计算propDf['property'] = combined_data['new_property'].map(getTitle)combined_data = pd.concat([combined_data,propDf],axis=1) 对数据集进行去重处理，同一个商品若具有多条相同商品属性则删除： 1combined_data.drop_duplicates(['cat1','property'],inplace=True) 最终得到2690条数据，提取具体属性之后的结果： 第五步中特征选择不同 这里是将所有的字段都作为特征输入，包括提取后的property字段，而标签为年龄组： 12train_features_1 = combined_data[['user_id','cat_id','gender','auction_id','cat1','property']]train_labels_1 = combined_data['age_group'] 总结 机器学习建模分析的流程与业务指标分析的流程大同小异。特别注意的是机器学习的模型效果很大程度取决于特征工程，特征工程做得好，即便模型简单，参数不优，也能获得很好的性能。本文主要以呈现机器学习算法分析流程为目标，特征工程工作做得相对简单，得到的模型准确率并不高，待后续深入学习这块知识后再作改进。]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>data_analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[探索性数据分析基本流程]]></title>
    <url>%2F2019-05-14%2F</url>
    <content type="text"><![CDATA[以分析数据集未前往就诊的预约挂号为例，总结一下探索性数据分析的基本流程。 目录 简介 提出问题 理解数据 数据清洗 探索性数据分析（建立指标模型与可视化） 结论/交流 简介 数据集的简介 数据来源kaggle，未前往就诊的挂号预约指一个人预约了医生，收到了所有的指示却没有按约去医院就诊。该数据集包含11万条巴西病人预约挂号的求诊信息，每行数据包含有关患者特点的14个变量，具体有： PatientId：病人ID， AppointmentID：预约流水号ID， Gender：预约者的性别， ScheduledDay：作出预约的具体时间， AppointmentDay：预约的就诊日期， Age：病人年龄， Neighbourhood：医院所在位置， Scholarship：是否参加巴西福利项目 Bolsa Família Hipertension： 是否是高血压， Diabetes：是否是糖尿病， Alcoholism：是否是酗酒， Handcap：是否是残障， SMS_received：病人是否收到短信通知， No-show：no表示病人如约就诊，yes表示病人没有前往就诊。 提出问题 数据分析的目标就是解决问题，提出感兴趣的或是以业务为导向的问题。一般是两种情况：要么获取一批数据，然后根据它提问；要么先提问，然后根据问题收集数据。 这里提的问题如下： 病人的年龄是如何分布的？哪个年龄段的病人更多？ 未按约去就诊的病人有多少？占多大的比例？ 人们一般会预约在一周的哪一天就诊？ 一天中的哪个时段，预约的人最多？ 一般会提前几天预约挂号，即病人等待就诊的时长是多久？ 哪些重要因素会影响病人是否如约去就诊？例如，年龄，性别，是否参加福利项目，一周中的哪天就诊等。 理解数据 理解数据背景，每个字段的实际含义。主要包括：导入数据和查看数据基本信息，比如，一些描述统计信息。 导入必要的包和数据： import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline df = pd.read_csv('noshowappointments-kagglev2-may-2016.csv') 查看基本信息 查看数据集中有哪些列 df.head() 查看数据集是否有缺失值、重复值以及每列的数据类型。共14列，每列都是110527个值，不存在缺失值，也不存在重复值。 df.info() df.duplicated().sum() 检查异常值，发现年龄的最小值有误。 df.describe() df.Age.describe() 输出如下： count 110527.000000 mean 37.088874 std 23.110205 min -1.000000 25% 18.000000 50% 37.000000 75% 55.000000 max 115.000000 Name: Age, dtype: float64 数据清洗 这一步主要是评估数据来识别数据质量或结构中的问题，并通过修改、替换或删除数据来清理数据，以确保数据集具有最高质量和尽可能结构化。 1、将所有列名统一修改成小写并以下划线连接，便于操作。 df.rename(columns=lambda x: x.strip().lower().replace('-','_'), inplace=True ) df.head(2) 2、删除与问题无关的列 要删除的列包括： PatientId：病人ID， neighbourhood: 医院位置 Hipertension： 是否是高血压， Diabetes：是否是糖尿病， Alcoholism：是否是酗酒， Handcap：是否是残障， SMS_received：病人是否收到短信通知， df.drop([‘patientid’,‘hipertension’,‘diabetes’,‘alcoholism’,‘handcap’,‘sms_received’,‘neighbourhood’], axis=1, inplace= True) df.tail(2) 3、处理异常值。 通过上面的观察发现age列的最小值为-1，最大值为115。最小值明显是错误值，而最大值不确定是否有误，但我觉得它可能是真实的，所以保留。 df = df[df['age'] &gt; 0] df.age.describe() 4、对age列进行分段，并创建新的列 age_group bin_labels = ['Teenager','Young','Middle','Old'] bin_edges = [0,19,38,56,116] df['age_group'] = pd.cut(df['age'], bin_edges, labels=bin_labels) df['age_group'].isnull().sum() 探索性数据分析（建立指标模型与可视化） 执行EDA，可以探索并扩充数据，以最大限度地发挥自己数据分析、可视化和模型构建的潜力。 探索数据涉及在数据中查找模式，可视化数据中的关系，并对正在使用的数据建立直觉。 经过探索后，可以删除异常值，并从数据中创建更好的特征。 问题 1：病人的年龄是如何分布的？哪个年龄段的病人更多？ plt.subplots(figsize=(6,4)) df['age_group'].value_counts().plot(kind='bar') plt.title('Age Distribution') plt.xlabel('Age') plt.ylabel('Frequence'); 可以看到发出预约挂号的病人中，青年人最多，其次是中年人和青少年，老年人最少。 问题2：未按约去就诊的病人有多少？占多大的比例？ x_vaule = df['no_show'].value_counts() print(x_vaule) labels = 'Show up','No show' explode = [0, 0.1] plt.subplots(figsize=(5,5)) plt.axes(aspect=1) # 设置饼图样式，标签，突出显示，圆上文本格式，显示阴影，文本位置离圆心距离，起始角度，百分比的文本离圆心距离 plt.pie(x=x_vaule, labels=labels,explode=explode,autopct='%.1f %%', shadow=True, labeldistance=1.1, startangle = 90,pctdistance = 0.6 ) plt.title('The proportion of patients who did not go to the hospital as appointment '); 通过统计函数可以得出未按约定到医院就诊的病人有21680人，观察饼图可以知道，该人数占据总预约人数的20.3%。 问题3：人们一般会预约在一周的哪一天就诊？ # 先将预约日期转换成日期类型，然后换成一周表示 df['appointmentday'] = pd.to_datetime(df['appointmentday'], errors='coerce') df['appointment_weekday'] =df['appointmentday'].dt.weekday_name plt.subplots(figsize=(8,5)) plt.plot(df['appointment_weekday'].value_counts().keys(),df['appointment_weekday'].value_counts().values) plt.title('Appointment Day Distribution') plt.xlabel('Appointment Day') plt.ylabel('Frequence'); 可以看到，人们更喜欢在星期三就诊，其次是星期二，星期四和星期五预约就诊的人相对少些，星期六的人最少。这里没有出现星期天，也许是因为星期天是休息日。 问题4：一天中的哪个时段，预约的人最多？ #先将scheduleday处理成日期类型，然后提取小时 df['scheduledday'] = pd.to_datetime(df['scheduledday'], errors='coerce') df['scheduledhour'] = df['scheduledday'].dt.hour # df['scheduledhour'].describe() #将一天的时间分成三个时段，上午，下午，晚上 bin_labels = ['Morning','Afternoon','Night'] bin_edges = [5,12,18,22] df['scheduledhour_cut'] = pd.cut(df['scheduledhour'], bin_edges, labels=bin_labels) # df['scheduledhour_cut'].isnull().sum() df['scheduledhour_cut'].value_counts() plt.subplots(figsize=(6,4)) df['scheduledhour_cut'].value_counts().plot(kind='bar') plt.title('Scheduled Time Distribution') plt.xlabel('Scheduled Time') plt.ylabel('Frequence'); 可以看到，绝大多数的病人选择在上午预约，一部分病人选择在下午预约，极少病人选择在晚上预约。一般下午6点以后，医院的医生可能已经下班了，预约成功的几率会下降。 问题5：一般会提前几天预约挂号，即病人等待就诊的时长是多久？ df['waittime'] = df['appointmentday'].dt.date - df['scheduledday'].dt.date df['waittime'].describe() 等待时长的统计信息如下： count 106987 mean 10 days 04:00:04.710852 std 15 days 06:19:27.050399 min -6 days +00:00:00 25% 0 days 00:00:00 50% 4 days 00:00:00 75% 14 days 00:00:00 max 179 days 00:00:00 Name: waittime, dtype: object 得到的等待时长出现了负数值，这些值应该是错误的。至于等待时长很大的数据，不确定是否是错误的。这里留下那些等待时长大于等于0天的数据。 df = df[df['appointmentday'].dt.date &gt;= df['scheduledday'].dt.date] df['waittime'] = (df['appointmentday'].dt.date - df['scheduledday'].dt.date) df['waittime'].describe() 处理后的结果： count 106982 mean 10 days 04:00:53.840833 std 15 days 06:19:37.756884 min 0 days 00:00:00 25% 0 days 00:00:00 50% 4 days 00:00:00 75% 14 days 00:00:00 max 179 days 00:00:00 Name: waittime, dtype: object 人们平均提前10天预约，大多数人提前一星期预约，等待时长最短是0天，即当天预约当天就诊，等待时长最久的是179天。 问题6：哪些重要因素会影响病人是否如约去就诊？ 例如，年龄，性别，是否参加福利项目，一周中的就诊时间等 计算去就诊和未去就诊的人数的比例函数 def cal_proportion(col_name): &quot;&quot;&quot;按传入的列与no-show分组，统计不同类的数量，并计算各类的比例 &quot;&quot;&quot; counts = df.groupby(['no_show',col_name]).count()['appointmentid'] total = df.groupby(['no_show']).count()['appointmentid'] no_show_proportions = counts['Yes'] / total['Yes'] show_proportions = counts['No']/ total['No'] return no_show_proportions,show_proportions 绘制图形函数 def double_bar(bar_data1,bar_data2,xlabels,xtick_label): # 绘制条柱,先设置每个等级组的x坐标位置和每个条柱的宽度 ind = np.arange(len(bar_data1)) width = 0.35 red_bar = plt.bar(ind,bar_data1, width, color='r', alpha=0.7, label='No show') blue_bar = plt.bar(ind+width, bar_data2, width, color='b',alpha=0.7, label='Show up') # 标题和标签 plt.ylabel('Proportion') plt.xlabel(xlabels) plt.title('Proportion by '+ xlabels + ' and No-Show') locations = ind + width / 2 # x 坐标刻度位置 plt.xticks(locations, xtick_label) # 图例 plt.legend() 6.1 不同年龄段的病人，如约去就诊的情况怎么样？ 计算各个年龄段，按约去就诊的比例和未按约去就诊的比例 no_show = cal_proportion('age_group')[0] show_up = cal_proportion('age_group')[1] xtick_label = ['Teenager', 'Young', 'Middle', 'Old'] double_bar(no_show,show_up,'Age Group',xtick_label) 通过上图可以看到，不同年龄段的病人如约去就诊的比例是不同的。年龄越高，去就诊的比例比未去就诊的比例就越高，而青少年和年轻人则是未去就诊的比例高于按约去就诊的比例。 6.2 不同性别的病人，如约去就诊的情况怎么样？ no_show = cal_proportion('gender')[0] show_up = cal_proportion('gender')[1] xtick_label = ['Female', 'Male'] double_bar(no_show,show_up,'Gender',xtick_label) 看起来，男女患者去医院就诊的概率相似。但在所有患者里，女性预约患者更多。 6.3 是否参加福利项目，影响患者如约去就诊吗？ no_show = cal_proportion('scholarship')[0] show_up = cal_proportion('scholarship')[1] xtick_label = ['False', 'True'] double_bar(no_show,show_up,'Scholarship',xtick_label) 可以看到，未参加巴西福利项目的病人更倾向于按约就诊；而参加了巴西福利项目的病人按约就诊的比例低于未按约就诊的比例。还可以清楚地看到，绝大多数的病人并没有参加福利项目。 6.4 在一周的哪天就诊，是否会影响病人如约去就诊？ no_show = cal_proportion('appointment_weekday')[0] show_up = cal_proportion('appointment_weekday')[1] xtick_label = ['Friday', 'Monday', 'Saturday', 'Thursday', 'Tuesday', 'Wednesday'] double_bar(no_show,show_up,'Appointment Weekday',xtick_label) 除开星期六外，病人更喜欢在星期四、星期二和星期三如约去就诊；而约在星期五和星期一的病人，未去就诊的比例更高。同时也可以知道，更多的病人选择在星期二和星期三就诊。 结论/交流 得出结论这一步通常使用机器学习或推理性统计来完成，本文的重点是使用描述性统计得出结论。 与他人交流结果，要证明发现的见解。如果最终目标是构建系统，那么需要分享构建的结果，解释得出设计结论的方式，并报告该系统的性能。 交流结果的方法有多种：报告、幻灯片、博客帖子、电子邮件、演示文稿，甚至对话。 数据可视化在这个过程中可以发挥很大的价值。 实验结果：在所有这些病人中，女性病人和中老年病人更关注身体健康，年龄越大，更愿意按约去就诊。人们一般倾向于在星期二和星期三的上午去就诊，有的病人会提前很久就预约医生，而大多数的病人会在预约当天去就诊。病人的年龄、病人是否参加福利项目以及在一周的哪天去就医这几个因素会影响病人是否按约去医院就诊。 数据本身的局限性：数据中还提供了病人患的几种病，例如，高血压，酗酒，糖尿病等，还提供了医院的地理位置，病人是否患这些病和医院的位置可能也会影响他们能否按约去医院就诊。此外，病人等待的时间长短，可能也会影响他是否按约去就诊。目前，暂时未作这三方面的探究，只对目前感兴趣的问题做了分析。 探索方式局限性：此次探索分析使用的只是一些简单的统计计算和可视化分析。这样得出的结论只能是暂时的，还有进一步验证的需要（使用统计检验或者机器学习建模等）。]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>data_analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[淘宝婴儿用品销售数据EDA之遇到的问题总结]]></title>
    <url>%2F2019-05-09%2F</url>
    <content type="text"><![CDATA[在阿里天池下载的淘宝天猫婴儿用品销量数据集，数据的背景介绍可参见数据出处或者这里。 在用Python对该数据进行初步业务指标分析后，我遇到了一些问题，这里做下总结。 本文主要内容，包含以下两方面： 1 . 在数据清洗过程，遇到了什么问题？有什么需要注意的？ 2 . 模型构建过程，即分析了哪些业务指标，需要注意什么问题？ 数据清洗 选择子集 对于要分析的业务指标，不需要用到交易记录数据中的auction_id，property等字段信息。 选择子集的方式，有不同方法：可以将不需要的字段删除，也可以选择只保留要使用的字段。 # 方式一，删除不用的 sub_trade_data = trade_data.drop(columns=['auction_id','property']) # 方式二，保留需要的 sub_trade_data = trade_data.loc[:, ['user_id','cat_id','cat1','buy_mount','day']] 规范列名-重命名 原始数据中所有字段名为英文缩写，为了在分析时便于查看和获取指标计算要使用的字段，将所有列名改为中文形式。原始数据集包含两个数据文件，没合并，一开始我以为需要为每个数据集单独定义一个列名字典来进行列名修改，但其实只需要定义一个，直接应用在两个数据集上。 newColName = {'user_id':'用户编号','cat_id':'商品编号','cat1':'商品种类','buy_mount':'销售数量','day':'销售时间','birthday':'出生日期','gender':'性别'} sub_trade_data.rename(columns=newColName,inplace=True) baby_data.rename(columns=newColName,inplace=True) 缺失值查看 先查看两个数据集是否存在缺失值，若存在，需要进行处理。对缺失值的处理，要么删除，要么用其它统计值填补。这两个数据集比较好，未存在任何缺失值。 查看数据集是否存在缺失值的方式可以使用isnull()函数，后面加any()查看列是否存在缺失，若存在会返回True；也可以使用info()函数，这个函数会显示数据有哪些列，每列有多少项，是否有缺失等信息。 # 方式一，使用isnull() sub_trade_data.isnull().any() #方式二，使用info() sub_trade_data.info() 数据类型转换 使用.dtypes查看数据字段类型时，发现都是int64类型，这对于销售时间和出生日期两个字段很明显是不合理的。需要将这两个字段转换为日期类型。 下面说下我在这个过程中经历的几个阶段： 第一阶段 一开始，我直接使用Pandas的to_datetime()函数将字段进行格式转换，程序不报错，如下： sub_trade_data['销售时间'] = pd.to_datetime(sub_trade_data['销售时间'],format='%Y-%m-%d') 得到结果： 图中展示的都是同一个时间，有点奇怪。第一时间查看原始表格中销售时间字段，发现并不是如图中那样，原始数据表中的销售时间是8位数字表示的时间： 那么就是时间函数解析的问题，网上查了一下，发现如果直接将一串数字作为参数传入to_datetime()函数，它会将这串数字当作unix时间戳，并把unix时间戳默认转为GMT标准时间。而北京时间比GMT标准时间要加8小时，所以如果是unix时间戳形式的时间，转换时还需要自己处理时区问题，具体解决方案可参考这里。 回到这里的问题，看了原始表格的时间发现它并不是unix时间戳形式，所以转换结果不对。 第二阶段 重新开始排查问题，查了Pandas的to_datetime文档，第一个参数的类型可以是整型，字符串型，也可以是日期型。既然直接传入数字转换结果不对，那么就想着传入正确格式（想要转换成的日期格式是年-月-日这种）的字符串试试。 所以就有了以下步骤： 1 . 先将原始表格里日期字段的 数字类型转成字符串类型 sub_trade_data['销售时间'] = sub_trade_data['销售时间'].astype('str') 2 . 定义函数：将字符串类型的日期转成年-月-日格式 def dateStrFormat(timeCol): ''' 输入：timeCol销售时间列，Series类型 输出：转成'y-m-d'字符串形式，返回也是Series类型 ''' timeList = [] for time in timeCol: year = time[:4] month = time[4:6] day = time[6:] timeFormat = str(year) + '-' + str(month) + '-' + str(day) timeList.append(timeFormat) timeColSer = pd.Series(timeList) return timeColSer # 获取销售时间字段 timeCol_sale = sub_trade_data['销售时间'] # 调用函数将数字日期转成字符串日期 dateSer_sale = dateStrFormat(timeCol_sale) # 修改销售时间这一列的值 sub_trade_data['销售时间'] = dateSer_sale 3 . 将格式调整后的字符串传入to_datetime函数，转成时间类型 sub_trade_data['销售时间'] = pd.to_datetime(sub_trade_data['销售时间']) 最终得到了正确结果： 第三阶段 调用函数调整格式后再作为参数这种做法，效果挺好的，但是缺点就是太繁琐。其实上面第二步中转换字符串格式的步骤，pandas的to_datetime()会自动完成的，后来我将转换成string类型的那串数字直接传入函数，发现也能得到正确的结果&gt;.&lt; 。大道至简，所以还是不用费时间写一个格式转换函数了。 去掉上面的第二个步骤，得到同样的结果： sub_trade_data['销售时间'] = sub_trade_data['销售时间'].astype('str') sub_trade_data['销售时间'] = pd.to_datetime(sub_trade_data['销售时间'], format='%Y-%m-%d') #第二个参数可选 数据排序 这个步骤，没什么问题，按照时间字段对数据集排序就好。注意排序后，要重新设置行索引值。 sub_trade_data = sub_trade_data.sort_values(by='销售时间',ascending=True) sub_trade_data = sub_trade_data.reset_index(drop=True) 查看/处理异常值 对于交易数据集，使用describe()函数查看统计信息，发现销售数量字段的值都为正数，没问题。 对于婴儿信息数据集，需要查看婴儿性别字段的值是否正常，发现除了0（男），1（女）两个值外，还存在2这个值， 应该是用户不愿意透露婴儿的性别信息，这部分值只有26个 ，故删除这部分数据，并将0和1分别替换成男和女。 baby_data['性别'].value_counts() #查看性别字段的各个值及对应数量 baby_data = baby_data[baby_data['性别']&lt;2] #保留0,1两个值 #将0和1分别替换成男和女 baby_data['性别'] = baby_data['性别'].map({0:'男',1:'女'}) #将原来的int64类型转成string字符串类型 baby_data['性别'] = baby_data['性别'].astype('str') 模型构建 总共分析了6个业务指标，包括： 1 . 月均购买次数 2 . 最畅销的10个商品 3 . 每类商品的月均销量 4 . 每类商品的月销量趋势 5 . 每类商品的销量峰值是在哪个月 6 . 每类商品的用户画像（年龄段，性别） 业务指标1：月均购买次数 = 总消费次数/月份数 第一个需要注意的是：总消费次数的计算要排除同一天内，同一个用户的重复消费。 ''' 根据字段（用户编号，销售时间），如果两列值同时相同，只保留1条，将重复的数据删除 ''' kpi1_df = sub_trade_data.drop_duplicates(subset= ['用户编号','销售时间']) total_times = kpi1_df.shape[0] 第二个需要注意的是：计算月份数，通过(销售时间的最大值-销售时间的最小值).days得到天数，再用天数整除30得到月份数。 销售时间的最值可以在排序数据后获取第一个和最后一个销售时间，也可以对销售时间列应用max()和min()函数得到。 #方法一，先排序再获取最值 # startTime = kpi1_df.loc[0,'销售时间'] # endTime = kpi1_df.loc[total_times-1,'销售时间'] # 方法二，使用函数 startTime = min(kpi1_df['销售时间']) endTime = max(kpi1_df['销售时间']) days = (endTime - startTime).days total_months = days // 30 业务指标2：最畅销的10个商品 计算步骤： 1 . 根据商品编号进行分组，统计不同商品的销量 2 . 根据销量排序，找出销量前10的商品 在步骤一中需要注意，统计的是商品的总销量，而不是销量次数，所以应该应用sum()函数，而不是count()函数 ''' 不同商品的销售量：按商品编号分组，统计每个商品的销售数量 ''' item_total_times = sub_trade_data.groupby(['商品编号']).sum()['销售数量'] 在步骤二中注意，在排序前，需要将分组后的数据转成dataframe结构，否则用sort_values()排序要报错。 ''' 降序排序，取销售数量前10 ''' item_total_times = pd.DataFrame(item_total_times) item_total_times = item_total_times.sort_values(by=['销售数量'],ascending=False)[:10] 业务指标3：每类商品的月均销量 = 各类别商品的销售总量 / 月份数 计算步骤： 1 . 先求各个类别商品的消费总次数：按商品种类字段分组，统计每个类别的销售总量 2 . 月份数在业务指标1中已计算，根据公式求每类商品的月均销量 同样，在步骤一中注意，统计的是每种类别商品的总销量，不是销售次数。 业务指标4：每类商品的月销量趋势 计算步骤： 1 . 先利用销售时间字段创建一个新的销售月份字段 2 . 再按商品种类和销售月份进行分组，统计各个商品种类的月销量 3 . 根据月销量值绘制每类商品的月销量趋势折线图 在步骤一中注意，创建销售时间的月份字段时，直接使用.month属性来提取月份，会报错AttributeError: ‘Series’ object has no attribute 'month'； 如果运行的是最新版本的pandas，那么解决办法是，可使用datetime属性dt来访问datetime组件 ''' 创建销售月份字段 ''' sub_trade_data['销售月份'] = sub_trade_data['销售时间'].dt.month 如果运行的是旧版本pandas，可以用以下办法： sub_trade_data['销售月份'] = sub_trade_data['销售时间'].apply(lambda x: x.month) 步骤二中，是按商品种类和销售月份两个字段进行分组，并且统计的也是总销量。 cate_sale_month = sub_trade_data.groupby(['商品种类','销售月份']).sum()['销售数量'] 这个指标是观察销量趋势，需要可视化每种类别商品的月销量折线图，这里暂不提可视化内容，放在后续。 业务指标5：每类商品的销量峰值是在哪个月 每个商品种类的月销量已由上个业务指标4得到，所以指标5的分析只需排序后提取销量峰值和对应的月份即可。 需要注意： 1 . 在排序前将数据转成dataframe格式 2 . 使用sort_values()函数排序，排序的目标是每种类别的月销量按从高到底排，那么by参数需要传入两个，一个是商品种类，另一个是销售数量。对于销售数量字段可直接传入，但是商品种类字段，不是cate_sale_month数据中的字段，而是index，因为之前按照商品种类进行的分组。所以这里需要使用cate_sale_month.index.names[0]来获取商品种类字段，并作为参数传入排序函数。 cate_sale_month = pd.DataFrame(cate_sale_month) cate_sale_sort = cate_sale_month.sort_values(by=[cate_sale_month.index.names[0],'销售数量'],ascending=False) 3 . 因为每个类别都有12个月份的销量信息，所以排序后，每隔12行提取一个销量，即为每个类别的销量峰值。 # 提取每个类别的销量峰值 cate_max_sales = [] for i in range(0,len(cate_sale_sort),12): cate_max_sales.append(cate_sale_sort.iloc[i,0]) # 根据销量峰值查询峰值对应的商品种类和月份 top_month = cate_sale_sort.query('销售数量 == {}'.format(cate_max_sales)) 业务指标6：每类商品的用户画像（年龄段，性别） 分析步骤： 1 . 先将交易数据表和婴儿信息数据表合并 2 . 分析购买每类商品的用户性别分布情况 3 . 分析购买每类商品的用户年龄分布情况 步骤一，合并数据集，注意交易数据集有两万多条，而婴儿信息数据只有930条，所以合并时主要以婴儿信息数据表为主。 combined_data = pd.merge(sub_trade_data,baby_data,how='right') 步骤二，分析购买每类商品的婴儿性别分布情况，先根据商品种类和性别字段对数据进行分组，并使用sum()函数统计总销售数量。 cate_baby_sex = combined_data.groupby(['商品种类','性别']).sum()['销售数量'] 第二步就是排序问题，与业务指标5中相同，需要注意排序传入的参数。这里排序目标是每种类别下不同用户性别的购买量按从高到底排，那么by参数需要传入两个，一个是商品种类，另一个是销售数量。对于销售数量字段可直接传入，但是商品种类字段，不是cate_baby_sex数据中的字段，而是index，因为之前按照商品种类进行的分组。所以这里需要使用cate_baby_sex.index.names[0]来获取商品种类字段，并作为参数传入排序函数。 cate_baby_sex = pd.DataFrame(cate_baby_sex) cate_baby_sex = cate_baby_sex.sort_values(by=[cate_baby_sex.index.names[0],'销售数量'], ascending=False) 步骤三，分析购买每类商品的用户年龄分布情况 计算步骤： 1 . 首先计算婴儿的年龄,增加婴儿年龄列 2 . 将年龄进行分段，增加年龄区间列 3 . 统计不同类商品在不同年龄段的销售情况 步骤一，计算婴儿的年龄：由购买日期（销售时间）- 婴儿出生日期得到年份差值，从而得到购买商品时婴儿的年龄。 这里要注意的问题与业务指标4相同，即从时间中提取年份，直接使用.year属性来提取年份，会报错。这里使用datetime属性dt来访问datetime组件 combined_data['婴儿年龄'] = (combined_data['销售时间'].dt.year - combined_data['出生日期'].dt.year) 通过describe()查看 婴儿年龄字段的统计信息，发现最大年龄为28岁，这明显不合理，是个异常值，所以删去。分析到这里也从侧面反映了，在数据分析流程中数据清洗的工作无处不在。 combined_data = combined_data[combined_data['婴儿年龄'] != 28] 步骤二，对年龄进行分段，并增加年龄区间列。注意区间临界值，可以根据婴儿年龄字段的5个分位数（最小值，3个分位数，最大值）来定，例如通过describe()查看到婴儿年龄分位数信息如下： 那么，分段临界值就可取bin_edges = [-3,0,1,2,3,4,5,12]，同时指明每个年龄段的区间名bin_labels = ['未出生','1岁','2岁','3岁','4岁','5岁','5岁以上']，之后再用pandas的cut函数进行分段： combined_data['年龄区间'] = pd.cut(combined_data['婴儿年龄'], bin_edges, labels=bin_labels) # 确认所有年龄都被分到正确区间 combined_data['年龄区间'].isnull().sum() # 0 最后一步，统计每种商品的用户年龄分布情况。除了需要注意传入排序函数的参数外，还需要注意，按照商品种类和年龄区间分组后，可能存在某些年龄段，销售数量为空的情况，所以需要对空值进行填充。 cate_baby_age = combined_data.groupby(['商品种类','年龄区间']).sum()['销售数量'] # 存在某些年龄段，销售数量为空，将其填充为0 cate_baby_age = cate_baby_age.fillna(0) # 排序得到，各类商品在哪个年龄段最畅销 cate_baby_age = pd.DataFrame(cate_baby_age) cate_baby_age = cate_baby_age.sort_values(by=[cate_baby_age.index.names[0],'销售数量'],ascending=False)]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>data_analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NumPy与Pandas的查漏补缺]]></title>
    <url>%2F2019-05-07%2F</url>
    <content type="text"><![CDATA[使用 NumPy 而不是列表的原因 NumPy是Scipy，Pandas的基础库，它提供的数据结构是Python数据分析的基础。 在Python本身的列表中，保存的是对象的指针。 比如，存一个简单数组[0,1,2]，就需要3个指针和3个整数对象，比较费内存和计算时间。且因为list的元素在系统内存中是 分散的，而NumPy数组存储在一个 均匀连续的内存块中。当遍历所有数组元素时，list需要对内存地址进行查找，NumPy不需要，就节省了计算资源。 NumPy 更简洁 在 NumPy 中读写元素更快 NumPy使用起来更方便，可以自由使用很多向量和矩阵运算—&gt;可采用多线程的方式，充分利用多核CPU计算资源 NumPy可以更高效地工作，因为它们的实现更高效。 这里顺便提一个提升内存和提高计算资源利用率的技巧： 避免采用隐式拷贝，而采用就地操作方式。 比如：让 x 的值为原来的两倍，直接写成 x *= 2，而不要写成 y = x*2 NumPy NumPy中的ndarray对象数组的维数称为秩rank，一维数组的秩为1，二维数组的秩为2，以此类推。每一个线性的数组称为一个轴axes，秩描述的就是轴的数量。 初始化NumPy 数组 若只是想创建一个空数组： import numpy numpy.array([]) 若想初始化一个数组，以下几种方式： numpy.zeros(shape=(4,2)) numpy.ones(3) numpy.empty(shape=(0,0)) 其他初始化 NumPy 数组的方法可参考这里 查看数组的大小 用.shape，它返回一个元组，元组元素的数量表示数组的秩（维度），元组元素的值表示在该维度上数组的元素个数。 查看元素的属性 .dtype，可利用dtype来创建 结构数组： import numpy as np persontype = np.dtype({ 'names': ['name','site'], 'formats': ['S32','S32'] }) person = np.array([('ipine','https://ipine.me/')], dtype=persontype) name = person[:]['name'] print(name) # [b'ipine'] 访问数组元素 对于一维数组，可以通过三种方式，一是用索引；二是用切片，切片要注意含前不含后原则；三是利用循环遍历数组的每个元素。 对于二维数组，有三种情况，一是查看具体的某一个元素，使用行列号，如a[1,2]；二是查看某一行的元素，如a[0,:]；三是查看某一列的元素，如a[:,0] Universal Function运算 NumPy方便快捷的原因就是它提供了很多ufunc函数，这些函数都是采用C语言实现，计算速度非常快。 1 . 统计函数 在数据分析中，理解数据，查看数据的统计信息至关重要。通过对数据进行描述性统计分析，能够对数据有更清晰的认识。 一些常见的统计函数如下： 计算数组或者矩阵中的最大值，用amax()；最小值用amin()；第二个可选参数指定轴，0代表行，1代表列；即统计行或者列的最值。 统计最大值与最小值之差，用ptp()；第二个可选参数指定轴，0代表行，1代表列；即统计行或者列的最值之差。 统计数组的百分位数，用percentile()，可指定第p个百分位数，p取值范围为0-100，p=0求最小值，p=100求最大值；同样也可指定轴。 统计数组中的中位数，用median()；平均数，用mean()。 统计数组中的加权平均值，用average(arr, weights)，传入一个权重设置参数。 统计数组中的标准差，用std()，方差用var()；方差是每个数值与平均值之差的平方和的均值，标准差是方差的算术平方根，表示的是一组数据的离散程度。 一个栗子：计算列表分位数 分位数对于数据集的summary是必不可少的，临近数据集的最小值和最大值。包括第25、50和75百分位数，它们也被称为第一分位数、中位数和第三分位数。这意味着总共需要5个数字来总结整个数据集：最小值、最大值、中值和两个四分位数（第一和第三）。 假设有25个元素，那么第一个四分位为：0.25*25 = 6.25，四舍五入到7；即第一分位数就是列表排序后的第7个元素 import numpy as np a = np.array([1,2,3,4,5]) p = np.percentile(a, 50) # 求中位数 2 . 创建连续数组 使用 arange() 和 linspace()可以很方便地创建连续数组： x1 = np.arange(1,11,2) # 不包含终值，第三个参数指定步长 x2 = np.linspace(1,9,5) # 包含终值，第三个参数指定元素个数；在指定间隔返回均匀间隔的数字 3 . 算术运算 包括加add，减subtract，乘multiply，除divide，求幂power，取余remainder/mod等 x1 = np.arange(1,11,2) x2 = np.linspace(1,9,5) print(np.add(x1, x2)) print(np.subtract(x1, x2)) print(np.multiply(x1, x2)) print(np.divide(x1, x2)) print(np.power(x1, x2)) print(np.remainder(x1, x2)) 4 . 排序 排序算法使用频率高，在数据分析中也常用。在 NumPy中，实现排序算法非常容易。 只需使用 sort()这一条语句： 语法说明：sort(a, axis = -1, kind=‘quicksort’, order=None)； axis默认值为-1，沿着数组的最后一个轴进行排序，为0表示按列排，为1表示按行排，若为None则是采用扁平化方式排序。默认使用快排quicksort，kind的值还可以为合并排序mergesort，堆排序heapsort。对于结构化的数组，order字段可以指定按照某个字段进行排序。 a = np.array([[4,3,2],[2,4,1]]) print(np.sort(a)) print(np.sort(a, axis=None)) print(np.sort(a, axis=0)) print(np.sort(a, axis=1)) Pandas Pandas的使用频率非常高，是数据分析的利器。 一方面，Pandas提供的DataFrame与json契合度高，且相比于NumPy的二维数组，DataFrame更利于表示像Excel中的数据，每一列都可以是不同的类型； 另一方面，当数据清理工作不是特别复杂时，几句Pandas代码就可以对数据进行规整。 数据结构Series和DataFrame Pandas的一维数组series比NumPy的一维数组array功能更多，series是建立在NumPy基础之上的。 它们的主要区别在于series有索引，可以在定义时使用index来指定。 栗子，定义： import pandas as pd data1 = pd.Series(data=[1,2,3,4], index=['a', 'b', 'c', 'd']) # 或者 d = {'a':1, 'b':2, 'c':3, 'd':4} data2 = pd.Series(d) NumPy数组中每个元素都是同一个类型，虽然在科学计算中很快，但是不利于表示像excel中的数据。 Pandas的二维数组DataFrame，类似于数据库表， 具有一维数组series中的index（行）索引功能，此外也有columns（列）索引值。 栗子，定义： d = {'Chinese': [68, 88, 90], 'Math': [70, 66, 89]} df1 = pd.DataFrame(d) #或者 df2 = pd.DataFrame(d, index=['ipine','Alex','Bob'], columns=['Chinese', 'Math']) 直接在数据框中传入定义的字典，产生的结果与定义的字典顺序不一致，因为字典是无序的数据结构。 可以在将字典传入数据框之前，定义一个有序字典，使字典输出顺序与定义时一样： from collections import OrderedDict orderedd = OrderedDict(d) df3 = pd.DataFrame(orderedd) 查看值 两种方式，根据位置获取值，iloc，根据索引获取值，loc： # 对于一维数组data2 data2.iloc[0] # 1 data2.iloc['a'] # 1 # 对于二维数组 df3 df3.iloc[0,1] #查询第1行第2列的元素，70 df3.iloc[0,:] #查询第1行所有元素，68,70 df3.iloc[:,1] #查询第1列所有元素，68,88,90 df3.loc[0,'Math'] #查询第1行第2列的元素，70 df3.loc[0,:] #查询第1行所有元素，这里index没有指定所以默认为从0开始的数字，68,70 df3.loc[:,'Math'] #查询Math列所有元素，68,88,90 # 简单方法 df3['Math'] 数据框DataFrame还涉及到复杂查询 利用切片功能和条件判断 通过列表选择某几列的数据 df3[['Chinese','Math']] 通过切片功能，获取指定范围的列 df3.loc[:, 'Chinese':] 通过条件判断筛选，首先构建查询条件(得到布尔值)，然后利用布尔索引筛选： querydf = df3.loc[:, 'Math'] &gt;= 70 df3.loc[querydf, ['Math']] # 70 89 数据统计 Pandas也有许多统计函数可调用。常用的包括但不限于： 计算数据量的count()，空值和NaN不计 查看数据的前几行或后几行，head()和 tail() 查看某一列的数据类型，dtype 查看行列数，shape 获取多个统计指标的describe() 获取最大最小值的max()与min()， 求和，sum() 求中值和均值，median()和mean() 求标准差和方差，std()和var 统计最小最大值的索引位置，argmin() 和 argmax() 统计最小最大值的索引值，idxmin() 和 idxmax() 如果是数据框，Pandas会自动按列计算。 数据清洗 删除列或者行：df.drop(columns=['Chinese']) 重命名列：df.rename(columns={'Chinese': 'Yuwen'}, inplace=True) 去重：df.drop_duplicates() 格式问题,常见的包括： 1 . 更改数据列格式，df['Chinese'].astype('str') 2 . 数据间空格的删除，转成str类型的格式，便于操作，df['Chinese'] = df['Chinese'].map(str.strip) 3 . 删除特殊符号，同样使用 strip函数；例如Math字段有#，删除它，df['Math'] = df['Math'].str.strip('#') 4 . 格式转换，经常需要将字段名统一大小写，可直接使用upper()，lower()，title()等函数；例如，转成大写，df.columns = df.columns.str.upper() 5 . 查找空值和对空值的处理，经常有字段存在空值，通过isnull()函数查找空值，fillna(value)函数将空值替换成其他值，dropna()函数删除空值；例如，df.isnull() 查找整个表的空值；df.isnull().any()查看哪些列存在空值 注：在pandas更新到最新版本0.24.2时，慎用fillna()填充缺失值，尤其是填充一些字符串时，虽然这一列不是dtype='category'，但是也可能会出现ValueError: fill value must be in categories的问题。如果项目在pandas的0.24.2版本中，报这个错误，最简单的方法是将fillna()更换为inplace()函数。 应用apply函数 若想将字段名格式统一，也可以应用apply函数： df['name'] = df['name'].apply(str.upper) 当然，最常用的是定义一个函数，在apply中使用，例如： def double(x): return x*2 df['Math'] = df['Math'].apply(double) 还可以定义更复杂的函数，比如，在df中新增1列new，为语文和数学成绩之和的n倍： def plusMathChinese(df,n): df['new'] = (df['Chinese'] + df['Math']) * n return df df = df.apply(plusMathChinese, axis=1, args=(2,)) #axis=1指明按照列为轴进行操作；args是传递参数 数据表的合并 两个DataFrame数据框的合并，就像两张数据表的合并，使用的是 merge()函数； 与SQL中表连接类似，包含以下5种形式： 1 . 按指定列进行连接 df3 = pd.merge(df1, df2, on='name') 2 . inner内连接，求两个数据框的交集，是合并时的默认选择 df3 = pd.merge(df1, df2, how='inner') 3 . 左连接，以第一个数据框为主 df3 = pd.merge(df1, df2, on='left') 4 . 右连接，以第二个数据框为主 df3 = pd.merge(df1, df2, on='right') 5 . 外连接，求两个数据框的并集 df3 = pd.merge(df1, df2, on='outer') 用SQL方式打开Pandas 在Python中可以直接用SQL语句来操作Pandas。 利用工具 pandasql，它的主要函数是 sqldf，该函数包含两个参数： 一是，SQL查询语句。 二是，一组环境变量 globals() 或 locals()。 举个栗子： 先在anaconda环境下安装pandasql包：conda install pandasql，然后在jupyter notebook上运行： import pandas as pd from pandasql import sqldf, load_meat, load_births df1 = pd.DataFrame({'name':['ZhangFei', 'GuanYu', 'a', 'b', 'c'], 'data1':range(5)}) sql = &quot;select * from df1 where name ='ZhangFei'&quot; pysqldf = lambda sql: sqldf(sql, globals()) print(pysqldf(sql)) # 查询到`ZhangFei`的data1为0]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>data_analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[易忘易忽略的Python入门知识点-续(二)]]></title>
    <url>%2F2019-05-06%2F</url>
    <content type="text"><![CDATA[模块与包 模块 模块是一个包含所有你定义的函数和变量的文件，其后缀名是.py。 可以被别的程序引入，以使用该模块中的函数等功能。这也是使用 python 标准库的方法。 一个模块只会被导入一次，不管你执行了多少次import。 Python的搜索路径，搜索路径是由一系列目录名组成的，Python解释器就依次从这些目录中去寻找所引入的模块。 搜索路径被存储在sys模块中的path变量 import sys print(sys.path) #输出是一个列表，其中第一项是当前目录 import语句：导入整个模块 from... import语句：从模块中导入指定的部分到当前命名空间中。这种导入方法不会把被导入的模块的名称放在当前的字符表中。 from fibo import fib, fib2 # fibo这个名称在当前命名空间中没有定义 from...import *语句：从模块中导入所有项目，不推荐使用。这种方法会导入所有名字（除了单一下划线开头的），这很可能会覆盖当前文件中已有的定义。 每个模块都有一个__name__属性，当其值是__main__时，表明该模块自身在运行，否则是被引入。 内置的函数 dir()可以找到模块内定义的所有名称。以一个字符串列表的形式返回： import fibo a = [1, 2, 3, 4, 5] fib = fibo.fib dir() # 得到当前模块中定义的属性列表 ['__builtins__', '__name__', 'a', 'fib', 'fibo', 'sys'] 包 包实际是从一个目录中引用模块，目录结构中必须带有一个 __init__.py文件。 可以采用from package import item方式，item可以是包的子模块，或者包里的其他名称，如函数，类，变量名； 也可以采用 import item.subitem.subsubitem这种方式，除了最后一项可以是模块或者包外（不能是函数，类，变量名），其余都必须是包。 导入语句遵循如下规则： 如果包定义文件 __init__.py 存在一个叫做 __all__ 的列表变量，那么在使用 from package import *的时候就把这个列表中的所有名字作为包内容导入。 如果__all__没有定义，那么使用from sound.effects import *这种语法的时候，就不会导入包sound.effects里的任何子模块。他只是把包sound.effects和它里面定义的所有内容导入进来（可能运行__init__.py里定义的初始化代码）。这会把 __init__.py里面定义的所有名字导入进来。 输入与输出 输出格式 1 . 想将输出的值转成字符串，可以使用 repr() 或 str() 函数来实现。 x = 10 * 3.25 y = 200 * 200 s = 'x 的值为： ' + repr(x) + ', y 的值为：' + repr(y) + '...' print(s) # x 的值为： 32.5, y 的值为：40000... 2 . 字符串对象的 rjust() 方法, 它可以将字符串靠右, 并在左边填充空格。还有类似的方法, 如 ljust() 和 center()。 3 . 另一个方法 zfill(), 它会在数字的左边填充 0 print('12'.zfill(5)) # 00012 4 . str.format()的基本使用 print('{0} site： &quot;{1}&quot;'.format('ipine', 'https://ipine.me/')) 在括号中的数字用于指向传入对象在 format()中的位置； 也可以使用关键字参数： print('{name} site： &quot;{site}&quot;'.format(name='ipine', site='https://ipine.me/')) !a (使用 ascii()), !s (使用 str()) 和 !r (使用 repr()) 可以用于在格式化某个值之前对其进行转化: import math print('常量 PI 的值近似为： {!r}。'.format(math.pi)) 对值进行更好的格式化，可使用可选项 : 和格式标识符（如f, d等） import math print('常量 PI 的值近似为 {0:.3f}。'.format(math.pi)) # 常量 PI 的值近似为 3.142。 在 : 后传入一个整数, 可以保证该域至少有这么多的宽度。 读写文件 假设有一个叫做f的文件： f = open(&quot;filepath&quot;, &quot;r&quot;) 1 . 调用 f.read(size)读取一定数目的数据，当 size 被忽略了或者为负, 那么该文件的所有内容都将被读取并且返回。 str = f.read() print(str) 2 . f.readlines() 将返回该文件中包含的所有行。也可以通过迭代文件对象来读取每行： # 打开一个文件 f = open(&quot;filepath&quot;, &quot;r&quot;) for line in f: print(line, end='') # 关闭打开的文件 f.close() 更好的打开文件的方式是使用with语句，它可以保证文件f总是会关闭，即使在处理过程中出问题了。 with open(&quot;myfile.txt&quot;) as f: for line in f: print(line, end=&quot;&quot;) 3 . f.write(string)将 string 写入到文件中, 然后返回写入的字符数。 如果要写入一些不是字符串的东西, 那么需要用 str()进行转换后再写入。 4 . f.tell()返回文件对象当前所处的位置, 它是从文件开头开始算起的字节数。 5 . 如果要改变文件当前的位置, 可以使用 f.seek(offset, from_what)函数。 第二个参数from_what如果是 0 表示开头(默认情况), 如果是 1 表示当前位置, 2 表示文件的结尾，例如： seek(x,0) ： 从起始位置即文件首行首字符开始移动 x 个字符 seek(x,1) ： 表示从当前位置往后移动x个字符 seek(-x,2)： 表示从文件的结尾往前移动x个字符 6 . Pythonopen()方法用于打开一个文件，使用该方法一定要保证关闭文件对象，即调用close()方法。 其完整语法格式为： open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None) 经常会遇到的读文件错误 Python3读CSV文件，出现 UnicodeDecodeError: ‘utf-8’ codec cant’t decode byte 0xd0 in position 0: invalid continuation byte 出现该错误原因：系统默认为UTF8编码，但文件不是UTF8编码。 解决方法： 一：修改文件对应的编码方式，以记事本打开CSV文件，在文件菜单中选择另存为，可以看到文件原来的保存类型是ASCII，在下拉框中选择UTF8编码。 二：从文件打开方式上解决 open(&quot;filename&quot;, encoding='ascii', errors='ignore') 异常处理 try... except语句工作方式：先执行try后面的子句，若没有异常发生，该子句执行后就结束（若有else子句，会执行完它后面的语句才结束），忽略except。若执行时发生异常，异常之后的try子句被忽略，将异常与except后的名称进行匹配，匹配成功后执行except后面的子句，最后执行try子句剩余的代码。若有finally子句，无论是否发生异常，该子句都会被执行。 一个 try 语句可能包含多个except子句，分别来处理不同的特定的异常。 最后一个except子句可以忽略异常的名称，它将被当作通配符使用。可使用它打印错误信息，并把异常抛出： import sys try: f = open('myfile.txt') s = f.readline() i = int(s.strip()) except OSError as err: print(&quot;OS error: {0}&quot;.format(err)) except ValueError: print(&quot;Could not convert data to an integer.&quot;) except: print(&quot;Unexpected error:&quot;, sys.exc_info()[0]) raise 抛出异常，使用raise语句，后面的参数指明要抛出的异常，该异常必须是一个异常的实例或者是Exception的子类 面向对象 1 . 类有一个名为 __init__()的特殊方法（构造方法），该方法 在类实例化时会自动调用。 2 . 类的方法与普通的函数只有一个特别的区别——它们必须有一个额外的第一个参数名称, 按照惯例它的名称是self。 self 代表的是类的实例，代表当前对象的地址，而 self.class 则指向类。 class Test: def prt(self): print(self) # &lt;__main__.Test instance at 0x100771878&gt; print(self.__class__) # __main__.Test t = Test() t.prt() 3 . 单继承与多继承时，基类必须与派生类定义在一个作用域内。 除了类，还可以用表达式，基类定义在另一个模块中时这一点非常有用 class DerivedClassName(modname.BaseClassName): 4 . 多继承时需要注意圆括号中父类的顺序，若是父类中有相同的方法名，而在子类使用时未指定，python从左至右搜索 — 即方法在子类中未找到时，从左到右查找父类中是否包含方法。 5 . 继承时，若子类对父类的方法重写了，子类实例化，调用该方法时是调用重写方法；若想用子类对象调用父类已被覆盖的方法，使用 super()函数 6 . 类的私有变量和私有方法都是以两个下划线开头，类的实例 不能访问类的私有变量也 不能调用类的私有方法。 标准库 1 . os（操作系统）模块与sys（命令行参数）模块的不同： os模块负责程序与操作系统的交互，提供了访问操作系统底层的接口； sys模块负责程序与Python解释器的交互，提供了一系列的函数和变量供用户操作Python运行时的环境。 2 . timeit模块，可以查看不同方法的性能差异。 例如：使用元组封装和拆封来交换元素看起来要比使用传统的方法要诱人的多,timeit 证明了现代的方法更快一些。 from timeit import Timer print(Timer('t=a; a=b; b=t', 'a=1; b=2').timeit()) # 0.20183640400000002 print(Timer('a,b = b,a', 'a=1; b=2').timeit()) # 0.15459948599999995 3 . 此外还有： 文件通配符模块glob，用于从目录通配符搜索中生成文件列表； 字符串正则匹配模块re，为复杂的匹配和处理提供简介的方法； 数学模块math，调用一些底层C数学函数方便运算；常用的random模块，用于生成随机数； 访问互联网的模块，最常见的是处理从urls接收的数据的 urllib.request，以及用于发送电子邮件的 smtplib； 日期和时间模块 datetime，可以更有效地处理和格式化输出； 数据压缩模块，zlib， gzip， bz2， zipfile等； 测试模块doctest， 扫描模块并根据程序中内嵌的文档字符串执行测试； 参考 Python菜鸟教程]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>data_analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[易忘易忽略的Python入门知识点-续(一)]]></title>
    <url>%2F2019-05-05%2F</url>
    <content type="text"><![CDATA[迭代器与生成器 迭代器 迭代是Python最强大的功能之一，是访问集合元素的一种方式。 迭代器是一个可以记住遍历位置的对象。迭代器对象从集合的第一个元素开始访问，直到所有的元素被访问完结束，它只能往前不会后退。 迭代器，常用的两个方法iter()和next()； import os list1 = [1,2,3,4] myiter = iter(list1) # 生成迭代器对象 # for ele in myiter: # print(next(ele)) while True: try: print(next(ele)) except StopIteration: os._exit(1) 当抛出StopIteration异常时，调用模块sys的exit()方法，会出现 SystemExit exception raised from sys.exit()；解决方法是使用调用 os._exit() ,它直接退出，不会抛出异常。参数是进程返回的退出码。 应用栗子：如何将列表分隔成大小均匀的块？ 一个方法是结合使用 zip()和 iter()函数： x = [1,2,3,4,5,6,7,8,9] y = zip(*[iter(x)]*2) # 2是表示每个块的大小 list(y) # [(1, 2), (3, 4), (5, 6), (7, 8)] 过程理解： iter()是序列上的迭代器 [iter(x)] * 2生成一个包含2个listiterator对象的列表：每个列表迭代器都是x的一个迭代器。 在将序列解压缩为参数之前传递给zip()函数的*，是为了将相同的迭代器传递给zip()函数4次，每次从迭代器中提取一个项。 具体步骤： 首先，会有2个列表迭代对象，就是原来相同的2个列表：[1,2,3,4,5,6,7,8,9]，[1,2,3,4,5,6,7,8,9] 然后， 第一次，zip()将按顺序接受列表中的一个元素，[1][2] 注意：迭代对象会保留迭代器中下一个元素的位置 第二次，元素将被添加到刚刚创建的2个列表中，最终将得到：[1, 3], [2,4] 第三次，执行相同的过程，最终得到：[1, 3, 5], [2, 4, 6] 第四次，执行相同的过程，最终得到：[1, 3, 5, 7], [2, 4, 6, 8] 最后，zip 将这三个列表压缩在一起，得到：(1, 2), (3, 4), (5, 6), (7, 8) 可以自己创建迭代器，将一个类作为一个迭代器，需要实现两个方法: class MyNumbers: def __iter__(self): self.a = 1 return self def __next__(self): if self.a &lt;= 5: x = self.a self.a += 1 return x else: raise StopIteration #StopIteration 异常用于标识迭代的完成，防止出现无限循环的情况 myclass = MyNumbers() myiter = iter(myclass) for x in myiter: print(x) 生成器 在 Python 中，使用了 yield的函数被称为生成器（generator）。 生成器是一个返回迭代器的函数，只能用于迭代操作，更简单点理解生成器就是一个迭代器。 在调用生成器运行的过程中，每次遇到 yield时函数会暂停并保存当前所有的运行信息，返回 yield的值, 并在下一次执行next()方法时从当前位置继续运行。 调用一个生成器函数，返回一个迭代器对象 应用栗子1：用yield实现斐波那契数列 import os def fibonacci(n): a,b,count = 0,1,0 while True: if count &gt; n: return yield a a,b = b, a+b count += 1 f = fibonacci(10) while True: try: print(next(f), end=' ') except StopIteration: os._exit(1) 应用栗子2：用yield实现将列表分隔成大小均匀的块 def chunks(list, chunkSize): for i in range(0, len(list), chunkSize): yield list[i:i + chunkSize] 函数 1 . 函数参数传递 在 python 中，类型属于对象，变量是没有类型的，例如： a=[1,2,3] a=&quot;ipine&quot; 以上代码中，[1,2,3] 是List类型，“ipine” 是 String 类型，而变量 a 是没有类型，它仅仅是一个 对象的引用（一个指针），可以是指向 List 类型对象，也可以是指向 String 类型对象。 再次提到 可变 VS 不可变对象 strings，tuples和numbers不可变；list，dict等可变 不可变类型：变量赋值 a=5 后再赋值 a=10，这里实际是新生成一个 int 值对象 10，再让a指向它，而 5 被丢弃，不是改变a的值，相当于 新生成了a。 可变类型：变量赋值 la=[1,2,3,4] 后再赋值 la[2]=5 则是将 list la 的第三个元素值更改，本身la没有动，只是其内部的一部分值被修改了。 函数的参数传递分为 可变与不可变类型 不可变类型：类似 c++ 的值传递，如 整数、字符串、元组。如fun（a），传递的只是a的值，没有影响a对象本身。比如在 fun（a）内部修改 a 的值，只是修改另一个复制的对象，不会影响 a 本身。 可变类型：类似 c++ 的引用传递，如 列表，字典。如 fun（la），则是将 la 真正的传过去，修改后fun外部的la也会受影响。 2 . 参数的类型 必需（位置）参数，必需参数强调参数顺序。调用时的数量和位置必须和声明时的一样。 关键字参数，使用关键字参数来确定传入的参数值，对参数顺序不敏感，通过参数名匹配参数值。 默认参数，若调用没有传递参数，则使用默认参数。 不定长参数，不确定调用时传入几个参数，那声明参数时不命名。两种形式，一个星号的参数和两个星号的参数： *args: 表示参数个数不确定，且想传入元组或列表形式的参数时使用；一个星号将序列或集合解包成位置参数 **kwargs: 表示参数个数不确定，且想传入字典的值作为关键字参数时使用；两个星号把字典解包成关键字参数 声明函数时，参数中星号*可以单独出现，但是星号后面的参数，必须用 关键字传入。例如： def f(a,b,*,c): return a+b+c f(1,2,3) #报错 f(1,2, c=3) #正确 3 . 匿名函数 lambda是一个表达式，不是一个代码块。它不能访问 自己参数列表之外或全局命名空间里的参数。 4 . 不带参数值的return语句返回的是None；如果函数没有使用 return 语句，则函数返回 None。 5 . 变量作用域 Python的作用域有4种，分别是： L （Local） 局部作用域 E （Enclosing） 闭包函数外的函数中 G （Global） 全局作用域 B （Built-in） 内置作用域（内置函数所在模块的范围） 查找的规则是：在局部找不到，去局部外的局部找（闭包），再找不到就全局找，最后再去内置找。 g_count = 0 # 全局作用域 def outer(): o_count = 1 # 闭包函数外的函数中 def inner(): i_count = 2 # 局部作用域 内置作用域是通过一个名为builtins 的标准模块来实现的, 必须导入这个文件才能够使用它。 import builtins print(dir(builtins)) 模块、类、函数（包括lambda表达式）会引入新的作用域，其他代码块不会。 6 . global和nonlocal关键字 当内部作用域想修改外部作用域的变量时，需要用global和nonlocal关键字。 global关键字用于修改全局作用域的变量，例如： num = 1 def fun1(): global num # 需要使用 global 关键字声明 print(num) # 1 num = 123 print(num) # 123 fun1() print(num) # 123，已经将全局变量的值修改了 nonlocal关键字用于修改嵌套（enclosing）作用域，例如： def outer(): num = 10 def inner(): nonlocal num # nonlocal关键字声明 num = 100 print(num) # 100 inner() print(num) # 100 outer() 一种特殊情况，函数使用全局作用域的变量，如下一段代码： a = 10 def test(): a = a + 1 print(a) test(a) 会抛出 局部作用域引用错误，因为test 函数中的a 使用的是局部变量，未定义，无法修改。 正确应该是： a = 10 def test(a): a = a + 1 print(a) test(a) # 11 print(a) # 10，传递参数类型是不可变类型，所以只是值传递 数据结构 1 . 列表的clear()方法，用于移除列表中的所有项，等于del a[:]。 使用 del 语句可以从一个列表中依索引而不是值来删除一个元素；也可以使用它传入key来删除字典元素。 2 . 可以用花括号{}创建集合。注意：如果要创建一个空集合，你必须用 set() 而不是{}；后者创建一个空的字典。集合的功能包括 成员关系检查和 消除重复元素。 3 . 选择正确的内置功能。 当遍历列表既要访问索引又要访问值时，使用enumerate()而不是range()进行迭代。 对于每个元素，enumerate()返回一个计数器和元素值。计数器默认为0，也是元素的索引。不想在0开始计数，只需使用可选的start参数来设置偏移量： numbers = [45, 22, 14] for i, num in enumerate(numbers, start=52): print(i, num) 当遍历字典时，使用 items() 将关键字和对应的值同时解读出来： knights = {'gallahad': 'the pure', 'robin': 'the brave'} for k, v in knights.items(): print(k, v) 同时遍历两个或更多的序列，使用 zip() 组合： questions = ['name', 'favorite color'] answers = ['ipine', 'red'] for q, a in zip(questions, answers): print(f'What is your {q}? It is {a}.') # print('What is your {0}? It is {1}.'.format(q, a)) 按顺序遍历序列，使用 sorted()函数返回有序序列，不改变原序列： basket = ['apple', 'orange', 'apple', 'pear', 'orange', 'banana'] for f in sorted(set(basket)): print(f) print(basket) 反向遍历一个序列，首先指定序列，然后调用reversed()函数： for i in reversed(range(1, 10, 2)): print(i,end=' ') # 9 7 5 3 1 参考 Python菜鸟教程]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>data_analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实习碰壁之后-正式入坑数据分析]]></title>
    <url>%2F2019-04-28%2F</url>
    <content type="text"><![CDATA[目前的状态 我是一名计算机专业的在读研究生，研究方向是数据可视化，可视分析。从实验室以及研究方向来看，周边大部分同学选择了前端开发，少部分也有做后台的。而我个人并不是特别愿意去做前端开发，我想做数据分析，数据挖掘相关的工作。目前，正是找实习的时候，但是我找数据分析的岗并不顺利，迟迟没有拿到实习offer，而周围那些找前端开发的同学早已盆满钵满。说不羡慕那是假的，但是能怎么样呢？路是自己选的，再哭也得走下去呀！有句话说，要做难且正确的事，我想我现在是在这条路上。 早在一年前就定了数据分析这个方向，但是自己平时学习就是看一些书籍，没有数据挖掘相关的实践项目，苦于没有带路人，也不知道岗位的具体要求。找实习的时候才发现，不同公司对数据分析、挖掘岗的要求都不一样，但大部分都要求对业务有了解，对挖掘算法熟悉，做过相关项目；所以尽管我投了很多简历，但是都石沉大海，我也像一个无头苍蝇，摸不到门道，到处碰壁。我之前做科研项目，写JavaScript比较多，后来自学数据分析课程，学习了Python，SQL和基本算法的知识，算是入门了数据分析。但是我想多做一些实践项目，更深入地学习某一个细分行业的数据分析和挖掘流程，为秋招求职助力。 未来想从事的具体行业 入门之后，选择一个方向，深入学习研究才可能成功，这跟做科研是一个道理。我在反思自己为什么找实习不顺时，发现一点就是自己的专业技能不够突出，前端会一点，数据分析刚入门，学习太过宽泛而都不够精专。公司更想招聘的一定是某一技能方面的“专家”，而不是什么都懂一点，什么都做不了的员工。所以有了这两个月找实习的经历，我意识到自己不能再这样一边焦虑未来，一边漫无目的地学习，更好的方式就是找到自己感兴趣的具体行业，然后实践。 虽然做过一个关于区块链加密货币交易数据的科研项目，但是我对金融行业的背景和业务知识知之甚少，没有系统学习过。与金融行业相比，互联网+电商行业的实践更为容易，我也有兴趣。所以，刚开始想从互联网+电商行业的数据分析、数据挖掘实践入手。从本科到研究生都是计算机专业的学生，读研期间学习的是数据分析流程中必不可少的一环—数据可视化，所以对自己从事这个方向的工作还是有信心的，不足的地方仍然是对行业的背景知识和业务知识了解不深入，这方面现在可以慢慢接触，在工作中积攒更多经验。 如何更近距离接触相关行业 找到兴趣行业，如何迈出了解行业的第一步呢？ 其实最简单的方式，就是去找一个这个行业的数据集，自己尝试着去探索数据。我从阿里天池下载了一个数据集，是用户在淘宝和天猫上购买婴儿用品的数据集，该数据包含两个文件，分别是淘宝会员历史婴儿用品交易样本数据文件，和婴儿信息样本数据文件。历史婴儿用品交易样本数据文件包含29971条数据，有以下7个字段： * user_id: 用户id * auction_id: 购买行为编号 * cat_id: 商品种类ID * cat1: 商品属于哪个类别 * property: 商品属性 * buy_mount: 购买数量 婴儿信息样本数据文件，包含953条，有以下3个字段： * user_id:用户id * birthday:出生日期 * gender:性别（0 男性；1 女性） 官方提供的典型分析主题包括： 根据父母的购买行为预测孩子的年龄； 根据孩子的信息(年龄、性别等)预测用户会购买什么样的商品。 在之后的文章中我会对这个数据集进行分析和探索。 学习规划 有了目标，自然少不了学习规划。有计划地学习，每完成一个阶段任务，都能获得一些学习成就感，激励自己继续向前。计划如下： 复习及巩固Python语言知识 使用Python的NumPy和Pandas包进行数据分析练习 机器学习算法相关的实践 学习如何撰写Python数据可视化和分析报告 推论统计相关的实践 完善简历，丰富项目经验，不定期]]></content>
      <categories>
        <category>method</category>
      </categories>
      <tags>
        <tag>data_analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[易忘易忽略的Python入门知识点]]></title>
    <url>%2F2019-05-04%2F</url>
    <content type="text"><![CDATA[最近在复习Python的基础知识，发现一些易忽略和忘记的点，这里做一些总结。 可改变 VS 不可改变 不可改变的数据类型包括：数值、字符串和元组；而可改变的数据类型包括：列表、字典、集合。需要注意字典的key必须为不可变类型；集合虽然是可变的但是它要求包含的元素是不可变类型。不可变类型换一种更官方的说法叫可Hashable，利用可Hashable的元素，检查成员关系时速度会更快。可通过collections模块的Hashable属性查看数据类型是否是不可变的： import collections print(isinstance({}, collections.Hashable)) # False 此处引出另外一个点，即isinstance()和type()的区别。type()常用于查看数据类型，isinstance()常用于判断数据类型；两者对于类的判断会稍有不同，isinstance()会将子类当作与父类一样的类型，而type()则认为子类与父类是不同的。 有序 VS 无序 有序的容器：元组、列表；无序的容器：字典、集合。 元组是不可变的有序列表，没有append()或extend()方法，也没有remove()或pop()方法；字典以键值对的形式存在，而列表只包含值；集合要求元素唯一不可变，列表可重复且可变。 提到append()或extend()方法，不得不说下它们之间的区别：extend()接受一个迭代器（列表、元组、集合、字符串等），一次一个地将迭代器中元素添加到列表中；而append()直接将迭代器当作一个对象。 容器的定义 列表的定义，元素类型可不同，可嵌套。创建方式： # 创建空列表 list1 = [] # 创建简单列表 list2 = ['ipine', 7, 'https://ipine.me/'] 元组的定义，需要注意空元组和只有一个值的情况： tup1 = () tup2 = (1,) 字典的定义，键唯一且不可变，值不必唯一可取任何数据类型。创建方式如下： # 创建空字典 dict1 = {} # 创建简单字典 dict2 = {'name': 'ipine', 'site': 'https://ipine.me/'} # 使用构造函数 dict() dict3 = dict([('name','ipine'),('site','https://ipine.me/')]) # 或者 dict4 = dict(name = 'ipine', site = 'https://ipine.me/') 集合的定义，集合的基本功能是删除重复元素和检查成员关系。创建方式如下： # 创建空集合,不能直接用空花括号，那是创建空字典 set1 = set() # 创建简单集合 set2 = {'ipine', 'Alex'} # 或者 set3 = set('ipine', 'Alex') 运算符 1.+用于连接字符串；*用于重复输出字符串；字符串在行尾使用反斜杠 \ ，表示续行；字符串前面加上字母 r/R，表示输出原始字符串。 2.is用于判断两个变量引用的是否是同一个对象，类似于 id(x) == id(y)，id用于获取对象内存地址; 而== 用于判断引用变量的值是否相等。 a = [1, 2, 3] b = a[:] print(b is a) # False print(b == a) # True 3.普通的格式化字符串，使用%s；若Python 3.6+，更好的格式化方法是使用f-strings def get_name_and_decades(name, age): return f&quot;My name is {name} and I'm {age / 10:.5f} decades old.&quot; 但要注意，如果是输出用户生成的值这种情况下，模板字符串可能是更安全的选择。 from string import Template def get_name_and_decades(name, age): a = Template(&quot;My name is ${key1} and I'm ${key2 / 10:.5f} decades old.&quot;) a.substitute(key1 = name, key2 = age) return a 从列表中选择元素 可以使用索引操作符 [] , 索引下标从0开始；获取最后一个元素，传入 -1；切片取多个值，遵循 含前不含后原则： a[start: end : step] # step默认取1，不会跳过任何元素 a[start : ] a[ : end] a[ : ] # 复制列表 从列表中任意选择一个元素 from random import choice list = ['a','b','c','d'] print(choice(list)) 利用索引任意选择一个元素 from random import randrange randomLetters = ['a','b', 'c', 'd'] randomIndex = randrange(0,len(randomLetters)) print(randomLetters[randomIndex]) 列表转其他数据结构 1.将列表转换成字符串： ''.join() listOfNumbers = [1, 2, 3] strOfNumbers = ''.join(str(n) for n in listOfNumbers) #对于数字要先转换成str print(strOfNumbers) 2.列表转元组：tuple() ; 列表转集合：set() 3.列表转字典：zip() helloWorld = ['hello','world','1','2'] # 转成字典 helloWorldDictionary = dict(zip(helloWorld[0::2], helloWorld[1::2])) print(helloWorldDictionary) # {'1': '2', 'hello': 'world'} # 解析成元组对后以列表形式输出 list(zip(helloWorld)) # [('hello',), ('world',), ('1',), ('2',)] 复制列表 方式很多，最常见的是切片方式：newList = oldList[ : ] ；使用内置函数：newList = list(oldList)；也可以使用库函数，这会涉及到浅拷贝和深拷贝的区分。浅拷贝：newList = copy.copy(oldList)；对于包含对象的列表，若想将这些对象也完全复制，用深拷贝：copy.deepcopy(oldList)。 注意：切片方式是浅拷贝。对于浅拷贝，改变对象，原来的列表也会改变 objectList = ['a','b',['ab','ba']] copiedList = objectList[:] copiedList[0] = 'c' copiedList[2][1] = 'd' print(objectList) # ['a', 'b', ['ab', 'd']] 列表解析的理解 列表解析是一种构建列表的优雅方法，它包含两种形式。 形式一： [i for i in range(k) if condition] 此时 if 起条件判断作用，满足条件的，将被返回成为最终生成的列表的一员。 形式二： [i if condition else exp for exp] 此时 if...else 被用来赋值，满足条件的 i 以及 else 被用来生成最终的列表。 举个栗子： print([i for i in range(10) if i%2 == 0]) # [0, 2, 4, 6, 8] print([i if i == 0 else 100 for i in range(10)]) # [0, 100, 100, 100, 100, 100, 100, 100, 100, 100] 列表排序 排序会改变原始列表，建议在原始列表不再使用的情况下使用此功能。列表排序有两种方式，一种是列表调用sort函数，list.sort()；另一种是往sorted函数传入列表，sorted(list) 。使用sorted对复杂列表排序，除了reverse参数外，还有个key参数，用于指定按哪个属性值排序，如下示例： sorted(animals, key=lambda animal: animal['age']) # animals是字典列表 有效利用数据结构 1.移除任何迭代器中的重复值，只需将其传递给内置的 set() 函数。迭代器可以是列表、字典等，查看一个变量是否是迭代器可以应用 .__iter__。 如果以后需要一个真正的列表，也可以类似地将 set 传递给 list() 函数。 duplicates = [1, 2, 3, 1, 2, 5, 6, 7, 8] print(list(set(duplicates))) # [1, 2, 3, 5, 6, 7, 8] 注意：使用了 set() 函数后，列表元素的顺序丢失。如果元素的顺序很重要，那么需要使用其他的机制：可参考这个博文 2.使用set存储唯一值。 举个栗子：假装你有一个名为get_random_word()的函数。 重复调用它以获取1000个随机单词，然后返回包含每个唯一单词的数据结构。 import random def get_random_word(words): return random.choice(words) 好的做法： def get_unique_words(words): words_set = set() for _ in range(1000): words_set.add(get_random_word(words)) return words_set 差的做法： def get_unique_words(words): words_set = [] for _ in range(1000): word = get_random_word(words) if word not in words_set: words_set.append(word) return words_set 这种不好的做法，必须将每个新单词与列表中已有的每个单词进行比较，时间复杂度O(n^2)；且在列表中检索元素比在集合中慢，集合元素是可 hashable 。集合存储元素的方式允许接近恒定时间检查值是否在集合中，而不像需要线性时间查找的列表。 3.使用 .get() 和 .setdefault() 在字典中定义默认值。 当需要添加，修改或检索可能在字典中或可能不在字典中的项: name = names.get('name', 'The Man with No Name') # 获取name的值，若name没值就返回后面参数的默认内容如果key存在，则返回对应的值。否则，返回设置的默认值。 但是，如果你仍想在访问name键时使用默认值更新字典，还是需要再次显式检查该值： if 'name' not in names: names['name'] = 'The Man with No Name' 更简洁的做法是使用.setdefault()，完成以上两个步骤。 name = names.setdefault('name', 'The Man with No Name') # 获取name的值，若name没值就将name的值设置为后面参数的默认内容 标准库的使用 1.使用collections包的 defaultdict() 处理缺少的字典键。为单个键设置默认值时，.get() 和 .setdefault() 可以正常工作，但通常需要为所有可能的未设置键设置默认值，这两种方式比较麻烦。更简洁的方法是使用defaultdict()方法。 举个栗子：统计字符串中字母出现的次数，数字或是其他符号不算。 常规做法：迭代字符串s，并检查字符是否已经是字典的key，如果不是，则将其添加到字典中，并将1作为默认值；如果是，则key对应值加1： def count_letter(s): dic = {} for ele in s: if ele.isalpha(): if ele in dic: dic[ele] +=1 else: dic[ele] = 1 return dic 更简洁做法： from collections import defaultdict def count_letter(s): dic = defaultdict(int) # int对应值为0 for ele in s: if ele.isalpha(): dic[ele] += 1 return dic 语法说明，dict =defaultdict(factory_function)，factory_function可以是list、set、str等等，作用是当key不存在时，返回的是工厂函数的默认值，比如list对应[ ]，str对应的是空字符串，set对应set( )，int对应0； 若想设置为某个value；那么可以传入lambda函数 dic = defaultdict(lambda: 1) 当然，统计迭代器中每个项出现的次数，可以直接使用 Counter() 函数； 若好奇最常见的一个项是什么，只需使用 .most_common()： from collections import Counter list = [&quot;a&quot;,&quot;b&quot;,&quot;b&quot;] counts = Counter(list) # Counter({'a': 1, 'b': 2}) counts.most_common(1) # b 2.使用collections包的deque()方法创建队列和栈数据结构。队列，先入先出的结构；栈，后入先出的结构，类似于浏览器的“后退”按钮。 定义队列，入队和出队操作： from collections import deque queue = deque([1, 3, 4, 7, 0]) queue.append(10) queue.popleft() 定义栈，入栈和出栈操作： stack = deque(['1st webpage','2nd webpage','3rd webpage']) stack.append('4th webpage') stack.pop() 3.使用collections包的排序字典方法 OrderedDict() ，使输出的字典顺序与key的输入顺序一致。 from collections import OrderedDict order_dict = OrderedDict({'first':1, 'second':2, 'third': 3}) 4.使用 itertool 生成排列和组合： itertools.permutations() 和 itertools.combinations()。 排列是考虑了顺序的，(A,B)和(B,A)不一样；组合是没考虑顺序，(A,B)和(B,A)一样。 import itertools friends = ['A', 'B', 'C', 'D'] list(itertools.permutations(friends, r=2)) # r指定每个分组有几个值 list(itertools.combinations(friends, r=2)) 5.使用operator模块中的 add() 方法，更机智地对列表元素求和。 from operator import add list(map(add, list1, list2)) # map每个元素；最后记得使用 list()打印 map()函数的结果 对两个列表的元素求和，也可以在列表解析中使用 zip() 函数 [sum(x) for x in zip(list1, list2)] 6.使用functools包的 reduce() 方法，将嵌套列表变成flat list。 from functools import reduce listOfList = [[1,2], [3,4], [5,6]] print(reduce(lambda x,y: x+y,listOfLists)) # reduce需传入两个参数，前一个是lambda表达式，后一个是列表； +操作符是连接列表的符号 也可以使用 sum() 函数： list = [[1,2],[3,4],[5,6]] sum(list, []) # [1, 2, 3, 4, 5, 6]]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>data_analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[散列表]]></title>
    <url>%2F2018-12-18%2F</url>
    <content type="text"><![CDATA[散列表效率 之前讲到散列表的查询效率并不能笼统地说成是 O(1) 。它跟散列函数、装载因子、散列冲突都有关系。在极端情况下，恶意攻击者，通过精心构造的数据，使得所有数据经过散列函数后，都散列到同一个槽里。若我们使用的是基于链表的冲突解决办法，那这个时候，散列表就会退化为链表，查询时间度也退化为 O(n)。 开篇问题 如何设计一个可以应对各种异常情况的工业级散列表？在散列冲突的情况下，能够避免散列表性能的急剧下降。 散列函数 设计不能太复杂 ：避免消耗很多计算时间 生成的值要尽可能随机并且均匀分布：避免或者最小化散列冲突 装载因子过大怎么办？ 装载因子越大，说明散列表中的元素越多，空闲位置越少，散列冲突的概率就越大。 不仅插入数据的过程要多次寻址或者拉很长的链，查找的过程也会因此变得很慢。 解决办法 使用动态扩容，重新申请一个更大的散列表，将数据搬移到这个新散列表中。 存在问题：数据搬移操作很复杂，需要通过散列函数重新计算每个数据的存储位置。 插入数据时间复杂度 最好情况：不需要扩容，复杂度为 O(1) 最坏情况：散列表装载因子过高，需扩容，重新申请内存，重新计算哈希位置，并搬移位置，复杂度为 O(n) 平均情况：摊还分析，时间复杂度接近最好情况，为 O(1) 平衡空间与时间的消耗 若对空间消耗非常敏感，可以为装载因子设置阈值，当装载因子小于阈值之后，启动动态缩容。 若更在意执行效率，能够容忍多消耗一点内存空间，就不需要缩容。 装载因子阈值需要选择得当。如果太大，会导致冲突过多；如果太小，会导致内存浪费严重。 阈值的设置要权衡时间、空间复杂度。 如果内存空间不紧张，对执行效率要求很高，可以降低负载因子的阈值； 相反，如果内存空间紧张，对执行效率要求又不高，可以增加负载因子的值，甚至可以大于 1。 避免低效扩容 为解决一次性扩容耗时过多的情况，可以将扩容操作穿插在插入操作的过程中，分批完成。 具体过程 当装载因子触达阈值之后，只申请新空间，并不将老的数据搬移到新的散列表中。 每当有新数据插入，将新数据插入新散列表，并从旧散列表拿一个数据放入新散列表。 时间复杂度 通过以上均摊方法，将一次扩容的代价，均摊到多次插入操作中。这种实现方式，在任何情况下，插入一个数据的时间复杂度都是 O(1) 查询操作 先在新散列表中查询 未查询到，再到旧散列表中查找 冲突解决方法 开放寻址法 优点 数据存储在数组中，有效利用CPU缓存加快查询速度 方便序列化 不需要额外空间 缺点 查找、删除数据时，涉及到delete标记，比较麻烦 所有数据存储在一个数组中，冲突代价比较高 装载因子的上限不能太大，更浪费空间 链表法 优点 对内存的利用率更高 对装载因子的容忍度更高；即便装载因子变成10，也只是链表的长度变长 缺点 需要额外的空间来保存指针 结点零散分布在内存中，不连续，对CPU缓存不友好 总结 当数据量比较小、装载因子小的时候，适合采用开放寻址法； 面对大对象、大数据量的散列表时，适合采用基于链表的散列冲突处理方法。 而且，比起开放寻址法，链表法更加灵活，支持更多的优化策略，比如用红黑树代替链表。 解答开篇 工业级的散列表应该具有哪些特性？ 支持快速的查询、插入、删除操作； 内存占用合理，不能浪费过多的内存空间； 性能稳定，极端情况下，散列表的性能也不会退化到无法接受的情况。 从三个方面来考虑设计思路 设计一个合适的散列函数； 定义装载因子阈值，并且设计动态扩容策略； 选择合适的散列冲突解决方法。]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>data_structure</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[探索气温趋势]]></title>
    <url>%2F2018-11-26%2F</url>
    <content type="text"><![CDATA[代码传送门，包括扩展了交互功能的改进版 背景 分析全球和自己所在地的气温数据，比较所在城市的气温走向与全球气温走向。 数据获取 从数据库中提取数据。通过Udacity提供的工作区，该工作区与数据库连接。 方法一 写入以下SQL语句，导出世界气温数据以及最接近自己居住地的大城市气温数据。 首先查看city_list表，country列等于China的城市有哪些; 然后将离自己所在城市长沙最近的城市武汉市的数据提取出来; 最后再将全球气温数据提取出来。 select * from city_list where country=‘China’; select * from city_data where city=‘Wuhan’; select * from global_data; 方法二 或者通过以下SQL命令，一次性将两个数据都提取到一个表格： select c.year, c.avg_temp as city_temp, g.avg_temp as global_temp from city_data c, global_data g where c.year = g.year and c.city = 'Wuhan'; 可视化 使用Python将提取的数据（这里的数据是采用方法一提取的）可视化成一个线条图，便于武汉市和全球气温比较。 说明 绘制图使用matplotlib.pyplot模块；移动平均值计算需使用pandas库；设置坐标轴的ticks需用到matplotlib.ticker模块；matplotlib绘图可视化的各属性设置可参考这里 为了使绘制的线图更加平滑，便于观察气温走向，所以采用气温的移动平均值，而不是原始的年平均值。因而需要先计算移动平均值-----&gt;函数calculate_moving_average() 1）移动平均值的计算方法这里采用pandas.rolling_mean函数。但是，按照文档说明传入参数后，会报出AttributeError: module 'pandas' has no attribute 'rolling_mean'错误，后来发现是pandas库的版本问题，具体解决方案可参考这里。 2）移动平均值窗口大小的设定需要权衡考虑。设置得过小，起不到平滑的作用，波动仍然会很剧烈；设置得过大，数据越平滑，但是准确性和敏感性就降低的越多。所以需要在数据变化准确性和平滑程度之间进行一个权衡，既想要观察到更多局部的波动，又想要观察长远趋势，10年左右是比较好的选择。 有了移动均值后，就可以开始绘制图形了-----&gt;函数show_fig() 1）绘制图形时需要考虑所在城市的数据范围是否与全球气温数据的范围一致。武汉市的数据是从1841年到2013年，而全球气温数据是从1750年到2015年，所以要将城市数据和全球数据处理成在相同时间段内，才能正确比较趋势。 注：只有当采用方法一获取数据时才需考虑第一点 2）需要考虑如何设置图形坐标轴的刻度尺大小，使得图形大小更为合适，便于查看和比较本地城市和全球的气温。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667"""分析所在城市和全球的气温趋势"""import pandas as pdimport matplotlib.pyplot as pltfrom matplotlib.ticker import MultipleLocator, FormatStrFormatter%matplotlib inlinedef calculate_moving_average(row_data,window): """计算移动平均值 参数1：需要计算移动平均值的csv文件 参数2：移动窗口大小 """ data = pd.read_csv(row_data) data['mavg_temp'] = round(data['avg_temp'].rolling(window).mean(),2) return datadef show_fig(city_data,global_data): """将所在城市与全球平均气温数据绘制成折线图并展示 参数1：城市平均气温数据 参数2：全球平均气温数据 """ #设置显示的图片大小 figsize = 10,5 figure, ax = plt.subplots(figsize=figsize) #X,Y轴的标签 plt.xlabel('Year') plt.ylabel('Moving Average Temperature (ºC)') xmajorLocator = MultipleLocator(20) #将x轴主刻度标签设置为20的倍数 xminorLocator = MultipleLocator(5) #将x轴次刻度标签设置为5的倍数 ax.xaxis.set_major_locator(xmajorLocator) ax.xaxis.set_minor_locator(xminorLocator) ymajorLocator = MultipleLocator(1) #将y轴主刻度标签设置为1的倍数 yminorLocator = MultipleLocator(0.2) #将y轴次刻度标签设置为0.2的倍数 ax.yaxis.set_major_locator(ymajorLocator) ax.yaxis.set_minor_locator(yminorLocator) ax.xaxis.grid(True, which='major') #x坐标轴的网格使用主刻度 ax.yaxis.grid(True, which='minor') #y坐标轴的网格使用次刻度 plt.plot(city_data['year'],city_data['mavg_temp'],color='skyblue',linewidth='2',label='Wuhan') plt.plot(global_data['year'],global_data['mavg_temp'],color='lightgreen',linewidth='2',label='Global') plt.legend() plt.title('Temperature Trend') plt.show() city_data_file = 'data/avg_temp_wuhan_data.csv'global_data_file = 'data/avg_temp_global_data.csv'window = 10city_data = calculate_moving_average(city_data_file,window)global_data = calculate_moving_average(global_data_file,window)# print(city_data)# print(global_data)# print(city_data[9:])city_data = city_data[9:] #计算移动均值后，武汉市数据表前9行没有数据，从1850年起-2013年，共164行，有平均气温数据# print(global_data[100:-2])global_data = global_data[100:-2] #故为了正确比较，全球平均气温数据也只取第100行(1850年)到倒数第二行(2013年)之间的164行数据show_fig(city_data,global_data) 问题与观察结论 Q1 全球气温的整体趋势是怎么样的？本地城市呢？ 从上图中可以看到整体上，从1841年起到2015年之间的70多年间，全球和武汉市的平均气温都是呈上升趋势。世界是真的越来越热了，经过这70多年，全球平均气温上升了1.6度左右，而武汉市上升了1.8度左右。武汉市比全球气温上涨幅度略大。 Q2 与全球平均气温相比，本地城市平均气温比较热还是比较冷？长期气温差异是否一致？ 可以很明显的看到，武汉市比全球更热，其平均气温比全球平均气温高出8度左右。不愧是中国的“四大火炉城市”之一呀！1841年的时候，武汉的平均气温是16.2度，比全球平均气温8度，高出8.2度，到了近几年，武汉市的平均气温升到18度，达到最高峰值，比全球平均气温最高峰值9.6度，高出8.4度。这说明，长期来看，武汉市和全球的气温差异是一致的。 Q3 在这几十年间，全球的气温波动如何？本地城市呢？ 全球的气温几乎是呈平稳上升趋势，但是到了最近几年，尤其是1980年之后，气温增长得越来越快，图形呈负偏斜分布；而武汉市的气温虽然整体上是上升的趋势，但是趋势波动很大，某个时间段内气温下降，之后再上升，图形呈多峰分布。 Q4 哪个时间段内，武汉市与全球气温趋势的趋势最为相似？ 通过图可以观察到1860年-1885年这25年间，武汉市与全球气温趋势最为相近。在这25年，武汉和全球气温都两次达到气温最低峰值。第一次出现在1862年左右，第二次出现在1885年，武汉市大概为15.8度，全球气温大概为7.8度。不知道在这个两个时间点发生了什么，温度出现了下降现象，但在1885年之后，无论是武汉还是全球，尽管气温有波动，但都没有再降到最低峰值。]]></content>
      <categories>
        <category>project</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>data_analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[散列表]]></title>
    <url>%2F2018-11-18%2F</url>
    <content type="text"><![CDATA[问题 Word有个拼写检查功能，一旦输入的英文单词有错，它就会在单词下方画上红色的波浪线。这个功能是如何实现的？ 散列表 散列表叫Hash Table，即哈希表或者Hash表。散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实是数组的一种扩展，由数组演化而来。如果没有数组，就没有散列表。 举个栗子 假如有89名选手参加学校运动会，为了方便记录成绩，每个选手胸前都会贴上自己的参赛号码。这89名选手的编号依次是1到89。现在需要编程实现，通过编号快速找到对应选手信息。 做法：将这89名选手的信息放在数组里，编号为1的选手，放在数组中下标为1的位置；编号为2的选手，放在数组中下标为2的位置。以此推类，编号为 k 的选手，放在数组中下标为 k 的位置。 当需要查询参赛编号为 x 的选手时，只需将下标为 x 的数组元素取出来就可以了，时间复杂为 O(1)。 这就是散列思想，其中，参赛选手的编号叫作键（key）或者关键字。把参赛编号转化为数组下标的映射方法就叫作散列函数（哈希函数），散列函数计算得到的值就叫作散列值（哈希值）。 散列表用的是数组支持按照下标随机访问，时间复杂度是 O(1) 的特性。 通过散列函数把元素的键值映射为下标，然后将数据存储在数组中对应下标的位置。 当按照键值查询元素时，用同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据。 散列函数 散列函数在散列表中起着非常关键的作用。将其定义成 hash(key)，其中 key 表示元素的键值，hash(key)的值表示经过散列函数计算得到的散列值。 上个例子中，编号就是数组下标，所以hash(key)就等于key。 散列函数设计的基本要求 1 . 散列函数计算得到的散列值是一个非负整数； 因为数组下标是从 0 开始的，所以散列函数生成的散列值也要是非负整数。 2 . 如果 key1 = key2 ，那 hash(key1) == hash(key2)； 相同的 key，经过散列函数得到的散列值也应该是相同的。 3 . 如果 key1 ≠ key2，那 hash(key1) ≠ hash(key2)。 这个要求看起来合情合理，但是在真实的情况下，要想找到一个不同的 key 对应的散列值都不一样的散列函数，几乎是不可能的。即便像业界著名的MD5、SHA、CRC等哈希算法，也无法完全避免这种散列冲突。而且，因为数组的存储空间有限，也会加大散列冲突的概率。 几乎无法找到一个完美的无冲突的散列函数，即便能找到，付出的时间成本、计算成本也是很大的，所以针对散列冲突问题，需要通过其他途径来解决。 散列冲突 常用的散列冲突解决方法有两类，开放寻址法（open addressing）和链表法（chaining）。 开放寻址法 开放寻址法的核心思想是，如果出现了散列冲突，就重新探测一个空闲位置，将其插入。那如何重新探测新的位置呢？ 一个比较简单的探测方法，线性探测（Linear Probing）。 1 . 插入数据 当往散列表中插入数据时，如果某个数据经过散列函数散列之后，存储位置已经被占用了，那就从当前位置开始，依次往后查找，看是否有空闲位置，直到找到为止。 例如下图所示： 黄色的块表示空闲，橙色的块表示已被存储数据。从图中可以看出，散列表的大小为 10，在元素 x 插入散列表之前，已经 6 个元素插入到散列表中。 x 经过 Hash 算法之后，被散列到位置下标为 7 的位置，但是这个位置已经有数据了，所以就产生了冲突。于是就顺序地往后一个一个找，看有没有空闲的位置，遍历到尾部都没有找到空闲的位置，只好再从表头开始找，直到找到空闲位置 2，于是将其插入到这个位置。 2 . 查找数据 在散列表中查找元素的过程有点儿类似插入过程。通过散列函数求出要查找元素的键值对应的散列值，然后比较数组中下标为散列值的元素和要查找的元素。如果相等，则说明就是要找的元素；否则就顺序往后依次查找。如果遍历到数组中的空闲位置，还没有找到，就说明要查找的元素并没有在散列表中。 如下图所示： 3 . 删除数据 对于使用线性探测法解决冲突的散列表，删除操作稍微有些特别，不能单纯地把要删除的元素设置为空。 在查找的时候，一旦通过线性探测方法，找到一个空闲位置，就可以认定散列表中不存在这个数据。 但是，如果这个空闲位置是后来删除的，就会导致原来的查找算法失效。本来存在的数据，会被认定为不存在。这个问题如何解决呢？ 如下图所示： 解决办法： 可以将删除的元素，特殊标记为 deleted。当线性探测查找的时候，遇到标记为 deleted 的空间，并不是停下来，而是继续往下探测。 线性探测法的问题 当散列表中插入的数据越来越多时，散列冲突发生的可能性就会越来越大，空闲位置会越来越少，线性探测的时间就会越来越久。极端情况下，可能需要探测整个散列表，所以最坏情况下的时间复杂度为 O(n)。同理，在删除和查找时，也有可能会线性探测整张散列表，才能找到要查找或者删除的数据。 对于开放寻址冲突解决方法，除了线性探测方法之外，还有另外两种比较经典的探测方法，二次探测（Quadratic probing）和双重散列（Double hashing）。 二次探测，跟线性探测很像，线性探测每次探测的步长是 1，那它探测的下标序列就是hash(key)+0，hash(key)+1，hash(key)+2……而二次探测探测的步长就变成了原来的“二次方”，也就是说，它探测的下标序列就是 hash(key)+0，hash(key)+1^2，hash(key)+2^2…… 双重散列，意思就是不仅要使用一个散列函数。而是使用一组散列函数 hash1(key)，hash2(key)，hash3(key)……先用第一个散列函数，如果计算得到的存储位置已经被占用，再用第二个散列函数，依次类推，直到找到空闲的存储位置。 为了尽可能保证散列表的操作效率，一般情况下，要尽可能保证散列表中有一定比例的空闲槽位。 用**装载因子（load factor）**来表示空位的多少。装载因子的计算公式是： 散列表的装载因子 = 填入表中的元素个数 / 散列表的长度 装载因子越大，说明空闲位置越少，冲突越多，散列表的性能会下降。 链表法 链表法是一种更加常用的散列冲突解决办法，相比开放寻址法，它要简单很多。 在散列表中，每个桶（bucket）或者槽（slot）会对应一条链表，所有散列值相同的元素都放到相同槽位对应的链表中。 如下图所示： 当插入的时候，只需要通过散列函数计算出对应的散列槽位，将其插入到对应链表中即可，所以插入的时间复杂度是 O(1)。 当查找、删除一个元素时，同样通过散列函数计算出对应的槽，然后遍历链表查找或者删除。 查找或删除操作的时间复杂度 两个操作的时间复杂度跟链表的长度 k 成正比，也就是 O(k)。 对于散列比较均匀的散列函数来说，理论上讲: k=n/m，其中 n 表示散列中数据的个数，m 表示散列表中“槽”的个数。 解答开篇 Word 文档中单词拼写检查功能是如何实现的？ 用散列表来存储整个英文单词词典。 常用的英文单词有 20 万个左右，假设单词的平均长度是 10 个字母，平均一个单词占用 10 个字节的内存空间，那 20 万英文单词大约占 2MB 的存储空间，就算放大 10 倍也就是 20MB。对于现在的计算机来说，这个大小完全可以放在内存里面。 当用户输入某个英文单词时，拿用户输入的单词去散列表中查找。如果查到，则说明拼写正确；如果没有查到，则说明拼写可能有误，给予提示。借助散列表这种数据结构，就可以轻松实现快速判断是否存在拼写错误。 思考 1 . 假设有 10 万条 URL 访问日志，如何按照访问次数给 URL 排序？ 遍历 10 万条数据，以 URL 为 key，数组的下标为 hash(key)得到的值，访问次数count为相应数组下标的内容，存入散列表，同时记录下访问次数count的最大值 K，时间复杂度 O(N)。 如果 K 不是很大，可以使用桶排序，时间复杂度 O(N)。如果 K 非常大（比如大于 10 万），就使用快速排序，复杂度 O(NlogN)。 2 . 有两个字符串数组，每个数组大约有 10 万条字符串，如何快速找出两个数组中相同的字符串？ 以第一个字符串数组构建散列表，key 为字符串，数组的下标为 hash(key)得到的值，出现次数count为相应数组下标的内容，时间复杂度为 O(N)。再遍历第二个字符串数组，以字符串为 key 在散列表中查找，找到散列值对应数组下标存储的count值，如果count大于零，说明存在相同字符串，时间复杂度为 O(N)。]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>data_structure</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[跳表]]></title>
    <url>%2F2018-11-15%2F</url>
    <content type="text"><![CDATA[二分查找底层依赖的是数组随机访问的特性，所有只能用数组实现。 数据存储在链表中，通过改造链表，也可以支持类似二分的查找算法。这种改造之后的数据结构叫作跳表。 跳表是一种各方面性能都比较优秀的动态数据结构，可以支持快速的插入、删除、查找操作。 可替代红黑树，代码实现比红黑树简单 问题 为什么 Redis 会选择用跳表来实现有序集合？ 如何理解跳表 对于单链表，即便表中存储的数据是有序的，要在其中查找某个数据，也只能从头到尾遍历链表。查找的时间复杂度也很高，O(n)。 如何提高这个查找效率呢？ 就是通过给链表建立索引，每两个结点提取一个结点到上一级，抽出来那一级叫作索引或索引层。上一级索引结点的有个 down 指针，指向下一级结点。 如图所示： 加一层索引之后，查找一个结点需要遍历的结点个数减少了，即效率得到提高。不断地往上建立索引，查找效率的提升就会很多。 这种链表加多级索引的结构，就是跳表。 跳表查询的时间复杂度 按上面所说，每两个结点抽出一个结点作为上一级索引的结点，那第一级索引的结点个数大约就是 n/2 ，第二级索引的结点个数大约就是 n/4 ，第三级索引的结点个数大约就是 n/8 ，依次类推，到了第 k 级索引的时候，结点个数是第 k-1 级索引的结点个数的 1/2， 即结点个数为 n/(2^k)。 假设索引有 m 级，最高级索引有2个结点。通过该公式，可以得到 n/(2^m) = 2 ，m=log2^n-1 。如果包含原始链表这一层，整个跳表的高度就是 log2^n 。若每层需要遍历 i 个结点，那么在跳表中查询一个数据的时间复杂度就是 O(i * logn) 。 i 的值在这里为3 原因：假设要查找的数据是 x，在第 k 级索引中，当遍历到 y 结点之后，发现 x 大于 y，小于后面的结点 z，所以就通过 y 的 down 指针，从第 k 级索引下降到第 k-1 级索引。在第 k-1 级索引中，y 和 z 之间只有 3 个结点（包含 y 和 z），所以，在 K-1 级索引中最多只需要遍历 3 个结点，依次类推，每一级索引都最多只需要遍历 3 个结点。 如图所示： 以上，跳表中查询任意数据的时间复杂度为 O(logn)。这个时间复杂度跟二分查找是一样的，但却是以空间换时间为代价。 跳表的空间复杂度分析 与纯粹的单链表相比，跳表需要存储多级索引，肯定需要消耗更多的存储空间。 假设原始链表大小为 n，那第一级索引大约有 n/2 个结点，第二级索引大约有 n/4 个结点，以此类推，每上升一级就减少一半，直到剩下 2 个结点。如果我们把每层索引的结点数写出来，就是一个等比数列。 原始链表大小为 n ,每2个结点抽取1个为索引，每层索引的结点数： n/2, n/4, n/8, ..., 8, 4, 2 这几级索引的结点总和就是 n/2+n/4+n/8…+8+4+2=n-2 。所以，跳表的空间复杂度是 O(n)。也就是说，如果将包含 n 个结点的单链表构造成跳表，需要额外再用接近 n 个结点的存储空间。 降低索引占用的内存空间 如果每三个结点，抽一个结点到上级索引，那么第一级索引需要大约 n/3 个结点，第二级索引需要大约 n/9 个结点。每往上一级，索引结点个数都除以 3。为了方便计算，假设最高一级的索引结点个数是 1。每级索引的结点个数相加，也是一个等比数列求和。 原始链表大小为 n ,每3个结点抽取1个为索引，每层索引的结点数： n/3, n/9, n/27, ..., 9, 3, 1 通过等比数列求和公式，总的索引结点大约就是 n/3+n/9+n/27+…+9+3+1=n/2 。尽管空间复杂度还是 O(n)，但比上面的每两个结点抽一个结点的索引构建方法，要减少了一半的索引结点存储空间。 实际上，在软件开发中，不必太在意索引占用的额外空间。 在实际的软件开发中，原始链表中存储的有可能是很大的对象，而索引结点只需要存储关键值和几个指针，并不需要存储对象，所以当对象比索引结点大很多时，那索引占用的额外空间就可以忽略。 高效的动态插入和删除 跳表，不仅支持查找操作，还支持动态的插入、删除操作，而且插入、删除操作的时间复杂度也是 O(logn)。 插入 在单链表中，一旦定位好要插入的位置，插入结点的时间复杂度是很低的，就是 O(1)。但是，为了保证原始链表中数据的有序性，是需要先找到要插入的位置，这个查找操作就会比较耗时。 对于纯粹的单链表，需要遍历每个结点，来找到插入的位置。但是，对于跳表来说，查找某个结点的的时间复杂度是 O(logn)，所以查找某个数据应该插入的位置，方法也是类似的，时间复杂度也是 O(logn)。 删除 如果要删除的结点在索引中也有出现，那么除了要删除原始链表中的结点，还要删除索引中的。因为单链表中的删除操作需要拿到要删除结点的前驱结点，然后通过指针操作完成删除。所以在查找要删除的结点的时候，一定要获取前驱结点。当然，如果用的是双向链表，就不需要考虑这个问题了。 跳表索引动态更新 当数据不断地被插入到跳表中，如果索引不更新，就有可能出现某2个索引结点之间数据非常多的情况，极端情况下，跳表就会退化成单链表。 作为一种动态数据结构，需要用某种手段来维护索引与原始链表大小之间的平衡，也就是说，如果链表中结点多了，索引结点就相应地增加一些，避免复杂度退化，以及查找、插入、删除操作性能下降。 跳表是通过随机函数来维护前面提到的平衡性。 当往跳表中插入数据的时候，可以选择同时将这个数据插入到部分索引层中。 如何选择加入哪些索引层呢？ 通过一个随机函数，来决定将这个结点插入到哪几级索引中，比如随机函数生成了值 K ，那就将这个结点添加到第一级到第 K 级，总共 K 级索引中。 随机函数的选择很有讲究，从概率上来讲，能够保证跳表的索引大小和数据大小平衡性，不至于性能过度退化。 解答开篇 Redis 中的有序集合是通过跳表来实现的，严格点讲，其实还用到了散列表。 Redis 中的有序集合支持的核心操作主要有下面这几个： 插入一个数据； 删除一个数据； 查找一个数据； 按照区间查找数据（比如查找值在 [100, 356] 之间的数据）； 迭代输出有序序列。 为什么选择跳表而不是红黑树实现有序集合？ 原因1：对于插入、删除、查找以及迭代输出有序序列这几个操作，红黑树的完成时间复杂度跟跳表一样；而按照区间来查找数据这个操作，红黑树的效率没有跳表高。 对于按照区间查找数据这个操作，跳表可以做到 O(logn) 的时间复杂度定位区间的起点，然后在原始链表中顺序往后遍历就可以了。 原因2：跳表代码实现更容易。虽然跳表的实现也不简单，但比起红黑树来说还是相对容易一些。跳表也更加灵活，它可以通过改变索引构建策略，有效平衡执行效率和内存消耗。 不过，跳表也不能完全替代红黑树。因为红黑树比跳表的出现要早一些，很多编程语言中的 Map 类型都是通过红黑树来实现的。业务开发的时候，直接拿来用就可以了，不用自己去实现一个红黑树，但是跳表并没有一个现成的实现，所以在开发中，如果想使用跳表，必须要自己实现。]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>data_structure</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二分查找]]></title>
    <url>%2F2018-11-09%2F</url>
    <content type="text"><![CDATA[问题 通过IP地址可用查找到IP归属地。在百度搜索框里，随便输入一个IP地址，就会看到它的归属地。 这个功能是通过维护一个很大的IP地址库来实现的，地址库中包括IP地址范围和归属地的对应关系。 问题是，如果有12万条这样的IP区间与归属地的对应关系，如何快速定位出一个IP地址的归属地呢？ 二分查找的变形 二分查找中最简单的一种情况，是在不存在重复元素的有序数组中，查找值等于给定值的元素。最简单的二分查找比较容易，但是，二分查找的变形问题就没那么好写了。 特别说明：假设要处理的数据是从小到大排列为前提，如果要处理的数据是从大到小排列的，解决思路也是一样的。 几个典型的变形问题 查找第一个值等于给定值的元素 有序数据集合中存在重复的数据，找到第一个值等于给定值的数据。比如，有以下这样一个有序数组，希望找到第一个等于8的数据位置，即下标为5的8. a[10] 1 3 4 5 6 8 8 8 11 19 0 1 2 3 4 5 6 7 8 9 若用之前的二分查找代码实现，首先拿 8 与区间的中间值 a[4] 比较，8 比 6 大，于是在下标 5 到 9 之间继续查找。下标 5 和 9 的中间位置是下标 7，a[7] 正好等于 8，所以代码就返回了。 尽管 a[7] 等于 8，但它并不是要找的第一个等于 8 的元素，因为第一个值等于 8 的元素是数组下标为 5 的元素。 正确的代码 def binary_search_variant1(arr,ele): length = len(arr) low = 0 high = length - 1 while low &lt;= high: mid = low + ((high-low)&gt;&gt;1) if arr[mid] &gt; ele: high = mid -1 elif arr[mid] &lt; ele: low = mid + 1 else: if mid == 0 or arr[mid-1] != ele: return mid high = mid-1 return -1 分析 重点在于：当 a[mid] = ele 时，该如何处理？如果查找的是任意一个值等于给定值的元素，当 a[mid] 等于要查找的值时，a[mid] 就是要找的元素。但是，求解的是第一个值等于给定值的元素，当 a[mid] 等于要查找的值时，还需要确认一下这个 a[mid] 是不是第一个值等于给定值的元素。 如果 mid 等于 0，那这个元素已经是数组的第一个元素，那它肯定是正确的；如果 mid 不等于 0，但 a[mid] 的前一个元素 a[mid-1] 不等于 ele，那也说明 a[mid] 就是要找的第一个值等于给定值的元素。 如果经过检查之后发现 a[mid] 前面的一个元素 a[mid-1] 也等于 ele，那说明此时的 a[mid] 肯定不是要查找的第一个值等于给定值的元素。那就更新 high=mid-1，因为要找的元素肯定出现在 [low, mid-1] 之间。 查找最后一个值等于给定值的元素 理解上一个变体的做法，对于查找最后一个值等于给定值的元素，就很好做了。 代码 def binary_search_variant2(arr,ele): length = len(arr) low = 0 high = length - 1 while low &lt;= high: mid = low + ((high-low)&gt;&gt;1) if arr[mid] &gt; ele: high = mid -1 elif arr[mid] &lt; ele: low = mid + 1 else: if mid == (length-1) or arr[mid+1] != ele: return mid low = mid+1 return -1 分析 如果 a[mid] 这个元素已经是数组中的最后一个元素了，那它肯定是正确的；如果 [mid] 的后一个元素 a[mid+1] 不等于 ele，那也说明 a[mid] 就是要找的最后一个值等于给定值的元素。 如果经过检查之后，发现 a[mid] 后面的一个元素 a[mid+1] 也等于 ele，那说明当前的这个 a[mid] 并不是最后一个值等于给定值的元素。那就更新 low=mid+1，因为要找的元素肯定出现在 [mid+1, high] 之间。 找第一个大于等于给定值的元素 第三类变形问题。在有序数组中，查找第一个大于等于给定值的元素。 比如，数组中存储的这样一个序列：3，4，6，7，10。如果查找第一个大于等于 5 的元素，那就是 6。 代码 def binary_search_variant3(arr,ele): length = len(arr) low = 0 high = length - 1 while low &lt;= high: mid = low + ((high-low)&gt;&gt;1) if arr[mid] &gt;= ele: if mid == 0 or arr[mid-1] &lt; ele: return mid high = mid - 1 else: low = mid + 1 return -1 分析 如果 a[mid] 小于要查找的值 ele，那要查找的值肯定在 [mid+1, high] 之间，所以，更新 low=mid+1。 对于 a[mid] 大于等于给定值 ele 的情况，要确认这个 a[mid] 是不是要找的第一个值大于等于给定值的元素。如果 a[mid] 前面已经没有元素，或者前面一个元素小于要查找的值 ele，那 a[mid] 就是要找的元素。 如果 a[mid-1] 也大于等于要查找的值 ele，那说明要查找的元素在 [low, mid-1] 之间，所以，将 high 更新为 mid-1。 查找最后一个小于等于给定值的元素 查找最后一个小于等于给定值的元素。 比如，数组中存储了这样一组数据：3，5，6，8，9，10。最后一个小于等于 7 的元素就是 6。 代码 def binary_search_variant4(arr,ele): length = len(arr) low = 0 high = length - 1 while low &lt;= high: mid = low + ((high-low)&gt;&gt;1) if arr[mid] &lt;= ele: if mid == (length-1) or arr[mid+1] &gt; ele: return mid low = mid + 1 else: high = mid - 1 return -1 解答开篇 如何快速定位出一个 IP 地址的归属地？ 如果 IP 区间与归属地的对应关系不经常更新，就可以先预处理这 12 万条数据，让其按照起始 IP 从小到大排序。 IP 地址可以转化为 32 位的整型数。所以，可以将起始地址，按照对应的整型值的大小关系，从小到大进行排序。 然后，这个问题就可以转化为第四种变形问题“在有序数组中，查找最后一个小于等于某个给定值的元素”了。 当要查询某个 IP 归属地时，可以先通过二分查找，找到最后一个起始 IP 小于等于这个 IP 的 IP 区间，然后，检查这个 IP 是否在这个 IP 区间内，如果在，就取出对应的归属地显示；如果不在，就返回未查找到。 Key 凡是用二分查找能解决的问题，绝大部分更倾向于用散列表或者二叉查找树。即便是二分查找在内存使用上更节省，但是毕竟内存如此紧缺的情况并不多。 实际上，求“值等于给定值”的二分查找确实不怎么会被用到，二分查找更适合用在“近似”查找问题，在这类问题上，二分查找的优势更加明显。比如上面的几种变体问题，用其他数据结构，比如散列表、二叉树，就比较难实现了。 变体的二分查找算法写的时候容易因为细节处理不好而产生 Bug，这些容易出错的细节有：终止条件、区间上下界更新方法、返回值选择。]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>data_structure</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二分查找]]></title>
    <url>%2F2018-11-08%2F</url>
    <content type="text"><![CDATA[问题 假设有1000万个整数数据，每个数据占8字节，如何设计数据结构和算法，快速判断某个整数是否出现在1000万数据中？（存在多次查找的情况） 前提：该功能不要太占用内存空间，最好不超过100MB 二分查找思想 栗子 生活中的“猜数字大小”游戏，猜的过程中，玩家每猜一次，庄家告诉你是猜大了还是猜小了，直到猜中为止。最快速猜中的方法就是用二分查找的思想。每次说猜测范围的中间数字，如果中间数有两个，则选择较小的那个。按照这个思想，这样即使猜的数字范围在0-999，最多也只要10次就能猜中。 二分查找针对的是一个有序的整数集合，查找思想类似于分治思想。每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为0。 时间复杂度 二分查找是一种非常高效的查找算法。 假设数据大小是 n，每次查找后数据都会缩小为原来的一半，也就是会除以 2。最坏情况下，直到查找区间被缩小为空，才停止。 被查找区间的大小变化： n, n/2, 2/4, n/8, ... n/2^k 这是一个等比数列，其中 n/2^k=1 时， k 的值就是总共缩小的次数，每次缩小操作只涉及两个数据的大小比较，所以，经过 k 次区间缩小操作，时间复杂度为 O(k)。通过 n/2^k=1， 可以求得 k = log2^n ，所以时间复杂度就是 O(logn)。 O(logn) 对数时间复杂度 这是一种极其高效的时间复杂度，有的时候甚至比时间复杂度是常量级 O(1) 的算法还要高效。 为什么这么说呢？ 因为 logn 是一个非常“恐怖”的数量级，即便 n 非常非常大，对应的 logn 也很小。比如 n 等于 2 的 32 次方，n 大约是 42 亿。也就是说，如果我们在 42 亿个数据中用二分查找一个数据，最多需要比较 32 次。 在用大 O 标记法表示时间复杂度的时候，会省略掉常数、系数和低阶。 对于常量级时间复杂度的算法来说，O(1) 有可能表示的是一个非常大的常量值，比如 O(1000)、O(10000)。所以，常量级时间复杂度的算法有时候可能还没有 O(logn) 的算法执行效率高。 反过来，对数相对的就是指数。这也是为什么，指数时间复杂度的算法在大规模数据面前是无效的。 简单二分查找的递归与非递归实现 简单二分查找，是指在不存在重复元素的有序数据中查找值等于给定值的数据。 非递归代码 def binary_search(arr, ele): lenght = len(arr) low = 0 high = length -1 while low &lt;= high: mid = (low + high) // 2 # low + (high-low)&gt;&gt;1 if arr[mid] == ele: return mid elif arr[mid] &gt; ele: high = mid - 1 else: low = mid + 1 return -1 提示点 1 . 循环退出条件。是 low &lt;= high，而不是 low &lt; high 2 . mid 的取值。实际上，mid=(low+high)//2 这种写法有问题。因为如果 low 和 high 比较大的话，两者之和就有可能会溢出。改进的方法是将 mid 的计算方式写成 low+(high-low)//2。更进一步，可以将这里的除以 2 操作转化成位运算 low+(high-low) &gt;&gt;1。因为相比除法运算来说，计算机处理位运算要快得多。 3 . low 和 high 的更新。 low=mid+1，high=mid-1。注意这里的 +1 和 -1，如果直接写成 low=mid 或者 high=mid，就可能会发生死循环。 比如，当 high=3，low=3 时，如果 arr[3] 不等于 ele，就会导致一直循环不退出。 递归代码 def binary_search(arr, low, high, ele): if low &gt; high: return -1 mid = low + (high-low) // 2 if arr[mid] == ele: return mid elif arr[mid] &gt; ele: return binary_search(arr, low, mid-1, ele) else: return binary_search(arr, mid+1, high, ele) 应用场景的局限性 二分查找的时间复杂度是 O(logn)，查找数据的效率非常高。不过，并不是什么情况下都可以用二分查找，它的应用场景有很大局限性。 1 . 二分查找依赖的是顺序表结构，即数组。 二分查找不能依赖于其他数据结构，比如链表。主要原因是二分查找算法需要按照下标随机访问元素。数组按照下标随机访问数据的时间复杂度是 O(1)，而链表随机访问的时间复杂度是 O(n)。所以，如果数据使用链表存储，二分查找的时间复杂就会变得很高。 2 . 二分查找针对的是有序数据。如果数据没有序，需要先排序。排序的时间复杂度最低是 O(nlogn)。所以，如果针对的是一组静态的数据，没有频繁地插入、删除，就可以进行一次排序，多次二分查找。这样排序的成本可被均摊，二分查找的边际成本就会比较低。 针对有频繁插入、删除操作的这种动态数据集合，二分查找是不适用的。要用二分查找，要么每次插入、删除操作之后保证数据仍然有序，要么在每次二分查找之前都先进行排序。针对这种动态数据集合，无论哪种方法，维护有序的成本都很高。 3 . 数据量太小不适合二分查找。如果要处理的数据量很小，完全没有必要用二分查找，顺序遍历就足够了。比如在一个大小为 10 的数组中查找一个元素，不管用二分查找还是顺序遍历，查找速度都差不多。只有数据量比较大的时候，二分查找的优势才会比较明显。 有一个例外。如果数据之间的比较操作非常耗时，不管数据量大小，都推荐使用二分查找。比如，数组中存储的都是长度超过几百的字符串，如此长的两个字符串之间比大小，就会非常耗时。为了尽可能地减少比较次数，二分查找就比顺序遍历更有优势。 4 . 数据量太大也不适合二分查找。二分查找依赖的是数组这种数据结构，而数组为了支持随机访问的特性，要求内存空间连续，对内存的要求比较苛刻。二分查找是作用在数组这种数据结构之上的，所以太大的数据用数组存储就比较困难，就不能用二分查找了。 解答开篇 如何快速判断某个整数是否出现在在1000万数据中？ 内存限制是 100MB，每个数据大小是 8 字节，最简单的办法就是将数据存储在数组中，内存占用差不多是 80MB ，符合内存的限制。 先对这 1000 万数据从小到大排序，然后再利用二分查找算法，就可以快速地查找想要的数据。 散列表、二叉树这些支持快速查找的动态数据结构也可以解决这类问题。但因为内存的限制，使得这些方法在这里行不通。 虽然大部分情况下，用二分查找可以解决的问题，用散列表、二叉树都可以解决。但是，不管是散列表还是二叉树，都会需要比较多的额外的内存空间。如果用散列表或者二叉树来存储这 1000万 的数据，用 100MB 的内存肯定是存不下的。而二分查找底层依赖的是数组，除了数据本身之外，不需要额外存储其他信息，是最省内存空间的存储方式，所以刚好能在限定的内存大小下解决这个问题。 Key 1 . 若二分查找依赖于链表结构，时间复杂度如何分析？ 假设链表长度为 n，二分查找每次需要找到中间点，那么总共需要移动的指针次数为： n/2 + n/4 + n/8 + ... + 1 这也是一个等比数列，根据等比数列求和公式 (S = (a1-an*q)/1-q, q为公比, 且不为1)，其和等于 n-1 。所有最后算法时间复杂度为 O(n)。 时间复杂度和顺序查找时间复杂度相同，但是，在二分查找的时候，由于要进行多余的运算，严格来说，会比顺序查找时间慢。 2 . 用二分查找“求一个数的平方根”，要求精确到小数点后6位（类似于LeetCode 69题） def mySqrt(self, x): if x==0 or x==1: return x low = 0 high = max(x,1.0) #high = x mid = (low + high)/2.0 while abs(mid**2 - x) &gt; 1e-6: if mid**2 &gt; x: high = mid else: low = mid mid = (low + high)/2.0 return mid]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>data_structure</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序]]></title>
    <url>%2F2018-11-04%2F</url>
    <content type="text"><![CDATA[主要内容，总结前面的几种算法在各方面的性能。 如何选择合适的排序算法？ 总结前面几种主要排序算法的性能差异。一些算法在一些指标上达到最优情况，还有一些算法的复杂度虽然相同，但在实践中的表现却有差异。 最理想的排序算法是 O(n logn) 时间、O(1)空间、稳定、最好还具有适应性。当然目前，还没找到这种算法。 最坏时间复杂度 平均时间复杂度 最好时间复杂度 是否稳定 是否是原地排序 适应性 冒泡排序 O(n^2) O(n^2) O(n) 是 是 插入排序 O(n^2) O(n^2) O(n) 是 是 选择排序 O(n^2) O(n^2) O(n^2) 否 是 快速排序 O(n^2) O(nlogn) O(nlogn) 否 是 归并排序 O(nlogn) O(nlogn) O(nlogn) 是 否 计数排序 O(n+k) O(n+k) O(n+k) 是 否 桶排序 O(nlogn) O(n) O(n) 是 否 基数排序 O(k*n) O(k*n) O(k*n) 是 否 适用情况比较 1 . 三个线性排序算法的时间复杂度低，但是适用场景特殊，所以通用的排序函数，不选择线性排序算法。 2 . 对小规模数据进行排序，可以选择时间复杂度是 O(n2) 的算法，如，冒泡排序和插入排序； 3 . 对大规模数据进行排序，选择时间复杂度是 O(nlogn) 的算法更加高效。 所以，为了兼顾任意规模数据的排序，一般都会首选时间复杂度是 O(nlogn) 的排序算法来实现排序函数。 时间复杂度为O(nlogn)的排序算法，目前有两个，快速排序和归并排序 快速排序 快速排序适合来实现通用的排序函数，但是，快速排序在最坏情况下的时间复杂度是 O(n^2)。 为什么最坏情况下快排的时间复杂度是 O(n^2)呢？ 如果要排序的数据原来就是有序的或者接近有序的，而每次分区点都选择最后一个数据，那快排的性能就很不好，这时时间复杂度就为 O(n^2)。所以，这种 O(n^2) 时间复杂度出现的主要原因是因为分区点选择不够合理。 最理想的分区点是：被分区点分开的两个分区中，数据的数量差不多。若直接选择第一个或最后一个数据作为分区点，不考虑数据的特点，肯定就会出现最坏情况的时间复杂度。 几种常用的分区算法 1 . 三数取中法 从区间的首、尾、中间，分别取出一个数，然后对比这三个数的大小，取3数中的中间值作为分区点。每间隔某个固定的长度，取数据出来比较，将中间值作为分区点。这种思路肯定比单纯取某一个数据更好，但是，如果要排序的数组比较大，那“三数取中”可能就不够，需要扩大范围，“五数取中”或者“十数取中”。 2 . 随机法 随机法是每次从要排序的区间中，随机选择一个元素作为分区点。这种方法并不能保证每次分区点都选的比较好，但是从概率的角度来看，也不大可能会出现每次分区点都选的很差的情况，所以平均情况下，这样选的分区点是比较好的。时间复杂度退化为最糟糕的 O(n^2) 的情况，出现的可能性不大。 快速排序用递归实现。递归需要警惕堆栈溢出。为了避免快速排序里，递归过深而堆栈过小，导致堆栈溢出，有两种解决办法：第一种是限制递归深度。一旦递归过深，超过了事先设定的阈值，就停止递归。第二种是通过在堆上模拟实现一个函数调用栈，手动模拟递归压栈、出栈的过程，这样就没有了系统栈大小的限制。 举例分析排序函数 实际排序程序会采用多种算法的组合，即混成方法。 在一些复杂排序算法里，也需要处理较短序列的排序问题。快排和归并排序就是这方面的典型： 快速排序算法中，序列被划分为越来越短的片段。若序列已经很短，例如短于几个元素，快排还需要做几次递归调用（进栈、出栈）。这些赋值操作很耗时，表现在复杂度度描述中忽略了的常量因子。对于很短的序列，采用插入排序，效果很可能优于快速排序。 归并排序正好与快排相反，是从短的有序序列归并出越来越长的序列。从很多个各自包含一个元素的序列出发，通过几遍归并得到最终的有序序列，这其中需要做许多函数调用工作。与这几个元素做简单插入排序相比，归并排序消耗时间会更多。 在实际程序里的排序功能，特别是各自程序库里的排序函数，通常不是纯粹第采用一种算法，而是使用两种或两种以上方法的组合。常见的是归并排序和插入排序的组合，以及快速排序和插入排序的组合。 Python的内置排序算法 Python中的内置排序函数 sort 和表list类的对象的sort方法，两者共享同一个排序算法，是一种混成排序算法，叫作 Timsort(蒂姆排序)。 基本情况 蒂姆排序是一种基于归并技术的稳定排序算法，结合使用了归并排序和插入排序技术，最坏时间复杂度是O(nlogn)。它具有适应性，在被排序的数组元素接近排好序的情况下，它的时间复杂度可能远小于O(nlogn)，可能达到线性时间。在最坏情况下，它的需要n/2的工作空间，因此其空间复杂度是O(n)。 最坏时间复杂度 平均时间复杂度 最好时间复杂度 是否稳定 是否是原地排序 适应性 蒂姆排序 O(nlogn) O(nlogn) O(n) 是 否 蒂姆排序算法适合许多实际应用中常见的情况，特别是被排序的数据序列分段有序或者基本有序，但仍有些非有序元素的情况。人们通过许多试验，发现蒂姆排序在平均性能上超过快排，是目前实际表现最好的排序算法。虽然理论上，它并没有克服归并排序O(n)空间开销的弱点，但实际开发中经常不需要很大的额外空间，且现在计算机的内存都很大，很多时候追求的都是速度。 基本工作方式 蒂姆排序的优势是克服了归并排序没有适应性的缺陷，且又保持了其稳定性的特征。 1 . 考察待排序序列中非严格单调上升（后一个值大于等于前一个值）或严格单调下降（后一个值小于前一个值）的片段，反转其中的严格下降片段。 2 . 采用插入排序，对连续出现的几个短的上升排序序列，使整个序列变成一系列（非严格）单调上升的记录片段，每个片段都长于某个特定值。 3 . 采用归并产生更长的排序片段，控制这一归并过程，保证片段的长度尽可能均匀。归并中采用一些策略，尽可能地减少临时空间的使用。通过反复归并，最终得到排序序列 总结 1 . 如果序列中的数据基本有序而且序列长度n比较小，直接插入排序能很快完成排序，即具有适应性。这种情况下，冒泡排序也比较快。 2 . 简单排序算法多是稳定的，而大部分时间性能好的排序都不稳定，如快速排序（以及堆排序）等。 稳定性是具体算法实现的性质，采用同一种排序算法，有可能做出稳定的和不稳定的实现。但有些算法的实现可以很自然地做到稳定（如，插入排序，归并排序），另一些则需要附加的时间或空间开销（如，选择排序） 3 . 实际应用中，数据记录通常有一个主关键码，例如各种唯一标识码，如学号、身份证号、用户账户、商品订单号等。这种关键码一般都具有唯一性。如果要做的是按主关键码排序，所用排序方法是否稳定就无关紧要。 但在另一些应用中，经常需要把记录中的其他成分作为排序码使用，例如，按学生的姓名、籍贯、年龄、成绩等排序。在做这种排序时，应该根据问题所需慎重选择排序方法，经常需要用稳定算法。若用了不稳定的排序算法，可能就还需要对具有相同关键码的数据段再次排序。]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>data_structure</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序]]></title>
    <url>%2F2018-11-03%2F</url>
    <content type="text"><![CDATA[三种时间复杂度是 O(n) 的排序算法：桶排序、计数排序、计数排序。这些排序算法的时间复杂度是线性的，因而也叫线性排序。之所以能做到线性的复杂度，主要原因这三种算法都是非基于比较的排序算法，不涉及元素间的比较操作；但是这几种排序算法对数据的要求很苛刻，因而需要重点掌握它们的使用场景。 问题 如何根据年龄给100万用户排序？（时间复杂度最好要是线性的） 桶排序（Bucket sort） 核心原理 桶排序，顾名思义，会用到“桶”，这些“桶”，按区间划分，核心思想是将要排序的数据将其分到所属的桶里去，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就有序了。 为什么时间复杂度为 O(n)？ 如果要排序的数据有 n 个，将其均匀地划分到 m 个桶内，每个桶里就有 k = n/m 个元素。每个桶内部用快速排序，时间复杂度就为 O(k * logk) 。 m 个桶排序的时间复杂度就是 O(m * k * logk)，又因为 k = n/m，所以整个桶排序的时间复杂度就是 O(n* logn/m)。当桶的个数 m 接近数据个数 n 时，logn/m 就是一个很小的常量，这个时候桶排序的时间复杂度接近 O(n) 对数据的苛刻要求 1 . 首先，要排序的数据需要很容易就能划分成 m 个桶，并且，桶与桶之间有着天然的大小顺序。这样每个桶内的数据都排序完之后，桶与桶之间的数据不需要再进行排序。 2 . 其次，数据在各个桶之间的分布是比较均匀的。如果数据经过桶的划分之后，有些桶里的数据非常多，有些非常少，很不平均，那桶内数据排序的时间复杂度就不是常量级了。在极端情况下，如果数据都被划分到一个桶里，那就退化为 O(nlogn) 的排序算法了。 适用场景 桶排序比较适合用在外部排序中。所谓的外部排序就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。 栗子 问题描述 有 10GB 的订单数据，我们希望按订单金额（假设金额都是正整数）进行排序，但是我们的内存有限，只有几百 MB，没办法一次性把 10GB 的数据都加载到内存中。 思路 先扫描一遍文件，看订单金额所处的数据范围。假设经过扫描之后得到，订单金额最小是 1 元，最大是 10 万元。将所有订单根据金额划分到 100 个桶里，第一个桶存储金额在 1 元到 1000 元之内的订单，第二桶存储金额在 1001 元到 2000 元之内的订单，以此类推。每一个桶对应一个文件，并且按照金额范围的大小顺序编号命名 (00，01，02…99)。 理想的情况下，如果订单金额在 1 到 10 万之间均匀分布，那订单会被均匀划分到 100 个文件中，每个小文件中存储大约 100MB 的订单数据，就可以将这 100 个小文件依次放到内存中，用快排来排序。等所有文件都排好序之后，只需要按照文件编号，从小到大依次读取每个小文件中的订单数据，并将其写入到一个文件中，那这个文件中存储的就是按照金额从小到大排序的订单数据了。 不过，订单按照金额在 1 元到 10 万元之间并不一定是均匀分布的 ，所以 10GB 订单数据是无法均匀地被划分到 100 个文件中的。有可能某个金额区间的数据特别多，划分之后对应的文件就会很大，没法一次性读入内存。 针对这些划分之后还是比较大的文件，可以继续划分，比如，订单金额在 1 元到 1000 元之间的比较多，就将这个区间继续划分为 10 个小区间，1 元到 100 元，101 元到 200 元，201 元到 300 元…901 元到 1000 元。如果划分之后，101 元到 200 元之间的订单还是太多，无法一次性读入内存，那就继续再划分，直到所有的文件都能读入内存为止。 计数排序（Counting sort） 计数排序其实是桶排序的一种特殊情况。当要排序的 n 个数据，所处的范围并不大的时候，比如最大值是 k，就可以把数据划分成 k 个桶。每个桶内的数据值都是相同的，省掉了桶内排序的时间。 核心思想 查询高考成绩，系统会显示分数和所在省的排名。如果所在的省分有50万考生，如何通过成绩快速排序得出名次呢？ 假设考生的满分是 900 分，最小是 0 分，这个数据的范围很小，所以可以分成 901 个桶，对应分数从 0 分到 900 分。根据考生的成绩，将这 50 万考生划分到这 901 个桶里。桶内的数据都是分数相同的考生，所以并不需要再进行排序。最后只需要依次扫描每个桶，将桶内的考生依次输出到一个数组中，就实现了 50 万考生的排序。因为只涉及扫描遍历操作，所以时间复杂度是 O(n)。 这就是计数排序的算法思想，跟桶排序非常类似，只是桶的大小粒度不一样。 为什么叫计数排序 要了解计数的含义，就需要明白计算排序算法的实现方法。 还是那个栗子 假设只有 8 个考生了，分数在 0 到 5 分之间。将这 8 个考生的成绩放在一个数组 A[8] 中，它们分别是：2，5，3，0，2，3，0，3分。 #数组A8 数组 [2] [5] [3] [0] [2] [3] [0] [3] 下标 0 1 2 3 4 5 6 7 考生的成绩从 0 到 5 分，使用大小为 6 的数组 C[6] 表示桶，其中下标对应分数。 C[6] 中的每个元素存对应分数的考生个数。 #数组C6： 数组-出现次数： [2] [0] [2] [3] [0] [1] 下标-对应分数： 0 1 2 3 4 5 现在的问题是，如何快速计算出，每个分数的考生在有序数组 (最后排好序的结果数组) 中对应的存储位置呢？ 巧妙的思路 1 . 首先对 C6 数组顺序求和（前面一个位置的值+当前自己本身的值 = 当前的新值），得到新的 C6 数组： #新的数组C6： 数组-出现次数： [2] [2] [4] [7] [7] [8] 下标-对应分数： 0 1 2 3 4 5 2 . 然后从后到前依次扫描数组 A。 过程如下：扫描到第一个3时，从数组 C 中取出下标为3的值，即7， 7在这里的含义是，到目前位置，包括当前这个在内，小于等于3分的考生个数是7个，即3是结果数组 R 中的第7个元素（对应于数组下标为6的位置R[6] = 3）。将这个3放进 R 数组后，数组 C 中小于等于3的元素个数减一就变成6个，即 C[3] = 6。以此推类，扫描完整个数组A后，数组 R 内的数据就是按照分数从小到大有序排列。 过程图： 代码 from typing import List import itertools def counting_sort(a: List[int]): if len(a) &lt;= 1:return counts = [0] * (max(a) + 1) #创建桶，初始值为0 for num in a: #统计数组中每个元素出现的次数 counts[num] += 1 counts = list(itertools.accumulate(counts)) #对counts数组顺序求和，得到新的counts数组 a_sorted = [0] * len(a) #创建一个临时结果数组，存储排序后的结果 for num in reversed(a): #从后往前扫描需要被排序的数组 index = counts[num] - 1 #找到num在临时结果数组中位置 a_sorted[index] = num #将num放到临时结果数组中 counts[num] -= 1 #num对应的个数减1 a = a_sorted #将临时结果数组赋给原数组 适用场景 计数排序只能用在数据范围不大的场景中，如果数据范围 k 比要排序的数据 n 大很多，就不适合用计数排序了。而且，计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数。 栗子说明 比如，如果考生成绩精确到小数后一位，就需要将所有的分数都先乘以 10，转化成整数，然后再放到 9010 个桶内。再比如，如果要排序的数据中有负数，数据的范围是 [-1000, 1000]，那就需要先对每个数据都加 1000，转化成非负整数。 基数排序（Radix sort） 首先按照最低有效位进行排序，最低位优先 (Least Significant Digit first) 法，简称 LSD 法：先从kd开始排序，再对kd-1进行排序，依次重复，直到对k1排序后便得到一个有序序列。 应用 假设有 10 万个手机号码，希望将这 10 万个手机号码从小到大排序，什么排序方法比较快速？ 手机号码有 11 位，范围太大，显然不适合用桶排序和计数排序两种算法，基数排序就很适合。 处理思路 先按照最后一位来排序手机号码，然后，再按照倒数第二位重新排序，以此类推，最后按照第一位重新排序。经过 11 次排序之后，手机号码就都有序了。 注意，这里按照每位来排序的排序算法要是稳定的，否则这个实现思路就是不正确的。因为如果是非稳定排序算法，那最后一次排序只会考虑最高位的大小顺序，完全不管其他位的大小关系，那么低位的排序就完全没有意义了。 根据每一位来排序，可以用桶排序或者计数排序，它们的时间复杂度可以做到 O(n)。如果要排序的数据有 k 位，那就需要 k 次桶排序或者计数排序，总的时间复杂度是 O(k*n)。当 k 不大的时候，比如手机号码排序，k 最大就是 11，所以基数排序的时间复杂度就近似于 O(n)。 实际上，有时候要排序的数据并不都是等长的。 解决办法 可以把所有的数字补齐到相同长度，位数不够的可以在前面补0。而对于不等长的字符串排序，位数不够的可以在后面补&quot;0&quot;，因为根据ASCII 值，所有字母都大于&quot;0&quot;，所以补&quot;0&quot;不会影响到原有的大小顺序。这样就可以继续用基数排序了。 代码 def radix_sort(lt, d): #d表示d轮排序,取决于数组元素的长度 for k in xrange(d): s = [[] for i in xrange(10)] #创建10个桶，因为每位数字最大的就是9 #对数组中每个元素，按照最低有效数字进行排序，然后依次到高位 for i in lt: s[i/(10**k)%10].append(i) lt = [j for i in s for j in i] return lt 例如 对数组[321,22,890]排序，第一轮对个位排，s[0]=[890],s[1]=[321],s[2]=[22]，第一轮排序结果lt[890,321,22] 第二轮对十位排，s[2] =[321,22],s[9]=[890]，第二轮排序结果lt[321,22,890] 第三轮百位排，s[0] =[22],s[3]=[321],s[8]=[890]，第三轮排序结果lt[22,321,890]，结束。 适用场景 基数排序对要排序的数据是有要求的，需要可以分割出独立的“位”来比较，而且位之间有递进的关系，如果 a 数据的高位比 b 数据大，那剩下的低位就不用比较了。除此之外，每一位的数据范围不能太大，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到 O(n) 了。 解答开篇 实际上，根据年龄给 100 万用户排序，就类似按照成绩给 50 万考生排序。 假设年龄的范围最小 1 岁，最大不超过 120 岁。 遍历这 100 万用户，根据年龄将其划分到120 个桶里，然后依次顺序遍历这 120 个桶中的元素。 这样就得到了按照年龄排序的 100 万用户数据。]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>data_structure</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序]]></title>
    <url>%2F2018-11-02%2F</url>
    <content type="text"><![CDATA[之前说的，冒泡排序、插入排序、选择排序三种算法的时间复杂度都是O(n^2)，很高，适用于小规模数据的排序。 而，归并排序和快速排序，适用于大规模的数据排序，更为常用。 问题 归并排序和快速排序都用到了分治思想，借助这个思想可以解决非排序问题。 如何在O(n)的时间复杂度内查找一个无序数组中的第K大元素？ 归并排序 核心思想 对于要排序的数组，先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起。 归并排序用到的就是分治思想，顾名思义，就是分而治之，将大问题分解为小的子问题来解决。 分治思想跟递归很向。分治算法一般都用递归来实现的。分治是一种解决问题的处理思想，递归是一种编程技巧。 如何用递归代码实现归并排序？ 先写出归并排序的递推公式 递推公式： merge_sort(p...q) = merge(merge_sort(p...r), merge_sort(r+1...q)) #r = (p+q)/2,即数组的中间位置 终止条件： p &gt;= q #不用再继续分解的时候 代码 def merge_sort(lt): if len(lt) &lt;= 1: return lt middle = len(lt)/2 left = merge_sort(lt[:middle]) right = merge_sort(lt[middle:]) return merge(left, right) def merge(arr1, arr2): arr_result = [] while len(arr1)&gt;0 and len(arr2)&gt;0: if arr1[0] &lt;= arr2[0]: arr_result.append(arr1.pop(0)) else: arr_result.append(arr2.pop(0)) #while循环结束，说明有个条件不满足，即有个数组已经没有数据元素了，此时将另一个数组的数据加入到结果数组中 arr_result += arr1 arr_result += arr2 return arr_result 分析 1 . 归并排序是否是稳定排序算法？关键看merge()函数，即两个有序子数组合并成最终结果数组的那部分代码。在合并过程中，如果arr1[p...r]和arr2[r+1...q]之间有相同元素，那么可以先把arr1[p...r]中的元素放入最终结果数组。这样就保证了值相同的元素，在合并前后的先后顺序不变。所以，归并排序是稳定的排序算法。 2 . 时间复杂度分析。归并排序涉及递归，时间复杂度该如何分析？ 递归适用场景：一个问题A可以分解为多个子问题B、C，求解A就就分解为求解B、C。问题B、C解决后，在把这个问题的结果合成A的结果。 若定义求解问题A的时间为T(A)，求解问题B、C的时间分别为T(B)和T(C)，则有一个递推关系 T(A) = T(B) + T(C) + K 其中K为两个子问题B、C的结果合成问题A的结果所花费时间 套用这个公式来分析归并排序的时间复杂度： 假设对 n 个元素进行归并排序需要的时间是 T(n)，那分解成两个子数组排序的时间都是 T(n/2)。merge() 函数合并两个有序子数组的时间复杂度是 O(n)。所以，套用前面的公式，归并排序的时间复杂度的计算公式就是： T(1) = C; n=1 时，只需要常量级的执行时间 T(n) = T(n/2) *2 + n; n&gt;1 进一步分解可知道： T(n) = 2*T(n/2) + n = 2*(2*T(n/4) + n/2) + n = 4*T(n/4) + 2*n = 4*(2*T(n/8) + n/4) + 2*n = 8*T(n/8) + 3*n = 8*(2*T(n/16) + n/8) + 3*n = 16*T(n/16) + 4*n ...... = 2^k * T(n/2^k) + k * n ...... 即 T(n) = 2^k * T(n/2^k) + k * n, 当 T(n/2^k) = 1 时，k = log2^n 则 T(n) = 2^log2^n + log2^n * n = Cn + n*log2^n 用大O标记法表示，T(n) = O(nlogn) 归并排序的执行效率与要排序的原始数组的有序程度无关，所以其时间复杂度是非常稳定的，不管是最好情况、最坏情况，还是平均情况，时间复杂度都是 O(nlogn)。 3 . 空间复杂度分析。归并排序的时间复杂度在任何情况下都是 O(nlogn)，看起来非常优秀，但是它并没有像快排那样应用广泛，其原因就是因为它有一个致命弱点，归并排序不是原地排序算法。主要原因在于merge()函数，需要借助额外的存储空间。 分析递归代码的空间复杂度并不能像时间复杂度那样累加。尽管每次合并操作都需要申请额外的内存空间，但在合并完成之后，临时开辟的内存空间就被释放掉了。在任意时刻，CPU 只会有一个函数在执行，也就只会有一个临时的内存空间在使用。临时内存空间最大也不会超过 n 个数据的大小，所以空间复杂度是 O(n)。 快速排序 核心思想 如果要排序数组中下标从 p 到 q 之间的一组数据，我们选择 p 到 q 之间的任意一个数据作为 pivot（分区点）（可选择末尾数字）。 遍历 p 到 q 之间的数据，将小于 pivot 的放到左边，将大于 pivot 的放到右边，将 pivot 放到中间。经过这一步骤之后，数组 p 到 q 之间的数据就被分成了三个部分，前面 p 到 r-1 之间都是小于 pivot 的，中间是 pivot，后面的 r+1 到 q 之间是大于 pivot 的。 其中一次排序步骤如下： 递归排序下标从 p 到 r-1 之间的数据和下标从 r+1 到 r 之间的数据，直到区间缩小为 1，就说明所有的数据都有序了。 递推公式 quick_sort(p...q) = quick_sort(p...r-1) + quick_sort(r+1...q) 终止条件 p &gt;= q 代码 partition() 函数不需要额外的内存空间，保证快排是原地排序算法。 def quick_sort(lt, lindex, rindex): if lindex &lt; rindex: pivot = partition(lt, lindex, rindex) quick_sort(lt, lindex, pivot) quick_sort(lt, pivot+1, rindex) else: return def partition(lt, lindex, rindex): i = lindex - 1 for j in range(lindex, rindex): if lt[j] &lt;= lt[rindex]: #最后一个数据为pivot i += 1 lt[i],lt[j] = lt[j], lt[i] lt[i+1], lt[rindex] = lt[rindex], lt[i+1] #将分区点交换到数组中间位置 return i 分析 1 . 快速排序是原地排序算法，即空间复杂度为O(1)。但在分区的过程涉及交换操作，如果数组中有两个相同的元素，在经过一次分区操作后，元素的相对先后顺序会改变。所以，快排不是一个稳定的排序算法。 2 . 时间复杂度分析。快排也是基于递归思想，前面的递归公式在这里仍适用。如果每次分区操作，都能正好把数组分成大小接近相等的两个小区间，那快排的时间复杂度递推求解公式跟归并是相同的。所以，快排的时间复杂度也是 O(nlogn)。但是，公式成立的前提是每次分区操作，选择的 pivot 都很合适，正好能将大区间对等地一分为二。但实际上这种情况是很难实现的。如果以最后一个元素作为pivot，需要进行大约 n 次分区操作，才能完成快排的整个过程。每次分区平均要扫描大约 n/2 个元素，这种情况下，快排的时间复杂度就从 O(nlogn) 退化成了 O(n^2)。 以上两种情况分别对应于最好情况和最坏情况，在大部分情况下快排的时间复杂度都可以做到 O(nlogn)，只有在极端情况下，才会退化到 O(n^2)。而且，有方法将这个极端情况的概率降到很低。 快速排序和归并排序的区别 快排和归并都用到分治思想，地推公式和递归代码也相似，它们的区别在于：归并排序的处理过程是由下到上的，先处理子问题，然后再合并；而快排正好相反，它的处理过程是由上到下的，先分区，再处理子问题。 归并排序虽然是稳定的、时间复杂度为 O(nlogn) 的排序算法，但是它是非原地排序算法，主要原因是合并函数无法在原地执行。快速排序通过设计巧妙的原地分区函数，可以实现原地排序，解决了归并排序占用太多内存的问题。 解答开篇 利用分区思想，在O(n)时间复杂度时间内求无序数组中的第 K 大元素。 选择数组arr[0,n-1]的最后一个元素作为pivot，对数组arr[0,n-1]原地分区，数组分成三部分，arr[0...p-1], arr[p], arr[p+1...n-1]。如果 p+1=K，那 arr[p] 就是要求解的元素；如果 K&gt;p+1, 说明第 K 大元素出现在 arr[p+1…n-1] 区间，再按照上面的思路递归地在 arr[p+1…n-1] 这个区间内查找。同理，如果 K&lt;p+1，那就在 arr[0…p-1] 区间查找。 为什么上述解决思路的时间复杂度是 O(n)？ 第一次分区查找，需要对大小为 n 的数组执行分区操作，需要遍历 n 个元素。第二次分区查找，只需要对大小为 n/2 的数组执行分区操作，需要遍历 n/2 个元素。依次类推，分区遍历元素的个数分别为、n/2、n/4、n/8、n/16.…… 直到区间缩小为 1。 把每次分区遍历的元素个数加起来，就是：n+n/2+n/4+n/8+…+1 。这是一个等比数列求和，最后的和等于 2n-1。所以，上述解决思路的时间复杂度就为 O(n)。 另一种思路 每次取数组中的最小值，将其移动到数组的最前面，然后在剩下的数组中继续找最小值，以此类推，执行 K 次，找到的数据就是第 K 大元素。 思路是对的，但时间复杂度就不是 O(n) 了，而是 O(K * n)。当 K 是比较小的常量时，比如 1、2，那最好时间复杂度确实是 O(n)；但当 K 等于 n/2 或者 n 时，这种最坏情况下的时间复杂度就是 O(n^2) 了。]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>data_structure</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序]]></title>
    <url>%2F2018-10-30%2F</url>
    <content type="text"><![CDATA[问题思考 插入排序和冒泡排序的时间复杂度相同，都是O(n^2)，在实际软件开发里，为什么更倾向于使用插入排序算法而不是冒泡排序算法呢？ 如何分析一个排序算法 排序算法的执行效率 1 . 最好情况、最坏情况、平均情况时间复杂度 分析排序算法的时间复杂度是，要分别给出最好情况、最坏情况、平均情况下的时间复杂度。除此之外，还要说出最好、最坏时间复杂度对应的要排序的原始数据长什么样。 2 . 时间复杂度的系数、常数、低阶 尽管表示时间复杂度时，忽略了系数、常数、低阶。但实际软件开发中，排序的数据可能是10个、100个、1000个这样的小规模数据，因而对于同一阶时间复杂度的排序算法，在比较时，应该把系数、常数和低阶考虑进来。 3 . 比较次数和交换（移动）次数 基于比较的排序算法在执行过程中，会涉及到两种操作，一种是比较元素大小，另一种是元素交换或移动。 排序算法的内存消耗 内存消耗可以通过空间复杂度来衡量。针对排序算法的空间复杂度，引入一个新概念，原地排序（Sorted in place）。原地排序算法，就是特指空间复杂度是O(1)的排序算法。 插入排序、冒泡排序、选择排序都是原地排序算法。 排序算法的稳定性 稳定性指：如果待排序的序列中存在值相等的元素，经过排序后，相等元素之间原有的先后顺序不变。 相等元素之间原有的先后顺序没有变，这种排序算法叫作稳定的排序算法；否则，叫作不稳定的排序算法。 为什么要考察排序算法的稳定性？ 场景举栗 比如说，我们现在要给电商交易系统中的“订单”排序。订单有两个属性，一个是下单时间，另一个是订单金额。 如果我们现在有 10 万条订单数据，我们希望按照金额从小到大对订单数据排序。对于金额相同的订单，我们希望按照下单时间从早到晚有序。对于这样一个排序需求，我们怎么来做呢？ 思路分析 最先想到的方法是：先按照金额对订单数据进行排序，然后，再遍历排序之后的订单数据，对于每个金额相同的小区间再按照下单时间排序。这种排序思路理解起来不难，但是实现起来会很复杂。 更好的方法是：借助稳定排序算法。先按照下单时间给订单排序，注意是按照下单时间，不是金额。排序完成之后，再用稳定排序算法，按照订单金额重新排序。两遍排序之后，得到的订单数据就是按照金额从小到大排序，金额相同的订单按照下单时间从早到晚排序的。 原因：稳定排序算法可以保持金额相同的两个对象，在排序之后的前后顺序不变。第一次排序之后，所有的订单按照下单时间从早到晚有序了。在第二次排序中，因为用的是稳定的排序算法，所以经过第二次排序之后，相同金额的订单仍然保持下单时间从早到晚有序。 冒泡排序 排序过程 冒泡排序只会操作相邻的两个数据。每次冒泡操作都会对相邻的两个元素进行比较，看是否满足大小关系要求。如果不满足就让它俩互换。一次冒泡会让至少一个元素移动到它应该在的位置，重复 n 次，就完成了 n 个数据的排序工作。 优化过程 当某次冒泡操作已经没有数据交换时，说明已经达到完全有序，就不用再继续执行后续的冒泡操作。 代码 def bubble_sort(lt): length = len(lt) flag = False for i in range(0,length): for j in range(i+1,length): if lt[i] &gt; lt[j]: lt[i], lt[j] = lt[j], lt[i] #数据交换 flag = True if not flag: break return lt 分析 1 . 冒泡排序只涉及相邻数据的交换操作，只需常量级的临时空间，空间复杂度为O(1)，所以是一个原地排序算法。 2 . 冒泡排序中只有交换才改变两个元素的前后顺序。为了保证其稳定性，当相邻元素相等时，不做交换，那么相同大小的数据在排序前后不会改变顺序。所以冒泡排序是稳定的排序算法。 3 . 最好情况下，排序数据已经是有序的，只进行一次冒泡操作，所以时间复杂度是O(n)；最坏情况是，排序的数据刚好是倒序排列的，需要进行n次冒泡操作，所以时间复杂度为O(n^2)； 平均时间复杂度，采用一种不严格的方法，通过有序度和逆序度两个概念来分析。 有序度：数组中具有有序关系的元素对的个数。（默认，从小到大是有序的） 例：2,4,3,5这组数据的有序度为：5，分别是(2,4),(2,3),(2,5),(4,5),(3,5)；同理，对于一个倒序排列的数组，有序度为0；对于一个完全有序的数组，比如2,3,4,5，有序度就是n*(n-1)/2，也就是6。把这种完全有序的数组的有序度叫作满有序度。 逆有序度：其定义跟有序度正好相反。 逆有序度 = 满有序度 - 有序度 排序的过程就是一种增加有序度，减少逆序度的过程，最后达到满有序度，说明排序完成。 冒泡排序中，包含两个操作，比较和交换。每交换一次，有序度就加1。不管算法怎么改进，交换次数总是确定的，即为逆序度，也就是等于n*(n-1)/2 - 初始有序度。对于2,4,3,5这组数据来说，6-5=1，只需进行1次交换操作。 那么对于包含n个数据的数组进行冒泡排序，平均交换次数是多少呢？ 最坏情况：初始有序度为0，需要进行n*(n-1)/2次交换 最好情况：初始有序度为 n*(n-1)/2，需要进行0次交换 平均情况：取一个中间值 n*(n-1)/4，来表示初始有序度既不是很高也不是很低的平均情况。换句话说，平均情况下，需要 n*(n-1)/4 次交换操作，比较操作肯定比交换操作更多，而时间复杂度的上限是 O(n^2)，所以平均情况下的时间复杂度就是 O(n^2)。 插入排序 对于一个原本有序的数组，往里面加一个新数据后，如何继续保持数据的有序呢？很简单，只需要遍历数组，找到数据应该插入的位置将其插入即可。 插入排序就是借助这个思想来实现排序的。 排序过程 首先，将数组中的数据分为两个区间，已排序区间和未排序区间。初始已排序区间只有一个元素，就是数组的第一个元素。插入算法的核心思想是取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。重复这个过程，直到未排序区间中元素为空，算法结束。 两种操作 一种是元素的比较，另一种是元素的移动。对于不同的查找插入点方法（从头到尾，从尾到头），元素的比较次数是有区别的。但对于一个给定的初始序列，移动操作的次数总是固定的，即为逆序度。（为什么移动次数=逆序度，可以拿个实例画一个图，很容易明白） 代码 def insertion_sort(lt): length = len(lt) for i in range(1, length): value = lt[i] for j in range(i-1,-1,-1): if lt[j] &gt; value: lt[j+1] = lt[j] #数据移动 else: break #位置确定 lt[j+1] = value #插入数据 return lt 分析 1 . 插入排序算法的运行并不需要额外的存储空间，空间复杂度为 O(1)，所以是一个原地排序算法。 2 . 插入排序中，对于值相同的元素，可以选择将后面出现的元素，插入到前面出现元素后面，保持原有前后顺序不变。所以插入排序是稳定的排序算法。 3 . 时间复杂度分析。 最好情况：数据已经有序，不需要搬移任何数据。如果从尾到头在有序数组里查找插入位置，每次只需比较一个数据就能确定插入位置。时间复杂度为 O(n)； 最坏情况：数组是倒序的，每次插入都相当于在数组的第一个位置插入新数据，所有需要移动大量数据。时间复杂度为 O(n^2)； 平均情况：在数组中插入一个数据的平均时间复杂度为 O(n)，对于插入排序来说，每次插入操作就相当于在数组中插入一个数据，循环执行 n 次插入操作。所以平均时间复杂度为 O(n^2)。 选择排序 思路与插入排序类似，也分已排序区间和未排序区间。但是选择排序每次会从未排序区间中找到最小的元素，将其放到已排序区间末尾。最开始没有已排好的区间，找到数组中最小元素，将其与第一个元素交换，然后再执行以上过程。 代码 def selection_sort(lt): length = len(lt) for i in range(0,length-1): smallest_index = i for j in range(i+1, length): if lt[j] &lt; lt[smallest_index]: lt[j], lt[smallest_index] = lt[smallest_index], lt[j] return lt 分析 1 . 选择排序空间复杂度为O(1)，是一种原地排序算法。 2 . 时间复杂度分析。最好情况、最坏情况和平均时间复杂度都为 O(n^2)。 3 . 选择排序是一种不稳定的排序算法。因为其每次都要找剩余未排序元素中的最小值，并和前面的元素交换位置，这样就破坏了稳定性。也因为此，相比于冒泡排序和插入排序，选择排序没那么好。 解答开篇 冒泡排序和插入排序的时间复杂度都是 O(n2)，都是原地排序算法，为什么插入排序要比冒泡排序更受欢迎呢？ 前面说过，冒泡排序不管怎么优化，元素交换的次数是一个固定值，是原始数据的逆序度。插入排序是同样的，不管怎么优化，元素移动的次数也等于原始数据的逆序度。 但是，从代码实现上来看，冒泡排序的数据交换要比插入排序的数据移动更复杂，冒泡排序需要 3 个赋值操作，而插入排序只需要 1 个。所以在对相同数组进行排序时，冒泡排序的运行时间理论上要长于插入排序。]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>data_structure</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[递归]]></title>
    <url>%2F2018-10-29%2F</url>
    <content type="text"><![CDATA[应用场景 很多APP都有推荐用户注册返佣金或奖励的功能。在这个功能中，用户A推荐用户B来注册，用户B又推荐用户C来注册。那么用户C的最终推荐人就为A，用户B的最终推荐人也为A，用户A没有最终推荐人。在数据库表中，可以记录两行数据，其中user_id表示用户ID，referrer_id表示推荐人ID。 那么，问题是，给定一个用户ID，如何查找这个用户的最终推荐人？ 应用到的思想就是递归 如何理解递归 很多数据结构和算法的实现都要用到递归，比如DFS深度优先搜索、前中后序二叉树遍历等。 一个生活中的例子，在电影院由于太黑，你看不清自己在第几排，但是又想知道，怎么办呢？问前一排的人，他的排数+1就是你的排数。但是前一排的人也看不清自己在第几排，他又通过问自己的前一排，就这样一直传递问下去，直到第一排的人说我是第一排，于是又一排一排把数字传回来。 这样一个过程就是递归求解问题的分解过程，去的过程叫递，回来的过程叫归。基本上，所有的递归问题都可以用递推公式来表示。 f(n) = f(n-1) + 1, 其中，f(1) = 1 什么样的问题能用递归来解决？ 同时满足以下三个条件，就可用递归来解决。 一个问题的解可以分解为几个子问题的解。 子问题是指数据规模更小的问题 想知道你“自己在哪一排”，可以分解为“前一排的人在哪一排”这个子问题 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样。 你求解“自己在哪一排”，与前面一排求解“自己在哪一排”的思路是相同的。 存在递归终止条件 问题分解为子问题，子问题再一层层分解下去，但是不能无限循环，必须有终止条件。 电影院第一排的人知道自己在哪一排，即f(1) = 1,这就是递归终止条件。 编写递归代码 关键点：写出递归公式，找到终止条件。 栗子 假设有n个台阶，每次可以跨1个台阶或者2个台阶，那么请问走完这n个台阶，共有多少种走法？ 分析 可以根据第一步的走法把所有走法分为两类，第一类是，第一步走1个台阶；第二类是，第一步走2个台阶。 所以，n个台阶的走法就等于，先走1阶后，n-1个台阶的走法，加上，先走2阶后，n-2个台阶的走法。 公式表示为： f(n) = f(n-1) + f(n-2) 有了递推公式还不够，再分析终止条件：当有1个台阶时，就只有一个走法，所以f(1) = 1。用小规模数试验一下，该终止条件是否合理，当n=2时，f(2) = f(1) + f(0)。发现f(2)没法求解，因为没给f(0)的值。可以给定f(0) = 0，表示走0个台阶有1种走法，但这不符合常识，因而可以直接给定f(2) = 2，表示走2个台阶有2种走法，要么一步1个台阶走，要么一次跨2个台阶。 这样再试验f(3) = f(2) + f(1)，可以得出结果并正确。所以，递归终止条件就为f(1)= 1, f(2) = 2。 最终公式 f(1)= 1 f(2) = 2 f(n) = f(n-1) + f(n-2) 递归代码 def climbStairs(self,n): if n==1: return 1 elif n==2: return 2 else: return self.climbStairs(n-1) + self.climbStairs(n-2) Key 对于递归代码，试图想清楚整个递和归过程的做法，实际上是进入了一个思维误区。很多时候，理解起来比较吃力，主要原因就是自己给自己制造了这种理解障碍。 正确的思维方式应该是: 如果一个问题 A 可以分解为若干子问题 B、C、D，就假设子问题 B、C、D 已经解决，在此基础上思考如何解决问题 A。而且，只需要思考问题 A 与子问题 B、C、D 两层之间的关系即可，不需要一层一层往下思考子问题与子子问题，子子问题与子子子问题之间的关系。屏蔽掉递归细节，这样子理解起来就简单多了。 因此，编写递归代码的关键是，只要遇到递归，就把它抽象成一个递推公式，不用想一层层的调用关系，不要试图用人脑去分解递归的每个步骤。 注意问题 1 . 递归代码要警惕堆栈溢出。函数调用会使用栈来保存临时变量。每调用一个函数，都会将临时变量封装为栈帧压入内存栈，等函数执行完成返回时，才出栈。系统栈或者虚拟机栈空间一般都不大。如果递归求解的数据规模很大，调用层次很深，一直压入栈，就会有堆栈溢出的风险。 解决思路：可以通过在代码中限制递归调用的最大深度的方式来解决这个问题。递归调用超过一定深度（比如 1000）之后，我们就不继续往下再递归了，直接返回报错。 2 . 递归代码要警惕重复计算。刚刚的例子中，就存在这个问题，比如要计算f(5)，就需要计算f(4)和f(3)，而计算f(4)，需要计算f(3)和f(2)，这个过程中f(3)就被计算了多次。 解决思路：通过一个数据结构（散列表）来保存已经求解过的f(i)。当递归调用到f(i)时，先查找这个值是否已经求解。若是，则直接从散列表中取值返回，避免重复计算。 修改栗子中的代码 def climbStairs(self,n): hash_list = [0,1,2] if n==1: return hash_list[1] elif n==2: return hash_list[2] else: for i in range(3, n+1): hash_list.append(hash_list[i-1] + hash_list[i-2]) return hash_list[n] 3 . 时间和空间成本很高。在时间效率上，递归代码里多了很多函数调用，当这些函数调用的数量较大时，就会积聚成一个可观的时间成本。在空间复杂度上，因为递归调用一次就会在内存栈中保存一次现场数据，所以在分析递归代码空间复杂度时，需要额外考虑这部分的开销，比如前面的电影院递归代码，空间复杂度并不是 O(1)，而是 O(n)。]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>data_structure</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[队列]]></title>
    <url>%2F2018-10-16%2F</url>
    <content type="text"><![CDATA[引言 CPU资源有限，任务的处理速度与线程个数并不是线性正相关。过多的线程会导致CPU频繁切换，处理性能下降。 当我们向固定大小的线程池中请求一个线程时，如果线程池中没有空闲资源了，这个时候线程池如何处理这个请求？是拒绝请求还是排队请求？各种处理策略又是怎么实现的呢？ 这就需要用到队列这个数据结构 如何理解队列 理解成，排队购票，排在前面的先买，排在后面的后买。即先进者先出（FIFO）。 队列跟栈非常相似，支持的操作也有限，最基本的也是两个：入队和出队，一端出队，另一端入队；所以队列也是一种操作受限的线性表数据结构。 顺序队列和链式队列 用数组实现的队列叫作顺序队列，用链表实现的队列叫作链式队列。 顺序队列 不理想的设计： 1 . 若使用顺序表的尾端插入实现enqueue操作，根据队列性质，出队操作应该在表的首端进行。为了维护顺序表的完整性（表元素在表前端连续存放），出队操作取出当时的首元素后，就需要把表中其余元素全部前移，这样就会是一个 O(n) 时间的操作。 2 . 反过来：从尾端出队是 O(1) 操作，但从首端入队就是 O(n) 时间操作，这种设计也不理想。 3 . 另一种是在队首元素出队后表中的元素不前移，但记住新队头位置。如果队列中没有空闲了，只需要在入队时，再集中触发一次数据的搬移操作。 链式队列 最简单的单链表只支持首端 O(1) 的操作，在另一端操作需要 O(n) 时间。不适合作为队列的实现基础。 考虑带表尾指针的单链表，它支持 O(1) 时间的尾端插入操作；再加上表首端的高效访问和删除，基于单链表实现队列就很容易。 示例 class LNode: def __init__(self, elem, next_=None): self.data = elem self.next = next_ class QueueUnderflow(ValueError): pass class LQueue: def __init__(self): self._head = None self._rear = None def is_empty(self): return self._head is None def peek(self): &quot;&quot;&quot;查看队列最早元素，不删除&quot;&quot;&quot; if self._head is None: #是空队列 raise QueueUnderflow('in peek of Queue') else: return self._head.data def dequeue(self): &quot;&quot;&quot;删除队列头结点，并返回这个结点里的数据&quot;&quot;&quot; if self._head == None: raise QueueUnderflow(&quot;in dequeue&quot;) e = self._head.data self._head = self._head.next return e def enqueue(self, elem): if self._head is None:#空表 self._head = LNode(elem, self._head) self._rear = self._head else: self._rear.next = LNode(elem) self._rear = self._rear.next 循环队列 一个具体的实现示例：基于Python的list实现顺序表示的循环队列。 考虑定义一个可以自定扩充存储结构的队列类。 注：不能直接利用list的自动存储扩充机制。两个原因： 1 . 队列元素的存储方式与list元素的默认存储方式不一致；list元素总在其存储器的最前面一段，而队列的元素可能是表里的任意一段，有时还分为头尾两段。 2 . list没有提供检测元素存储区容量的机制，队列操作中无法判断系统何时扩容。 示例 class SQueue(): def __init__(self, init_len = 8): self._len = init_len #存储区长度 self._elems = [0] *init_len #元素存储 self._head = 0 #表头元素下标 self._num = 0 #元素个数 def is_empty(self): return self._num == 0 def peek(self): if self._num is None: #是空队列 raise QueueUnderflow('in peek of SQueue') return self._elems[self._head] def dequeue(self): if self._num == 0: raise QueueUnderflow('in dequeue of SQueue') e = self._elems[self._head] self._head = (self._head+1)%self._len self._num -= 1 return e def enqueue(self,e): if self._num == self._len: #队满时 self._extend() self._elems[(self._head+self._num)%self._len] = e self._num += 1 def _extend(self): old_len = self._len self._len *= 2 new_elems = [0]*self._len #扩大元素存储区 for i in range(old_len): #将原有元素搬迁到新表里（最前面的位置） new_elems[i] = self._elems[(self._head+1)%old_len] self._elems, self._head = new_elems, 0 注解 1 . 队列对象的4个属性，_elems，_head，_num，_len的作用分别是：存放队列元素，记录队列首元素所在位置的下标，记录表中元素个数，记录当存储区的有效容量（便于换存储表）。 2 . 在_num = _len 的情况下（队满）出现入队操作，就扩大存储区；队空就是 _num == 0。 3 . 队列里的元素总保存在_elems里，从_head开始的连续位置中。 4 . 新入队的元素存入在 (_head + _num)%len 算出的位置；若需要把元素存入下标_len的位置时，改为在下标0位置存入。 5 . 在_extend函数中新元素尚未入队，但_extend在enqueue返回后，enqueue的最后两句语句将正常完成这个工作。 阻塞队列 阻塞队列其实就是在队列基础上增加了阻塞操作。简单来说，就是在队列为空的时候，从队头取数据会被阻塞。因为此时还没有数据可取，直到队列中有了数据才能返回；如果队列已经满了，那么插入数据的操作就会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回。 注：可以用阻塞队列实现一个“生产者-消费者模型”。基于阻塞队列，可以通过协调“生产者”和“消费者”的个数，来提高数据的处理效率。 并发队列 在多线程情况下，会有多个线程同时操作队列，这个时候就会存在线程安全问题。 要实现一个线程安全的队列就需要并发队列。 最简单直接的实现方式是直接在 enqueue()、dequeue() 方法上加锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存或者取操作。 解答引言 一般有两种处理策略。 第一种是非阻塞的处理方式，直接拒绝任务请求； 另一种是阻塞的处理方式，将请求排队，等到有空闲线程时，取出排队的请求继续处理。 那如何存储排队的请求呢？ 公平地处理每个排队的请求，先进者先服务，所以队列这种数据结构很适合来存储排队请求。 队列有基于链表和基于数组这两种实现方式。两种实现方式对于排队请求又有什么区别呢？ 基于链表的实现方式 可以实现一个支持无限排队的无界队列（unbounded queue），但是可能会导致过多的请求排队等待，请求处理的响应时间过长。 所以，针对响应时间比较敏感的系统，基于链表实现的无限排队的线程池是不合适的。 基于数组的实现方式 可以实现的是有界队列（bounded queue），队列的大小有限，所以线程池中排队的请求超过队列大小时，接下来的请求就会被拒绝，这种方式对响应时间敏感的系统来说，就相对更加合理。 这时，设置一个合理的队列大小，就非常重要。队列太大导致等待的请求太多，队列太小会导致无法充分利用系统资源、发挥最大性能。 注：对于大部分资源有限的场景，当没有空闲资源时，基本上都可以通过队列这种数据结构来实现请求排队。 思考 队列的其他应用 1 . 文件打印 2 . 万维网服务器 3 . Windows系统和消息队列 4 . 离散事件系统模拟]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>data_structure</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[栈]]></title>
    <url>%2F2018-10-13%2F</url>
    <content type="text"><![CDATA[思考 首先思考一个问题，浏览器的前进、后退功能是如何实现的呢？ 理解栈 栈结构：先进的后出，后进的先出。类似于洗好的盘子，叠一摞，下次用的时候只能从最上面那个盘子开始拿。 操作特性：操作受限的线性表 什么时候用：当某个数据集合只涉及在一端插入和删除数据，并且满足后进先出的特性，就应该首选栈这种结构。 如何实现栈 栈既可以用数组实现，也可以用链表来实现。用数组实现的栈，叫作顺序栈，用链表实现的栈，叫作链栈。 示例：顺序栈 class Stack(): def __init__(self,size): &quot;&quot;&quot;初始化&quot;&quot;&quot; self.size = size self.num = 0 self.stack = [] def getSize(self): &quot;&quot;&quot;获取栈的长度&quot;&quot;&quot; return self.num def print_all(self): &quot;&quot;&quot;输出栈元素&quot;&quot;&quot; for s in self.stack: print s def append_stack(self,value): &quot;&quot;&quot;入栈&quot;&quot;&quot; if self.num &gt;= self.size: print(&quot;the stack is full&quot;) return else: self.stack.append(value) self.num += 1 def pop_stack(self): &quot;&quot;&quot; 出栈&quot;&quot;&quot; if self.num is None: print(&quot;the stack is empty&quot;) return else: self.stack.remove(self.stack[-1]) 复杂度分析 空间复杂度 无论是顺序栈还是链栈，存储数据只需要一个大小为n的数组。在入栈和出栈过程中，只需要一两个临时变量存储空间，所以空间复杂度为O(1)。 注：存储数据需要一个大小为n的数组，并不是指空间复杂度就为O(n)。因为，这 n 个空间是必须的，无法省掉。 我们说空间复杂度的时候，是指除了原本的数据存储空间外，算法运行还需要的额外的存储空间。 时间复杂度 不管顺序栈还是链栈，入栈、出栈只涉及栈顶个别数据的操作，所以复杂度为O(1)。 支持动态扩容的顺序栈的入栈、出栈时间复杂度分析 对于出栈操作来说，不会涉及内存的重新申请和数据的搬移，所以出栈的时间复杂度仍然是 O(1)。但是，对于入栈操作来说，情况就不一样了。当栈中有空闲空间时，入栈操作的时间复杂度为 O(1)。但当空间不够时，就需要重新申请内存和数据搬移，所以时间复杂度就变成了 O(n)。 也就是说，对于入栈操作来说，最好情况时间复杂度是 O(1)，最坏情况时间复杂度是 O(n)。而平均时间复杂度，由摊还分析法分析可知为 O(1)。 栈的应用 在函数调用中的应用 操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成“栈”这种结构, 用来存储函数调用时的临时变量。每进入一个函数，就会将临时变量作为一个栈帧入栈，当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈。 栈在表达式求值中的应用 实现一个表达式求值的功能，编译器就是通过两个栈来实现的。 其中一个保存操作数的栈，另一个是保存运算符的栈。 从左向右遍历表达式，当遇到数字，就直接压入操作数栈；当遇到运算符，就与运算符栈的栈顶元素进行比较。 如果当前运算符优先级高，就将当前运算符压入栈；如果运算符栈顶元素优先级高，就从运算符栈中取栈顶运算符，从操作数栈的栈顶取 2 个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。 栗子 3+5*8-6表达式的计算过程如下： 栈在括号匹配中的应用 假设表达式中只包含三种括号，圆括号 ()、方括号 [] 和花括号{}，并且它们可以任意嵌套。比如，{[{}]}或 [{()}([])] 等都为合法格式，而{[}()] 或 [({)] 为不合法的格式。 问题 给定一个包含三种括号的表达式字符串，如何检查它是否合法呢？ 方法 用栈来保存未匹配的左括号，从左到右依次扫描字符串。当扫描到左括号时，则将其压入栈中；当扫描到右括号时，从栈顶取出一个左括号。如果能够匹配，比如(跟)匹配，[跟]匹配，{跟}匹配，则继续扫描剩下的字符串。如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。 当所有的括号都扫描完成之后，如果栈为空，则说明字符串为合法格式；否则，说明有未匹配的左括号，为非法格式。 示例 left_brackets = '{[(&lt;' right_brackets = '}])&gt;' matching_brackets = {'}': '{', ']': '[', ')': '(', '&gt;': '&lt;'} def judgment_brackets_matching(rows): stack = [] label = True for row in rows: if row in left_brackets: stack.append(row) elif row in right_brackets: if len(stack) &lt; 1: label = False break elif matching_brackets[row] == stack[-1]: stack.pop() else: label = False break else: continue if stack: label = False return label 解答开篇 用栈实现浏览器的前进、后退功能 方法 使用两个栈，X 和 Y，把首次浏览的页面依次压入栈 X，当点击后退按钮时，再依次从栈 X 中出栈，并将出栈的数据依次放入栈 Y。当我们点击前进按钮时，我们依次从栈 Y 中取出数据，放入栈 X 中。当栈 X 中没有数据时，那就说明没有页面可以继续后退浏览了。当栈 Y 中没有数据，那就说明没有页面可以点击前进按钮浏览了。 示例 顺序查看了a,b,c三个页面，将其依次压入栈，栈中数据情况为： X: a-&gt;b-&gt;c Y: None 点击后退按钮，从c页面推到a页面，栈中数据情况为： X: None Y: c-&gt;b-&gt;a 想再次查看b页面，点击前进按钮到b页面，此时栈中数据情况为： X: a-&gt;b Y: c 假设，此时在b页面跳转到新页面d，页面c就无法通过前进或后退按钮重复查看了，因而需要清空Y栈： X: a-&gt;b-&gt;d Y: None]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>data_structure</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[链表]]></title>
    <url>%2F2018-10-11%2F</url>
    <content type="text"><![CDATA[主要学习几个写链表代码的技巧 理解指针或引用的含义 有些语言有指针概念，比如C语言；有些语言没有指针，取而代之的是引用，比如Java、Python。不管“指针”还是“引用”，都是一个意思，指存储所指对象的内存地址。 指针含义：将某个变量（对象）赋值给指针（引用），实际上就是就是将这个变量（对象）的地址赋值给指针（引用） 示例： p—&gt;next = q; 表示p节点的后继next指针存储了q节点的内存地址 p—&gt;next = p—&gt;next—&gt;next; 表示p节点的后继next指针存储了p节点的下下个节点的内存地址。 警惕指针丢失和内存泄漏 示例： 单链表的插入，希望在节点a和相邻节点b之间插入节点x，假设当前指针p指向节点a，则造成指针丢失和内存泄漏的代码： p-&gt;next = x; x-&gt;next = p-&gt;next 导致将x自身赋值给了x-&gt;next，自己指向自己。 对于有些语言来说，比如 C 语言，内存管理是由程序员负责的，如果没有手动释放结点对应的内存空间，就会产生内存泄露。 所以，插入结点时，一定要注意操作的顺序。上面代码的正确写法是：两句代码顺序调换。 同理，删除链表时，也一定要手动释放内存空间，否则，也会出现内存泄漏问题。 Python语言不需手动释放，它的解释器的存储管理系统会自动回收不用的存储。 利用哨兵（头结点）简化实现难度 哨兵含义： 链表中的哨兵节点是解决边界问题的，不参与业务逻辑。如果我们引入哨兵节点，则不管链表是否为空，head指针都会指向这个“哨兵”节点。我们把这种有“哨兵”节点的链表称为带头链表，相反，没有哨兵节点的链表就称为不带头链表。 示例： 不带头结点时： 对于单链表的插入操作 1 . 如果在p节点后插入一个新节点，只需2行代码即可搞定： new_node—&gt;next = p—&gt;next; p—&gt;next = new_node; 2 . 如果向空链表中插入一个新结点，则代码就不同： if(head == null){ head = new_node; } 对于单链表的删除操作 1 . 如果要删除节点p的后继节点，只需1行代码即可搞定： p—&gt;next = p—&gt;next—&gt;next; 2 . 如果删除的是链表的最后一个节点（链表中只剩下这个节点），则代码如下： if(head—&gt;next == null){ head = null; } 以上示例可以看出，不带头结点时，单链表的插入、删除操作，需要对插入第一个结点和删除最后一个结点作特殊处理。 带头结点时： 哨兵节点不存储数据，无论链表是否为空，head指针都会指向它，作为链表的头结点始终存在。 这样，插入第一个节点和插入其他节点，删除最后一个节点和删除其他节点都可以统一为相同的实现逻辑。 留意边界条件处理 常用的检查链表代码是否正确的边界条件： 1 . 如果链表为空时，代码是否能正常工作？ 2 . 如果链表只包含一个节点时，代码是否能正常工作？ 3 . 如果链表只包含两个节点时，代码是否能正常工作？ 4 . 代码逻辑在处理头尾节点时是否能正常工作？ 举例画图，辅助思考 对于稍微复杂的链表操作，可以找一个具体例子，将它画在纸上。比如，向单链表中插入一个数据，就画出链表插入前和插入后的情况。 对着图写代码，写完之后，也可举列子，照着代码走一遍，很容易发现代码中的Bug。 多写多练，没有捷径 精选的5个常见链表操作：（以及在LeetCode上对于的题目编号） 1.单链表反转 ---- 206 2.链表中环的检测 ---- 141 3.两个有序链表合并 ---- 21 4.删除链表倒数第n个节点 ---- 19 5.求链表的中间节点 ---- 876]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>data_structure</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[链表]]></title>
    <url>%2F2018-10-10%2F</url>
    <content type="text"><![CDATA[如何用链表实现LRU缓存淘汰算法？ 缓存 缓存定义：一种高效数据读取性能的技术，比如常见的CPU缓存、数据库缓存、浏览器缓存等。缓存在计算机软件、硬件开发中应用都很广。 缓存特点：大小有限，被用满时，需要清理一部分数据，而哪些数据应该被清理哪些应该被保留，由缓存淘汰策略决定。 缓存淘汰策略 常见的缓存淘汰策略有：FIFO（First in,First out）先进先出策略、LFU（Least Frequently Used）最少使用策略、LRU（Least Recently Used）最近最少使用策略。 三种链表 链表通过指针将一组零散的内存块串联在一起。其中内存块叫做链表的结点，记录结点地址的叫做指针。链表的第一个结点叫头结点，最后一个结点叫尾结点。 单链表 单链表的“尾结点”，它的指针并不指向下一个结点，而是指向一个空地址NULL 单链表插入和删除操作，复杂度为O(1) 循环链表 一种特殊的单链表，与单链表的区别就在于尾结点，其尾结点指向链表的头结点 相比于单链表，循环链表的优势在于从链尾到链头很方便。著名的约瑟夫问题，就适合这种数据结构。 双向链表 单链表只有一个方向，结点只有一个后继指针next指向后面的节点。双向链表有两个方向，一个后继指针next和一个前驱指针pre。 双向链表在某些情况下的插入、删除操作比单链表更高效。 例如删除操作，从链表中删除一个数据，有两种情况： 删除链表中值等于某个给定值的结点 删除给定指针指向的结点 对于第一种情况，无论单链表还是双向链表，为了查找到值等于给定值的结点，都需要从头结点开始一个一个依次遍历对比，直到找到值等于给定值的结点，然后再将其删除。 尽管单纯的删除操作时间复杂度是 O(1)，但遍历查找的时间是主要的耗时点，对应的时间复杂度为 O(n)。根据时间复杂度分析中的加法法则，删除值等于给定值的结点对应的链表操作的总时间复杂度为 O(n)。 对于第二种情况，已经找到了要删除的结点，但是删除某个结点 q 需要知道其前驱结点，而单链表并不支持直接获取前驱结点。这种情况下单链表删除操作需要 O(n) 的时间复杂度，而双向链表只需要 O(1) 的时间复杂度。 以上的情况涉及到一个空间换时间的设计思想：双向链表更费内存，但仍比单链表应用更广泛。 缓存实际上就是利用了空间换时间的设计思想。 如果我们把数据存储在硬盘上，会比较节省内存，但每次查找数据都要询问一次硬盘，会比较慢。 但如果我们通过缓存技术，事先将数据加载在内存中，虽然会比较耗费内存空间，但是每次数据查询的速度就大大提高了。 若内存空间充足，如果更加追求代码的执行速度，就选择空间复杂度相对较高，时间复杂度相对较低的算法和数据结构，例如，缓存技术。 若内存比较紧缺，比如代码跑在手机或者单片机上，这时，就要反过来用时间换空间。 数组与链表对比 时间复杂度 删除、插入：链表O(1)、数组O(n) 随机访问操作：链表O(n)、数组(1) 缓存支持 数组在实现上使用的是连续的内存空间，可以借助 CPU 的缓存机制，预读数组中的数据，所以访问效率更高。 链表在内存中并不是连续存储，所以对 CPU 缓存不友好，没办法有效预读。 原因 CPU在从内存读取数据的时候，会先把读取到的数据加载到CPU的缓存中。而CPU每次从内存读取数据并不是只读取要访问的地址，而是读取一个数据块，并保存到CPU缓存中，然后下次访问内存数据的时候就会先从CPU缓存开始查找，如果找到就不需要再从内存中取。 这样就实现了比内存访问速度更快的机制，也是CPU缓存存在的意义：为了弥补内存访问速度过慢与CPU执行速度快之间的差异。 对于数组来说，存储空间是连续的，所以在加载某个下标的时候可以把以后的几个下标元素也加载到CPU缓存这样执行速度会快于存储空间不连续的链表存储。 灵活性 数组大小固定，一经声明就要占用整块连续内存空间。如果声明的数组过大，系统可能没有足够的连续内存空间分配给它，导致内存不足（out of memory）。如果声明的数组过小，则可能出现不够用的情况。这时只能再申请一个更大的内存空间，把原数组拷贝进去，非常费时。 链表本身没有大小的限制，天然地支持动态扩容。 链表实现LRU缓存淘汰策略的思路 维护一个有序单链表，越靠近链表尾部的结点是越早被访问过的。当有新的数据被访问时，从链表头开始顺序遍历链表。----缓存访问的时间复杂度为O(n) 过程： 1 . 当访问的数据存储在缓存的链表中时，遍历得到数据对应的结点，将其从原位置删除，再将其插入到链表表头； 2 . 当访问的数据未出现在缓存的链表中时 1）若缓存有空闲，将该数据直接插入到链表表头。 2）若缓存被占满，则将链表尾部的数据删除，再将新数据插入到链表表头。 优化：使用散列表，记录每个数据的位置，将缓存访问的时间复杂度降到 O(1)。 思考 如何用数组实现LRU缓存淘汰策略？ 方式一：首位置保存最新访问数据，末尾位置优先清理 当访问的数据未存在于缓存的数组中时 1）缓存有空闲，将数据插入数组第一个元素位置，数组所有元素需要向后移动1个位置，新数据插入数组第一个元素位置，时间复杂度为O(n)； 2）缓存无空闲，清理数组末尾位置的元素，数组所有元素需要向后移动1个位置，新数据插入数组第一个元素位置，时间复杂度为O(n)； 当访问的数据存在于缓存的数组中时，查找到数据，将其从原位置删除，并将其插入数组的第一个位置，此时亦需移动数组元素，时间复杂度为O(n)。 方式二：首位置优先清理，末尾位置保存最新访问数据 当访问的数据未存在于缓存的数组中时 1）缓存有空闲，直接将数据添加进数组作为当前最后一个元素，时间复杂度为O(1)； 2）缓存无空闲，清理数组首位置的元素，数组所有元素向前移动1个位置， 将新元素插入数组，时间复杂度为O(n)； 当访问的数据存在于缓存的数组中时，查找到数据，将其从原位置删除，并将其插入当前数组最后一个元素的位置，此时亦需移动数组元素，时间复杂度为O(n)。 优化：清理的时候可以考虑一次性清理一定数量，从而降低清理次数，提高性能。 如何通过单链表实现“判断某个字符串是否为回文字符串”？ 比如 “123454321” 1 . 前提：字符串以单个字符的形式存储在单链表中。 2 . 遍历链表，判断字符个数是否为奇数，若为偶数，则不是。 3 . 将链表中的字符倒序存储一份在另一个链表中。 4 . 同步遍历2个链表，比较对应的字符是否相等，若相等，则是回文字符串，否则，不是。]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>data_structure</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数组]]></title>
    <url>%2F2018-10-09%2F</url>
    <content type="text"><![CDATA[数组定义 一组线性表数据结构。它用一组连续的内存空间，来存储一组具有相同类型的数据。 关键词解释 线性表：每个线性表上的数据最多只有前和后两个方向。 除了数组，链表、队列、栈都是线性表结构 联想到非线性表：数据之间并不是简单的前后关系。 如，二叉树、堆、图等 连续的内存空间和相同类型的数据 正因为有了这两个限制，才使得数组有了随机访问的特性； 也正是因为这两个限制，使得数组的删除、插入操作效率很低。 如何实现根据下标随机访问数组元素？ 计算机会给每个内存单元分配一个地址，再通过地址来访问内存中的数据。 而计算机通过寻址公式来计算元素存储的内存地址： //a[i]的地址就是从首地址偏移i*data_type_size的位置 a[i] = base_address + i * data_type_size base_address: 内存块的首地址 data_type_size：数中每个元素的大小；根据存储的数据类型而定，如int型，该值为4 为什么数组要从0开始编号，而不是从1开始呢？ 若数组从1开始计数，那么上面的公式就变成 a[i] = base_address + (i-1) * data_type_size 修改后，每次随机访问数组元素都多了一次减法运算，对于CPU就多了一次减法指令。 两个操作 数组的插入操作 效率低的原因：将某个数据插入到数组中的第i个位置。为了给新来的元素腾出这个位置，需要移动后面的i~n个元素，复杂度为O(n); 改进方法：当数组是无序的，简单的方法就是将原来第i个位置上的元素放到数组最后，然后将新来的元素放到第i个位置。复杂度为O(1); 数组的删除操作 与插入操作类似，若删除第i个位置的元素，需要搬移后面的i~n个元素，才能保证内存的连续性。 复杂度：若删除开头元素，最坏复杂度为O(n)；若删除数组末尾元素，最好复杂度为O(1)；平均复杂度为O(n)。 改进方法：不要求数组中数据的连续性，就可将多次删除操作集中在一起执行。 每次删除元素时，并不真正搬移元素，而是记录下数据已被删除。当数组没有更多空间存储数据时，再执行一次真正的删除操作（做数据元素的搬移工作） 数组的访问越界问题 分析以下一段代码： int main(int argc, char* argv[]){ int i = 0; int arr[3] = {0}; for(; i&lt;=3; i++){ arr[i] = 0; printf(&quot;hello world\n&quot;); } return 0; } 问题 不是只打印三行“hello world”；而是无限打印 原因 数组大小为3，a[0]，a[1]，a[2]，而我们的代码 for 循环的结束条件错写为了i&lt;=3 而非 i&lt;3，所以当 i=3 时，数组 a[3] 访问越界。 根据我们前面讲的数组寻址公式，a[3] 也会被定位到某块不属于数组的内存地址上，而这个地址正好是存储变量 i 的内存地址，那么 a[3]=0 就相当于 i=0，所以就会导致代码无限循环。 注：例子中死循环的问题跟编译器分配内存和字节对齐有关。数组3个元素，加上一个变量i。4个整数刚好能满足8字节对齐，所以i的地址恰好跟在a[2]后面，导致死循环。如果数组本身有4个元素，则这里不会出现死循环。因为编译器64位操作系统下，默认会进行8字节对齐，变量i的地址就不会紧跟在数组后面了。 Key 1 . 常会问的一个面试题：数组和链表的区别？ 正确表述：链表适合插入、删除操作，时间复杂度为O(1)；数组适合随机访问数组元素(而不应该说查找)，根据下标随机访问的时间复杂度为O(1)。 明确的点：数组是适合查找，但查找的时间复杂度不为O(1)。即便是排好序的数组，用二分查找，时间复杂度也是O(nlogn)。 2 . 数组越界在 C 语言中是一种未决行为，并没有规定数组访问越界时编译器应该如何处理。因为，访问数组的本质就是访问一段连续内存，只要数组通过偏移计算得到的内存地址是可用的，那么程序就可能不会报任何错误。 在这种情况下，一般都会出现莫名其妙的逻辑错误，就像上面举的那个例子，debug的难度非常的大。而且，很多计算机病毒也正是利用到了代码中的数组越界可以访问非法地址的漏洞，来攻击系统，所以写代码的时候一定要警惕数组越界。 3 . 二维、多维数组如何寻址？ 行优先 int a[d1][d2][d3]; int *p0 = &amp;a[0][0][0]; int *p = &amp;a[i][j][k]; int idx = i * (d2*d3) + j * d3 + k ASSERT( p0 + idx == p);]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>data_structure</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法复杂度分析]]></title>
    <url>%2F2018-09-29%2F</url>
    <content type="text"><![CDATA[先举个栗子 分析以下这段代码的时间复杂度 // n 表示数组 array 的长度 int find(int[] array, int n, int x) { int i = 0; int pos = -1; for (; i &lt; n; ++i) { if (array[i] == x) pos = i; } return pos; } 分析 功能：在一个无序数组array中，查找变量x出现的位置 时间复杂度：O(n), n表示数组的长度 更为优化的方式 // n 表示数组 array 的长度 int find(int[] array, int n, int x) { int i = 0; int pos = -1; for (; i &lt; n; ++i) { if (array[i] == x) { pos = i; break; } } return pos; } 时间复杂度： 不一定是O(n)了，不同情况下这段代码的时间复杂度是不同的。 引入概念 最好情况时间复杂度：在最理想情况下，执行代码的时间复杂度 最坏情况时间复杂度：在最糟糕情况下，执行代码的时间复杂度 平均情况时间复杂度：最好情况和最坏情况都是属于极端情况，发生的概率并不大。需要表示平均情况下的复杂度 如何分析平均情况时间复杂度 以上面的例子为例：查找x在数组中的位置，有n+1种情况，在数组0~n-1的位置上和不在数组中。 将每种情况下，查找需要遍历的元素个数相加，再除以n+1种情况，就可得到需要遍历的元素个数的平均值 (1+2+3+...+n+n)/n+1 = n (n+3) /2(n+1) 省略掉系数、低阶、常量后得到平均时间复杂度为O(n) 存在问题 在以上的n+1种情况中，未考虑x在每种情况下出现的概率。 现在假设，x在数组中与x不在数组中的概率各为1/2； 要查找的x出现在0~n-1这n个位置的可能性是相同的，即1/n； 那么，要查找的x出现在0~n-1中任意位置的概率为 1/2*1/n = 1/(2n) 将每种情况发生的概率考虑进去后，平均时间复杂度计算过程变成： 这个值就是概率论中的加权平均值，也叫作期望值。因而平均时间复杂度的全称为加权平均时间复杂度或期望时间复杂度。 注：很多时候，我们只使用一个复杂度就可以满足要求。只有同一块代码在不同情况下，时间复杂度有量级的差距，才会使用以上3种复杂度的表示法来区分。 均摊时间复杂度、摊还分析（平摊分析） 举栗子说明： // array 表示一个长度为 n 的数组 // 代码中的 array.length 就等于 n int[] array = new int[n]; int count = 0; void insert(int val) { if (count == array.length) { int sum = 0; for (int i = 0; i &lt; array.length; ++i) { sum = sum + array[i]; } array[0] = sum; count = 1; } array[count] = val; ++count; } 分析 功能：实现了往数组中插入数据的功能。 具体： 1 . 当数组满了count == array.length,就用for循环遍历求和，将求得的和放在数组的第一个位置，并清空数组其余元素；然后再插入新的元素。 2 . 当数组一开始就有空闲，则直接将数据插入数组。 复杂度分析： 1 . 最好情况：数组中有空闲，直接将数据插入到count的位置，为O(1) 2 . 最坏情况：数组没有空闲空间，需要先做一个遍历求和，再作插入。所以复杂度为O(n) 3 . 平均时间复杂度：O(1) 平均时间复杂度如何得到 总共n+1种情况，前n中情况每种时间复杂度都为O(1)，后一种情况时间复杂度为O(n)；n+1种情况发生的概率一样，为1/(n+1)；根据加权平均计算方法： 1*1/(n+1) + 1*1/(n+1) + ... + n*1/(n+1) = O(1) 均摊时间复杂度（amortized time complexity） 分析发现: 1 . insert()函数在大部分情况下，时间复杂度都为O(1)；极个别情况下，复杂度才高，为O(n) 2 . insert()函数中，O(1)时间复杂度的插入和O(n)时间复杂度的插入，出现频率很有规律。存在前后时序关系，一般一个O(n)插入之后，紧跟着n-1个O(1)的插入操作，循环往复。 针对这种场景，引入均摊分析法 大致思路：每一次 O(n) 的插入操作，都会跟着 n-1 次 O(1) 的插入操作，所以把耗时多的那次操作均摊到接下来的 n-1 次耗时少的操作上，均摊下来，这一组连续的操作的均摊时间复杂度就是 O(1)。 总结 对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系，这个时候，我们就可以将这一组操作放在一块儿分析，看是否能将较高时间复杂度那次操作的耗时，平摊到其他那些时间复杂度比较低的操作上。 而且，在能够应用均摊时间复杂度分析的场合，一般均摊时间复杂度就等于最好情况时间复杂度。 均摊时间复杂度就是一种特殊的平均时间复杂度，没必要花太多精力去区分它们。]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>data_structure</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法复杂度分析]]></title>
    <url>%2F2018-09-28%2F</url>
    <content type="text"><![CDATA[执行效率是算法优劣的度量指标，而执行效率又是通过时间、空间复杂度分析。 为什么需要复杂度分析？ 将代码跑一遍，通过统计、监控，就能得到算法执行时间和占用的内存大小。-----这叫事后统计法 存在局限性： 1 . 测试结果依赖于测试环境 2 . 测试结果受测试数据的规模影响 如何能不用具体的测试数据，就可以粗略估计算法的执行效率呢？ 大O复杂度表示法 首先看个栗子： int cal(int n){ int sum = 0; int i = 1; for (; i &lt;= n; ++i) { sum = sum + i; } return sum; } CPU角度分析 这段代码每一行都执行着操作：读数据-运算-写数据。 每行代码对应的CPU执行个数和执行时间不同，但假设每行代码执行时间为 unit_time。 那么上面一段代码的执行时间应为： (2n+2) * unit_time 总结 所有代码的执行时间 T(n) 与每行代码的执行次数成正比。 按照这个思路分析下面一段代码的执行时间T(n) 1 int cal(int n) { 2 int sum = 0; 3 int i = 1; 4 int j = 1; 5 for (; i &lt;= n; ++i) { 6 j = 1; 7 for (; j &lt;= n; ++j) { 8 sum = sum + i * j; 9 } 10 } 11 } 分析过程 对于2,3,4行，执行每行需要1个unit_time；执行5,6行需要n个unit_time；执行7,8行需要n * n个unit_time 总的执行时间为 T(n) = (2n * n + 2n + 3)*unit_time 规律总结 通过以上两段代码的执行时间推导，得到规律： 所有代码的执行时间 T(n) 与每行代码的执行次数 n 成正比。 公式表示 T(n) = O(f(n)) 其中，T(n)表示代码执行时间；n 表示数据规模的大小；f(n) 表示每行代码执行的次数总和； O 表示代码的执行时间 T(n) 与 f(n) 表达式成正比。 那么以上两个例子的大O表示应为： T(n) = O(2n + 2) -----&gt; T(n) = O(n) T(n) = O(2n * n + 2n + 3) -----&gt; T(n) = O(n * n) 注：公式中的低阶、常量、系数三部分并不决定增长趋势，因而可以忽略。 大O时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的变化趋势。 因而，也叫作渐进时间复杂度，简称时间复杂度。 如何分析时间复杂度 1、只关注循环执行次数最多的一段代码 2、加法法则 总复杂度等于量级最大的那段代码的复杂度 举个栗子：分析下面这段代码的时间复杂度 int cal(int n) { int sum_1 = 0; int p = 1; for (; p &lt; 100; ++p) { sum_1 = sum_1 + p; } int sum_2 = 0; int q = 1; for (; q &lt; n; ++q) { sum_2 = sum_2 + q; } int sum_3 = 0; int i = 1; int j = 1; for (; i &lt;= n; ++i) { j = 1; for (; j &lt;= n; ++j) { sum_3 = sum_3 + i * j; } } return sum_1 + sum_2 + sum_3; } 分析 代码是要分别求sum_1、sum_2、sum_3。那么算时间复杂度就分别求每一部分的时间复杂度，再放在一起，取量级最大的那个作为整段代码的时间复杂度。 sum_1处：执行100次，与n无关；是常量执行时间 注：即使循环100000次，只要是一个已知数，与n无关，那么就是一个常量执行时间。 因为时间复杂度表示的是一个算法执行效率与数据规模增长的变化趋势，所以不管常量的执行时间多大，都可以忽略。因为其本身对增长趋势没有影响。 sum_2处：O(n) sum_3处：O(n*n) 根据方法2，整段代码的时间复杂度就等于 O(n*n) 将规律抽象成公式 若 T1(n)=O(f(n))，T2(n)=O(g(n))； 那么 T(n)=T1(n)+T2(n) = max(O(f(n)), O(g(n))) = O(max(f(n), g(n))). 3、乘法法则 嵌套代码的复杂度等于嵌套内外代码复杂度的乘积 公式 若 T1(n)=O(f(n))，T2(n)=O(g(n))； 那么 T(n)=T1(n)*T2(n) = O(f(n))*O(g(n)) = O(f(n)*g(n)). 几种常见时间复杂度实例分析 注 O(1) &lt; O(logn) &lt; O(n) &lt; O(nlogn) &lt; O(n*n) 以上罗列的复杂度量级，可以粗略分为两类：多项式量级、非多项式量级（指数阶和阶乘阶） 将时间复杂度为非多项式量级的算法问题叫作NP问题。当数据规模增大时，非多项式时间复杂度的算法效率非常低。 重点关注常见的多项式时间复杂度 1 . O(1) 明确：O(1)是常量级时间复杂度的一种表示方法，并不是指只执行了一行代码。 一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是Ο(1)。 2 . O(logn)、O(nlogn) 最难分析的一种时间复杂度 最常见的例子： 1 i=1; 2 while (i &lt;= n) { 3 i = i * 2; //i = i * 3; 4 } 分析：变量i从1开始取，每循环一次就乘以2。变量i的取值就是一个等比数列： 2的x次方为n，求解x为多少。x=logn(以2为底) 若循环体内的代码改成 i = i * 3;那么时间复杂度为O(logn)(以3为底) 我们把所有对数阶的时间复杂度都记为O(logn) 因为：log3n 就等于 log32 * log2n，所以 O(log3n) = O(C * log2n)，其中 C=log32 是一个常量。 基于我们前面的一个理论：在采用大 O 标记复杂度的时候，可以忽略系数，即 O(C*f(n)) = O(f(n))。 如果一段代码的时间复杂度是 O(logn)，我们循环执行 n 遍，时间复杂度就是 O(nlogn) 。 O(nlogn) 也是一种非常常见的算法时间复杂度。比如，归并排序、快速排序的时间复杂度都是 O(nlogn)。 3 . O(m+n)、O(m*n) 代码的时间复杂度由两个数据的规模来决定。无法评估m和n谁的量级更大，所以在表示复杂度时，就不能简单利用加法法则，而忽略掉其中一个。 针对这种情况，原来的加法法则应改为 T1(m) + T2(n) = O(f(m) + g(n))；乘法法则继续有效 T1(m)*T2(n) = O(f(m) * f(n))。 空间复杂度分析 渐进空间复杂度，表示算法的存储空间与数据规模之间的增长关系。 我们常见的空间复杂度就是 O(1)、 O(n)、 O(n*n )，像 O(logn)、 O(nlogn) 这样的对数阶复杂度平时都用不到。 小结]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>data_structure</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[发表的论文]]></title>
    <url>%2F2018-09-26%2F</url>
    <content type="text"><![CDATA[Title: Evaluating Multi-Dimensional Visualizations for Understanding Fuzzy Clusters Acceptted by IEEE VIS 2018 Paper Video Talk-PPT]]></content>
      <categories>
        <category>可视化论文</category>
      </categories>
      <tags>
        <tag>paper</tag>
        <tag>presentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Use Seaborn to Create Animated Graph]]></title>
    <url>%2F2018-09-15%2F</url>
    <content type="text"><![CDATA[今天看到一篇很好的文章，教我们如何在Python中创建动画图。很具有实践性，于是跟着码了一遍代码。 附上文章链接 问题 这过程中遇到两个问题： 1 . 用pip install ffmpeg安装了FFmpeg之后，仍然不能正常运行 2 . 解决第一个问题后,又报AttributeError:Seaborn Lineplot Module Object Has No Attribute 'Lineplot' 解决方案 问题一 在Windows上安装FFmpeg需要设置环境变量。 步骤一 从这里下载FFmpeg包，ffmpeg-20180913-1b98bfb-win64-static到本地，解压后，重命名文件夹为FFmpeg。复制或者剪切修改好的文件夹到C盘。 步骤二 接下来在命令行中启用FFmpeg。右键单击此电脑，选择属性，找到高级系统设置，进去。点击环境变量，可以看到两个设置变量的框，在上面的xxx的用户变量框里，找到Path,选择新增，将C:\FFmpeg\bin添加进去，点击确定。 步骤三 测试FFmpeg是否安装成功。快捷方式win+R，输入cmd进入命令控制窗口。键入ffmpeg -version，回车，若出现一系列关于FFmpeg的信息，说明设置成功。 问题二 对于Seaborn包没有属性Lineplot问题，我首先百度了下，几乎都是建议先确认自己的Python环境是否正确，是否安装了需要用的包。于是我分别执行命令pip install matplotlib和pip install seaborn后，再重新导入这些模块到代码中，运行仍然报错。 一番折腾后，发现是seaborn包版本问题。Linplot在0.9版本下的seaborn环境中才可以，因而需要对seaborn包进行升级，运行命令pip install seaborn==0.9.0之后，再次运行代码，不报错了。 注 1 . 原文中使用的代码在读取excel文件的时候使用了已废弃的sheetname参数，正确应该修改为sheet_name 2 . 若使用的是jupyter notebook，确保在代码首行加入了%matplotlib notebook。%matplotlib notebook提供了一些交互性，可能会很慢，因为渲染由服务器端完成。 然鹅，我加了后，在jupyter notebook中并没有看到正常的动画效果(原因未知)。程序运行是没有问题的，在本地生成了一个视频，可以正常显示动画图。]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django入门练习的两个错误]]></title>
    <url>%2F2018-09-12%2F</url>
    <content type="text"><![CDATA[学习《Python从入门到实践》书中的第18章Django入门时，实践书中代码，遇到'learning_logs' is not a registered namespace和ImproperlyConfigured: Specifying a namespace in include() without providing an app_name is not supported. Set the app_name attribute in the included module, or pass a 2-tuple containing the list of patterns and app_name instead. 第一个错误来源 在创建显示Topics数据的网页时，按照书中流程走，创建了父模板，然后子模板继承父模板。 再按照三步标准流程走完： 1 . 添加URL模式; 2 . 视图创建; 3 . 模板创建 一切工作就绪，运行，报错了。 错误如下： 添加的父模板，里面有两个链接用到了命名空间learning_logs。 父模板代码如下： &lt;p&gt; &lt;a href=&quot;{% url 'learning_logs:index' %}&quot;&gt;Learning Log&lt;/a&gt; &lt;a href=&quot;{% url 'learning_logs:topics' %}&quot;&gt;Topics&lt;/a&gt; &lt;/p&gt; &lt;!-- 插入的块标签 content ，是一个占位符，其中包含的信息将由子模板指定。 --&gt; {% block content %}{% endblock content %} 排查问题 1 . 首先看项目文件夹下的settings.py文件，确认应用程序learning_logs是否被添加进去了 不是这里的问题 2 . 定位到urls.py文件，打开项目文件下的urls.py文件；发现有两个版本的urlpatterns列表. 旧版本，url式的： from django.conf.urls import url, include #根据书上的代码写的 urlpatterns = [ url(r'^admin/',admin.site.urls), url(r'',include('learning_logs.urls')), ] 出现新旧版本，原因在于安装的Django的版本，书中的是1.8；而我装的2.1 新版本，path式的： from django.urls import include, path urlpatterns = [ path('admin/', admin.site.urls), path('',include('learning_logs.urls',namespace='learning_logs')), ] 我将path式的注释掉了，用的是url式的。这在今天创建新网页之前，运行都是正常的。 错误解决 首先想到：将url式注释掉，尝试path式，看能否运行正常；结果，又报另一个错误：( ImproperlyConfigured: Specifying a namespace in include() without providing an app_name is not supported. Set the app_name attribute in the included module, or pass a 2-tuple containing the list of patterns and app_name instead. 理解下意思，大致就是：应该在include模块中设置app_name属性，或者传递一个包含模式列表和app_name的2元组 又百度一番，知道了include模块需包含两个参数，前一个为2元组，后一个为namespace；于是将代码修改成这样： from django.urls import include, path urlpatterns = [ path('admin/', admin.site.urls), path('',include(('learning_logs.urls','learning_logs'),namespace='learning_logs')), ] OK! 运行正常 网上搜索一番，发现新旧版本的区别在于是否显示声明namespace的值，在旧版本url式中namespace是注册了的，而新版本未注册。 用url式的代码如下： from django.conf.urls import url, include urlpatterns = [ url(r'^admin/',admin.site.urls), url(r'',include(('learning_logs.urls','learning_logs'))), ] Include只需包含一个参数arg：2元组；namespace不需要显示声明。 到此，问题解决。 完：）]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UnboundLocalError]]></title>
    <url>%2F2018-08-30%2F</url>
    <content type="text"><![CDATA[做《Python编程：从入门到实践》一书中的练习时，运行一段代码，遇到了变量引用的错误： UnboundLocalError: local variable ‘range’ referenced before assignment 代码 依赖模块car.py &quot;&quot;&quot;一个可用于表示汽车的类&quot;&quot;&quot; #父类 class Car(): &quot;&quot;&quot;一次模拟汽车的简单尝试&quot;&quot;&quot; ... #提取出的单独类，并将其实例作为子类的一个属性值 class Battery(): &quot;&quot;&quot;一次模拟电动汽车电瓶的简单尝试&quot;&quot;&quot; def __init__(self, battery_size=60): &quot;&quot;&quot;初始化电瓶的属性&quot;&quot;&quot; self.battery_size = battery_size def describe_battery(self): &quot;&quot;&quot;打印一条描述电瓶容量的消息&quot;&quot;&quot; print(&quot;This car has a &quot; + str(self.battery_size) + &quot;-kWh battery.&quot;) def get_range(self): &quot;&quot;&quot;打印一条描述电瓶续航里程的消息&quot;&quot;&quot; if self.battery_size == 70: range = 240 elif self.battery_size == 85: range = 270 message = &quot;This car can go approximately &quot; + str(range) message += &quot; miles on a full charge.&quot; print(message) #子类 class ElectricCar(Car): &quot;&quot;&quot;模拟电动汽车的独特之处&quot;&quot;&quot; def __init__(self, make, model, year): &quot;&quot;&quot; 初始化父类的属性，再初始化电动汽车特有的属性 &quot;&quot;&quot; super().__init__(make, model, year) #将Battery类的实例作为属性值 self.battery = Battery() 执行my_electric_car.py中的代码 from car import ElectricCar #实例化电动汽车 my_tesla = ElectricCar('tesla', 'model s', 2016) print(my_tesla.get_descriptive_name()) my_tesla.battery.describe_battery() #调用方法get_range() my_tesla.battery.get_range() 运行结果 前面运行正常，就是get_range()方法调用出错 2016 Tesla Model S This car has a 60-kWh battery. UnboundLocalError: local variable 'range' referenced before assignment 错误意思：在定义前就调用；即是range的作用域出了问题； 观察Battery类可以发现变量battery_size的默认值给的是60；而对于get_range()方法中的if判断条件，battery_size的值不满足任何条件；因而变量range并没有得到实例化（即没有被声明），后面的message调用它就会出错。 解决办法 在方法内，if条件外声明变量range。例如： def get_range(self): &quot;&quot;&quot;打印一条描述电瓶续航里程的消息&quot;&quot;&quot; #解决办法：在if条件外声明变量range range = 220 # 当两个if条件都没有满足时，后面调用range会报错；因为它没有被声明 if self.battery_size == 70: range = 240 elif self.battery_size == 85: range = 270 message = &quot;This car can go approximately &quot; + str(range) message += &quot; miles on a full charge.&quot; print(message)]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IOPub data rate exceeded]]></title>
    <url>%2F2018-08-28%2F</url>
    <content type="text"><![CDATA[仍然是在爬区块链数据的过程中遇到的问题。加载爬取的blockdata.json数据文件时,Jupyter notebook显示不出来还报错： IOPub data rate exceeded 原因 Jupyter botebook 内存设置的问题，调整过后即可正常显示。 如何设置 打开anaconda prompt命令窗口 在该命令窗口运行： jupyter notebook --generate-config 可以看见一个路径，找到该路径下的配置文件， 从中找到iopub_data_rate_limit 将它的值调大（后面可以多填几个0），去掉注释 重启jupyter notebook就可以正常显示]]></content>
      <categories>
        <category>tool</category>
      </categories>
      <tags>
        <tag>Jupyter nootebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[json读取数据出错]]></title>
    <url>%2F2018-08-27%2F</url>
    <content type="text"><![CDATA[这几天在爬区块链的数据，需要进一步做处理。在爬取过程中，遇到了问题。 问题描述 当我需要读取一个字典结构的数据，该数据存在json文件中,类似于如下所示数据。 { &quot;hash160&quot;: &quot;6c6be098a685e95270812137e8b01b1ae7d8ffd0&quot;, &quot;address&quot;: &quot;1AtHDAdBEw6bUtnjYQbbYjVKvSmRrdmomY&quot;, &quot;n_tx&quot;: 2, &quot;total_received&quot;: 60451790, &quot;total_sent&quot;: 60451790, &quot;final_balance&quot;: 0, } 我使用jupyter notebook 运行如下代码，读 该文件中的数据： with open('xxx.json','r') as f: lines = json.loads(f.read()) 程序抛出异常： Extra data: line 1 column 225215 (char 225214) 解决办法 发生以上错误原因是json只能读取一个文档对象。刚才读的字典数据在文件中的格式不正确，导致无法正确读数据。解决办法有两个： 方法一：单行读取文件 适用于有多行json，行与行之间没有关联的情况 with open(‘xxx.json’,‘r’) as f: for line in f.readlines(): dic = json.loads(line) 方法二：保存数据源的时候，格式化为一个对象 适用于普遍情况；将这个文件写成一个大的json with open(‘address.json’,‘a’) as f: for line in address: #循环，写入多条记录；最外层格式化 f.write(’{“address”:[’) #将每条记录以json格式写入f f.write(json.dumps(line,ensure_ascii=False,indent=2 ) + ‘\n’) f.write(’,’) f.write(’]}’) 读取时再作为一个文档对象处理 with open(‘address.json’,‘a’) as f: str_json = json.loads(f.read()) 读写json文件函数 写json 一般都是方法二提到的json格式，整个文件中的数据是一个大json。函数如下 f.write(json.dumps()) json.dumps()函数是将一个Python数据类型列表进行json格式的编码。可以理解为：json.dumps()函数是将字典转化为字符串 读json 函数如下 json.loads(f.read()) json.loads()函数是将json格式数据转换为字典。可以理解为：json.loads()函数是将字符串转化为字典]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>json</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018为期一个月的暑期记事]]></title>
    <url>%2F2018-08-04%2F</url>
    <content type="text"><![CDATA[成为研究僧的第一个暑假（哈哈，此处应该有掌声，研究生也有暑假），总觉得有必要记录一下，不然对不起炎炎夏日在实验室享受着空调，搞学术的孩子们，认真脸：）。 在知乎上看到很多类似“xxx是种什么体验”的问题，每次一边浏览着高赞回答一边感慨，遗憾自己一个都答不上来，因为没有那个经历。然鹅，由于过去一年都沉迷在科研的海洋里无法自拔，我和小组小伙伴们居然做了一篇顶级paper，也许在某个时候也可以分享“做一篇顶级会议投稿是种什么体验”了。 时间来到6月底，距离paper的开始已经快一年了。当paper的Revise版本提交后，导师说可以休息一个月了，交代边等最终结果边准备后续的工作。收到通知那一刻，我惊喜今年还能有假期，立马收拾东西，多一天也不想像去年，大四的那个暑假，来小组准备VIS，每天三点一线，寝室–实验室–大排档食堂。连续一个多月，在同一个食堂吃饭（除了便宜，都是毛病），真的吃到怀疑人生。 怀着٩(๑&gt;◡&lt;๑)۶激动的心情开始了我的休假生活。终于有时间做一些想做而未做的事情。 关于学习 学习了优达学城上的Git课程，重新熟悉和理解了git的基本概念和基本命令。终于搞清楚了git和github的关系。版本控制系统非常多，例如，我们熟悉的Wikipedia,Google Docs,手动保存也是，而git是控制系统的一种，利用版本控制系统我们可以浏览历史，可以回退到之前的某个版本。而github是一个可托管代码可利用git的远程仓库。学习完课程之后，顺便整理了自己的github仓库。在之前未完成的项目上使用版本控制工具，整理后期需要用到的数据，做了首次commit。 英语学习按计划进行着。放假的那天果断报名了英语流利说的核心课程，为期半年。每天学习半小时以上，目前打卡记录为34天。读研后，看了比较多的论文可能阅读能力提高了，但是口语和听力真的有心无力。为了能听懂大佬们的报告，再加上身边有同学报名，我也没犹豫地报了。跟着AI机器人学英语感觉还不赖，分小组监督一周至少5天的打卡。据说是定制课程，开始学习前测试了英语等级，L3，目标是学完半年达到L6。 7月12号得到论文的最终录用通知。论文最后的修改和Camera-ready排版完成。紧接着开始制作Fast forward视频，迭代了两个版本后，被告知8月回学校再做。记得当时有两个晚上，可能做得太晚，闭上眼，脑子里不断跳出修改思路。说明太晚不碰工作是有道理的，影响睡眠质量。 看《技术之瞳》一直是日程上的事情，之前计划在暑假结束前一定要看完一遍。这次囫囵吞枣地看了一遍，记录了一些题目和概念。印象最深的就是阿里的人才观：聪明、皮实、乐观、自省。聪明是既包括智商高也包括情商高，技术上“有两把刷子”的同时能够很开放地对待身边的人和事。皮实是既有抗打击的能力，也有填坑的能力，还得有经得起夸奖的能力。 之前收藏的微信公众号推送的python文章，这次放假也一并读了。算是粗略过了一遍基础知识，之后会边练习边加深理解。 看了《六神磊磊读唐诗》。读到这本书纯属缘分，有几天太无聊，翻开了它。被六神磊磊幽默诙谐的笔调给圈粉了，以读武侠的方式读唐诗，有趣。整本书的大结构就是按照唐朝的四个时期，初唐、盛唐、中唐、晚唐来讲唐诗。读完这本书，仿佛读到了千年前那些诗人们的心事，惊讶他们是如此丰富多面，有血有肉。从此伟人们也不再只是静立在书上的名字。 这一个月，终于好好思考了自己未来的职业方向。问题一直在，读研后，忙着上课，忙着实验，忙着论文，好像一直没有思考清楚。抱着增强技能的心态来读研，实际上是常常疑问自己为什么来读研，是不是自己不适合读研。我是一个不擅长做长远计划的人，或者说不会思考的人，从大学到现在一直都是顺其自然地走着，每一次做了选择，我会调整自己去适应，努力做到最好。这一年，我在成长的同时也常焦虑未来。抱怨少了，但是焦虑导致的失眠、身体状态不佳却也越来越严重，周围人表现出来的很多特质无时无刻都在宣称着自己有多糟糕。就像玩抢椅子游戏，众人在这一曲音乐里欢快地玩着，音乐戛然而止的时候，每个人都找到了自己的那把椅子开始坐下，只有我茫然地站着。假期里，和很多已经工作的朋友同学交流，心态得以调整，再结合自身的兴趣和特长，终于认定了未来的职业方向。 关于娱乐 在北京吃了一家比较正宗的川菜，金紫茗张妈妈川味馆，算是解了在长沙吃不到钵钵鸡的馋。这家店真的很良心，开在北京但是价格特别亲民，炒菜分量也是足足的。只有在美食面前，作为死宅党的我才愿意在炎炎夏日“迈开腿”，平日里的日均步数不超过200步，那天的步数是25000+。 在北京吃了一家韩菜，Tiger，这是一家米酒屋。虽然餐厅比较小，但是影响不了络绎不绝的食客。装修氛围像酒吧，自制的米酒很nice，招牌菜也强推。吃完饭几个朋友一起聊聊天，感觉不能更棒。 看了部火爆的电影，《我不是药神》。火爆到什么程度呢，就是看完电影的那个晚上，打车回去，滴滴司机一路上都在跟我们聊它。真人事件改编而成，整个观影体验就是五味杂陈，一会笑，一会哭，还有憋屈和无可奈何。 各种熬夜，一星期追完一部30来个小时的韩剧，《请回答1988》。之前就被很多朋友安利这部剧，但一直没时间看。既舍不得看完，又按奈不住地往后追，看到后面有个剧情，大家一个接一个搬离那个承载着他们整个青春记忆的胡同时，我仿佛感觉自己的青春时光也一去不复返了。每集都是一个独立主题，很治愈，尽管隔着国界，我仍能从中找到共鸣感。亲情，爱情，邻里情都在剧里被刻画得真切细腻，能够触到心底最柔软的地方。记忆深刻地是讲家庭出现矛盾后各家人的处理方式以及对互相理解作出的努力，正焕和妈妈生涩的拥抱、和配合爸爸夸张的见面礼，德善妈妈接纳丈夫的不够浪漫与细腻，德善爸爸倾听德善的诉求……家人之间，同样需要像生命体一样不断地修复更新。家人之间最好的相处方式应该就是这样相互“接纳”与相互被需要吧。 近期规划 今天已经是8月4号，7号前从休假状态调整回科研模式。这个月要完成的事项包括： 15号前搞定Fast Forward和Preview视频 准备10月份会议Presentation的PPT 英语学习不能停，打卡满60天，口语练习，目标达到L4 开始阅读《增长黑客》，做读书笔记 每周2次，健身房锻炼或寝室内瑜伽]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>essay</tag>
        <tag>reading notes</tag>
        <tag>teleplay</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[冒泡排序]]></title>
    <url>%2F2018-06-11%2F</url>
    <content type="text"><![CDATA[冒泡排序 基本思想 理论： 1.比较轮数n-1。 2.比较次数n-1。 3.符合某个条件交换位置。 核心： 双重for循环。 步骤： 1 .双重for循环。 2 .指定轮数和次数。 3 .判断是否符合标准。如果符合标准交换位置。 版本比较 将数组元素从小到大排序： 初始版 按照前面的步骤很容易写出以下代码： var arr = [7,6,5,4,3,2,1]; //1.双重for循环。(外循环控制轮数) for(var i=0;i&lt;arr.length-1;i++){ //2.指定轮数和次数（内循环控制次数） for(var j=0;j&lt;arr.length-1;j++){ //3.判断是否符合标准。如果符合标准交换位置。 if(arr[j] &gt; arr[j+1]){ var temp = arr[j]; arr[j] = arr[j+1]; arr[j+1] = temp; } } } 改进思路：每比较一轮，就少比较一次。（每一轮都会比较出一个最大值，后一轮就没有必要再比较那个值了，所以每比较一轮，就少比较一次。） 改进版 var arr = [7,6,5,4,3,2,1]; var m = 0; var n = 0; for(var i=0;i&lt;arr.length-1;i++){ for(var j=0;j&lt;arr.length-1-i;j++){ if(arr[j] &gt; arr[j+1]){ var temp = arr[j]; arr[j] = arr[j+1]; arr[j+1] = temp; } m++;//记录比较次数 } n++;//记录比较轮数 } console.log(arr); console.log(m); console.log(n); 再升级思路：如果比较完，提前结束比较。（判断，如果本次比较没有移动任何元素，那么说明已经比较完成。） 升级版 var arr = [1, 2, 3, 4, 5, 6, 7]; var m = 0; var n = 0; for(var i=0;i&lt;arr.length-1;i++){ //开闭原则。（写在第一个for循环里，是为了每轮比较先初始化bool变量变为true。） var bool = true; for(var j=0;j&lt;arr.length-1-i;j++){ if(arr[j] &gt; arr[j+1]){ var temp = arr[j]; arr[j] = arr[j+1]; arr[j+1] = temp; //如果有交换，则bool置false。 bool = false; } m++; } n++; //如果本轮比较没有任何元素相互交换位置，那么说明已经比较完成，可以跳出循环。 if(bool){ break; } } console.log(arr); console.log(m); console.log(n);]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[js实现before :after伪类样式修改]]></title>
    <url>%2F2018-06-06%2F</url>
    <content type="text"><![CDATA[为了能对区块的产生有一个更直观的认识，想让最新区块的产生有一个动画的展示效果（从无到有的弹入）。这里用js的animate函数来实现。之前有提到每个区块都用了伪类来添加样式，所以这里还需要对伪类样式作一些修改。 伪类样式修改方案 元素的before和after伪类的样式修改方案 要实现某个元素的before和after伪类的样式修改，方法有4种。CSS中并不能直接选择某一个元素的:before和:after伪类元素。 举个栗子说明4种方案 HTML结构 &lt;div class=&quot;box&quot;&gt;Hello,it's me.&lt;/div&gt; CSS样式 .box:after{ content: '', font-weight: bold } 方案1 使用js或者jQuery改变，为元素添加类 .blue:after{ content: '', background-color: blue } $('div').addClass('blue'); 方案2 在存在的style文档中动态插入样式 document.styleSheet[0].addRule('.box:after','background-color:blue'); document.styleSheet[0].insertRule('.box:after{background-color:blue}',0); 方案3 创建一份新的样式表，并使用js或jQuery将其插入到head标签中 var style = document.creatElement('style'); document.head.appendChild(style); sheet = style.sheet; sheet.addRule('.box:after','background-color:blue'); sheet.insetRule('.box:after{background-color:blue}',0); &lt;!-- 插入操作 --&gt; $('&lt;style&gt;.box:after{background-color:blue}&lt;/style&gt;').appendTo('head'); 方案4 使用HTML5的data-属性，在属性中使用attr()动态修改 先给div标签增加data-attr = 'orange'属性，然后用jQuery修改该属性值 .box:after{ content: '', font-weight: bold, data-attr: 'orange' } $('div').attr('data-attr','blue'); 我的练习 最新区块修改伪类样式 说明：每个区块在展示时，因为有额外的图片和时间信息需要展示，所以借助了伪类before和after 具体实现可Pick我。 原来的做法：在CSS中选择要应用伪类的元素，然后设置伪类样式 图1 直接选择要应用伪类的元素，然后设置样式 现在的做法：给某个类名设置伪类样式，然后将该类名添加给需要该样式的元素。用的是上面提到的4个方案中的方案1，这是比较简洁和方便的做法。 注意：after伪类显示区块产生时间，需要通过js设置data-content属性值，CSS用content属性，获取属性data-content的值 图2 给某个类名应用伪类样式，before类设置小三角图片，after类显示区块产生时间 动态弹入效果 动态展示的最新区块，其HTML结构与其他区块不同，且还需要改变它的伪类样式。 实现过程 动态添加节点 首先把最新区块与其他区块分开。最新区块在页面加载完时是没有的，在写入区块信息之前，动态添加最新区块的节点。最新区块的HTML结构相比于其他区块，多了一层大的div.bnew，作用是为了显示链条的背景图，小三角，区块的产生时间(用到了before和after伪类，如图2所示)。因为每次刷新都需要动态添加最新区块的节点，因而在添加前需要把上一次的节点remove掉，所以具体实现如下所示。 图3 jQUery动态操作节点 动画方法 动画实现用到的是jQuery的animate()方法，语法包含4个参数 $(selector).animate(styles,speed,easing,callback) 参数的具体设置如下： 1 .首先动态修改style参数。通过marginLeft来控制进入方向；width来控制显示范围；opacity控制初始的隐藏状态； 2 .然后填加动画时间为2000ms。 具体如图4所示。 图4 给最新区块div.bnew添加动画 初始的div.bnew的CSS样式 图5 初始的div.bnew样式设置 注：因为在animate方法中，display属性无法起作用，所以在初始的样式设置中要用opacity属性。 3 .注意easing参数的设置，内置的只有2种，如下所示。 若要用到其他的运动轨迹需要添加库，然后引用，这里引用的是easeOutBounce。详细参数引用可Pick我 4 .最后回调函数callback的设置。为div.bnew添加类名bnewT（伪类样式的设置）放在这里，如图4所示。 实现效果]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>javascript</tag>
        <tag>css</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《白说》]]></title>
    <url>%2F2018-06-03%2F</url>
    <content type="text"><![CDATA[繁忙的5月，在并不忙的最后几天读了《白说》。这是我第一次读白岩松老师的书，我是个很难在短时间内读完一本书的人（武侠小说除外 0.0），但这次很快就看完了。可能这跟《白说》是本类似于鸡汤的演讲集有关。白岩松作为央视的著名主持人，肯定做过很多演讲，这本书其实就是把他所讲的相同主题的演讲内容组合在一起，总共二十来篇。 他说。他姓白，所以这本书叫《白说》。他说，说了也白说，但不说，白不说。读完《白说》，我觉得很多地方都没“白说”，下面是摘录的部分读书笔记。 主题：漂亮的失败是另一种成功 白说：人如果一直处于“成功”的状态，慢慢也就麻了，所谓温水中的青蛙，你觉得一切都是理所当然。反倒是时常降临的失败与挫折，是上帝对你的一个提醒，让你从“失败”这门课里，接受某些教育。 人处于一帆风顺的境地，久了，可能会觉得生活太平淡反而没有什么意思。偶尔的一些逆境，让人学会某些东西，然后体验生活的不同味道。“生命远非人智所及，它由伟大的孤寂中诞生，只有从苦难中才能触及。只有困厄与苦难才能使心眼打开，看到那不为他人所知的一切”。拥抱失败与痛苦，直面现实，才能触及到生命的本质。 白说：很多人的失败感，不是来自自己的感受，而是别人的眼光与当下世俗的标准。然后方寸大乱，然后就真觉得自己失败了。如果你不为别人的眼光与标准活着，失败的感受会在我们生活中消失大半。 无论做什么事，都应该建立自己的标准与目标，别让别人的眼光打扰自己的情绪和感受，信自己才是王道。 主题：沟通世界不是非黑即白 白说：国外的新闻报道几乎已成共识：通过具体人物，表达宏大事件。没有主人公就没有事件，就会让新闻可信度，尤其是吸引力降低。所以，你首先要明白，新闻写作传播，就是一个写故事和讲故事的过程。不要在“故事”和“虚构”之间画等号—真实的事情，也需要通过“讲故事”的方式进行传播。我们在对外、对内的宣传当中，有相当多的失败就是因为不会讲故事。花了很多钱出了很多力，却没有好的效果。 以前也一直认为故事就等于虚构，不够贴近生活。现在才知道，生活需要我们有讲一个好故事的能力。同样一个笑话，为什么别人讲，就好好笑，自己讲，空气中弥漫的尽是尴尬。传播信息本质上也是一个讲故事的过程，学习，工作时常需要作报告和分享，这些就能体现出一个人讲故事的能力。如何讲才能更通俗易懂，更能吸引听者听下去，这些都是是需要花很多时间思考和准备的。柏拉图说： “谁会讲故事谁就拥有世界”。与人交流，学会讲故事，才能掌握主动权。 主题：智商很高，情商却低 国家治理体系和治理能力的现代化，不仅包括依法治国，其中必然也包括提升整个社会的情商，尤其是执政者的情商。一个国家的良性运转，一个社会要达成和谐，情商必不可少，甚至高于智商。情商对于中国的执政者、媒体格外重要。中国老百姓最受用的一个词是“态度”，不管受多大委屈，如果你的情商很高，带着爱、带着温暖走到他的身边，人家立即眼泪一抹，“放心，我自力更生。”啥委屈都扔一边了。 读到这段话的时候，感触挺深的。因为正好碰到自己遇到不公平待遇，就感觉自己其实也很受用“态度”一词，不管受多大委屈，只有别人态度诚恳，说话不那么伤人，我就觉得这些都没啥了，多大的事可能都不是事了：），可能也只有高情商的人才会想到这样做。想起之前读的一本书《所谓情商高就是会说话》，说话不只是一种“感觉”，说话还是是一种技术，同一个意思，换一个表达，效果却全然不一样。]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>reading notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[判断一个数是否是2的幂]]></title>
    <url>%2F2018-05-30%2F</url>
    <content type="text"><![CDATA[最近在看《技术之瞳》，编程语言部分有一个笔试题目 Q：请填写一个表达式，用于判断一个数是否是2的幂？ A：首先要考虑数的正负，其次判断是否为2的幂，最好的办法是利用位运算技巧。 n&gt;0? (n&amp;(n-1))==0:false Key 将2的幂次方写成二进制形式后，很容易就会发现有一个特点： 二进制中只有一个1，并且1后面跟了n个0。如果将这个数减去1后会发现，那个1会变为0，而原来的n个0会变为1。 举栗子说明 十进制 二进制 2 10 4 100 8 1000 16 10000 ... ... 2-1 01 4-1 011 8-1 0111 16-1 01111 因此将原来的数与其减去1后的数字进行&amp;运算,为零，则原来的数是2的幂。 Note &amp; 按位与运算符：两位同时为1，结果才为1，否则为0。 作用： 1 . 清零；想让某个数清零，则另找一个数（原数为1的位置，新数为0），两个数作与运算。 2 . 取一个数中的某些位；若想取低字节位，则可和8个1作与运算。 3 . 保留指定位；若23，即10111，想保留左起的2,3,5位，则可和01101（13）作与运算。 | 按位或运算符：两位中有一个为1，结果就为1。 作用： 1 . 按位或运算常用来对一个数据的某些位定值为1；若想使18，即10010的低4位改为1，则只需和13（1101）进行按位或运算即可。 ^ 异或运算符：两位值不同，结果为1，否则为0。 作用： 1 . 交换两个值，不用临时变量； 举栗子说明 a=3 (011), b=5 (101); a=a^b; b=a^b; b=a^b; 另一种方式 a=a+b; b=a-b; a=a-b; ~ 取反运算符：将0变1，1变0；用于求整数的二进制反码。 &lt;&lt; 左移运算符：各二进制位全部左移若干位，左边丢弃，右边补0。 &gt;&gt; 右移运算符：各二进制位全部右移若干位，正数左补0，负数左补1，右边丢弃。 两个不同长度的数据进行位运算时，系统会将二者按右端对齐，然后进行位运算。短的那个数据如果是负数，左边补1，否则补0。]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>面试题</tag>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSS :before :after应用]]></title>
    <url>%2F2018-05-20%2F</url>
    <content type="text"><![CDATA[前面总结过将Flex布局应用在区块图的布局中，现在需要在区块.block之间显示每个块产生的时间间隔，以及一个小三角符号。这里总结用伪元素:before ，:after 来实现该效果。 伪元素 基本语法格式 selector:pseudo-element {property:value;} selector.class:pseudo-element {property:value;} 作用 方便给某些选择器添加特殊效果。 :before 用来给指定的元素的内容之前插入新内容； :after 用来给指定的元素的内容之后插入新内容。 举个栗子 .first:before{ content: 'It\'s '; color: blue; } &lt;p class=&quot;first&quot;&gt;me&lt;/p&gt; 效果： 说明 通过伪元素 :before ，:after添加的新内容区域默认的display 属性值为inline。可以像修改其他元素一样修改它的样式，可以将它的 display 属性值来改为 block。 对于伪元素 :before 和 :after ，属性 content 必须设置。 属性 content 的值可以是多类型的 再举栗子 1 . 可以是一张图片的url content: url( &quot;img/me.png&quot; ) 2 . 可以配合伪类使用，常配合伪类 :hover .first:hover:before{ content:'It\'s '; color:red; } &lt;p class=&quot;first&quot;&gt;me&lt;/p&gt; 注意两者使用的顺序，伪类 :hover 在前，伪元素:before在后，如果顺序改为 .before:before:hover ,则无效。 效果： 鼠标移上去就显示It’s，且颜色是红色 3 . 配合取值函数attr()使用 a:before{ content: attr(title); } &lt;a href=&quot;http://www.taobao.com&quot; title=&quot;我的最爱&quot;&gt;买买买&lt;/a&gt; 效果： 我的练习 最终效果图 图1 两个红色线框中的内容为实现效果 实现代码 HTML结构 图2 3个大的横排div，每个横排大div里，假设都有5个.block区块div CSS设置 以.rowMiddle行里的.block为例，用:before控制小三角的显示和位置；用:after控制区块产生的间隔时间的显示和位置 /.rowMiddle中每个.block后面的小三角符号的处理，控制 :before/ .rowMiddle .block:not(:first-child)::before{ content: ''; display: block; width: 9px; height: 9px; background-image: url(../images/chainicon1.png); background-repeat: no-repeat; position: absolute; top: 60%; left: -20%; } /.rowMiddle中每个.block的产生时间间隔，控制 :after/ .rowMiddle .block:not(:first-child)::after{ content: attr(data-content); font-size: 12px; display: block; width: 36px; height: 15px; line-height: 15px; position: absolute; top: 30%; left: -27%; } 注：attr(data-content)中 data-content的值通过js计算得到。每个.block的产生时间间隔不同，在js中为每个.block添加一个.timewait类，并设置属性data-content的值。 JS代码 $(&quot;.rowMiddle .block&quot;).each(function(i){ // 控制after伪类,为每个block添加不同的产生时间信息 // CSS中的.attr只能应用在伪类中的content属性 var timeWait = 0; timeWait = blockData[4+i].time - blockData[5+i].time; $(this).addClass('timewait').attr('data-content',timeWait+'秒'); });]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>javascript</tag>
        <tag>css</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[报告：多维数据的感知互补视图评估]]></title>
    <url>%2F2018-05-17%2F</url>
    <content type="text"><![CDATA[报告：多维数据的感知互补视图评估 论文原文：An Evaluation of Perceptually Complementary Views for Multivariate Data 作者：Chunlei Chang,Tim Dwyer,Kim Marriott 发表刊物/会议： PVis 2018 背景介绍 各个领域越来越多的自动化数据收集，因此越来越复杂的高维数据变得可用。有效的可视化和分析工具也越来越重要。多维数据的可视化技术和方法有很多，比如经典的MDS，PCA，但是这类方法在数据降维的过程中难免会损失原始数据的信息。而既能保证数据信息不丢失又能展示所有维度的信息，应用最广泛的是散点图矩阵（SPLOMs）和平行坐标图（PCPs）。 本文工作 评估散点图矩阵（SPLOM）、平行坐标图（PCP）、以及这两种技术的并排组合图（ Combined ）在可视化多维数据时的各自优势。 重点贡献 1 . 总结6个用户分析多维数据时需要解决的Tasks； 2 . 总结5种志愿者在使用组合图时常用的策略； 论文作者为什么会想到评估组合视图呢？ 思路来源于一篇感知互补网络可视化的评估论文（CHI 2017 Evaluating perceptually complementary views for network exploration tasks），这篇论文比较网络的单个矩阵视图、节点链接视图、以及它们的并排组合，发现并排组合的综合视图有明显的优势。但是现在还没有人做过这种组合视图的评估工作。 注：感知互补视图，不同于多视图，在感觉上是互补的，即它们都展示相同的信息，但是对不同类型的分析任务有不同的和互补的支持。 本文的组合视图与以往研究工作中的组合视图有什么区别呢？ 以往做法：使用复杂的视觉和交互设计将两种不同技术组合成单个可视化方案 本文：更为直接地做法将两种技术视图并排放在一起 主要研究的问题？ Q1：PCP或SPLOM各自适合哪些任务？ Q2：解决复杂任务时组合视图有没有优势？ Q3：用户是否意识到不同视图的优点？ 如果有，他们用什么策略来解决分析任务。 视觉编码及交互设计 视觉编码 PCP：每个数据项都被编码为一条折线，标签命名轴表示数据维度和范围； SPLOM：每个数据项由多个圆点表示，对角线上的空白单元格用来显示维度标签，数据范围标记在左边和底边。 图1：可视化技术介绍 交互设计 鼠标悬浮数据项，高亮，该数据点的相关信息展示在旁边 范围过滤，选择范围内的数据项高亮 组合视图Brush（两个视图的联动），包括上面两个交互技术的应用 平行坐标轴和矩阵重排序 ** 维度排序步骤：** 1 .点击PCP的轴或者散点图的轴启动拖拽 2 .将选定的轴拖动到所需的位置。 3 .释放鼠标，两个视图更新 图2 组合视图中轴的重排序 实验研究过程 首先总结Tasks，确定6个通用任务，来源于以往基于PCP和SPLOM的评估工作。 实验参与者需要回答每个任务后面的具体问题。问题是基于实验中使用的数据。 实验数据：学生成绩数据，6个维度，包含姓名、性别和4门学科的分数。 Best-Performer Q： 哪个学生的平均分数最高？ Subset-Tracing Q： 年龄在22岁，科学分数少于75分但是数学分数最高的学生姓名是？ Object-Comparison Q： 在所有4门学科中，标记3个数据为黄色，1个数据为蓝，要求找出标记为黄色的学生哪个与标记为蓝色的学生平均分最为相近？ Outlier-Detection Q： 找出工程得分高于其他三门课程的学生（最为明显的那一个）？ Correlation-Estimation Q： 哪两门学科最为正相关？ Cluster-Identification Q： 在哪对维度上有明显的簇？（找视觉上聚在一起的点或线） 实验 数据 人造的学生成绩数据，共15个。3个训练集，12个用于正式实验。 包括两个难易程度：容易6个、难6个。通过数据密度和答案选对的难易程度来控制题目的难易程度。 容易：100-120个数据项；正确答案与候选答案之间相差较远。 难：180-200个数据项；正确答案与几个候选答案之间很接近。 参与者 实验分成A、B两个部分进行，每部分完成3个任务，时间花费在45分钟左右。参加A部分的人完成前3个任务，参加B部分的人完成后3个任务。 每个参与者需要完成：3（任务）*12（数据）*3（可视化）次试验。 实验参与人员结构： 全部来自于大学里的学生或工作人员。 参加A部分的人，21名，8女13男； 参加B部分的人，30名，9女21男； 其中有9个人，同时参与两部分实验； 年龄在19岁-55岁，平均年龄为29岁。 实验环境 电脑配置 英特尔酷睿i7 MacBook Pro（2016）和24英寸屏幕（1920×1080） 使用Tobii X3-120眼动仪 跟踪参与者眼睛在屏幕上的移动情况，收集数据，为后面分析参与者使用组合视图时使用的策略作准备。 交互方式 通过鼠标和键盘进行答题和操作。 实验设计 分成A，B两部分：A：完成前3个任务；B：完成后3个任务； 为了实验条件平衡，每部分的参与者又分成2组，一组先用单独的一种可视化，后用组合可视化；另一组先用组合可视化，后用单独的可视化；对于同一个数据集，参与者在用不同可视化技术回答问题时，通过改变数据中学生的姓名和维度的排列顺序来避免可视化技术之间的影响； 每个参与者的问题顺序随机； 屏幕上方有计时条，参与者最多有30s使用可视化的时间；若在30s之内有答案，可按空格键停止计时； 屏幕弹出答案选项，包括几个候选答案，以及“太难”、“都不是”选项。 实验过程 1 .背景知识调查，调查参与者以往对两种可视化方法是否了解过。结果如图3所示。对于PCP技术，49%的人从未见过；而对于SPLOM技术，43%的人偶尔使用。 图3 参与者背景知识调查结果 2 .眼动仪校准 要求参与者对眼动仪进行校准，用时不超过30秒。 3 .训练 包括内容： 指导每个参与者如何用可视化完成任务； 每个任务有3个实例，训练之后给出正确答案； 通过第一个实例来演示交互技术；正确通过一个实例后才能进入下一个实例； 告诉参与者答题时间的限制，要尽可能快和准地完成实验。 4 .正式实验 为每个参与者，呈现3种视图，每个视图都包含3个任务，每个任务包含12个不同数据集的问题。 5 . 收集反馈 收集参与者对3种可视化方法在每个任务上的排名； 收集参与者对每个可视化方法的优缺点评论。 实验结果 数据分析 数据收集 每次试验的完成时间 每次试验的准确率 眼睛和鼠标的移动数据 对于每个任务，参与者对可视化技术的主观偏好 数据处理 移除6次不合理数据，完成时间小于1s 移除选“太难” 或“都没有” 的试验 统计分析 数据不服从正态分布，使用Friedman test检验数据的总体差异性 由于移除了不合理的数据导致样本量不等，成对差异比较使用Mann-Whitney U test 眼动仪数据分析 通过分析眼动仪数据来了解参与者使用组合视图采用的策略。 数据收集 组合视图中的每次试验，测量参与者花在观察两个AOI（兴趣区域），即PCP和SPLOM上各自的停留和访问持续时间，用于检查和识别参与者策略。 策略分类结果，总共分为5种，如图4所示。 只看SPLOM，每次在PCP上停留时间不超过1s。 只看PCP，每次在SPLOM上停留时间不超过1s。 先看SPLOM，后看PCP，每次来回切换的停留时间不超过1s。 先看PCP，后看SPLOM，每次来回切换的停留时间不超过1s。 参与者视线在两个视图上频繁切换。 图4 组合视图中5种常见的使用策略 任务结果 T1：Best-Performer 时间，3种技术无显著差异 准确率，PCP和Combined视图显著高于SPM 参与者更喜欢用PCP视图 眼追踪数据表明，参与者花在PCP上的时间多于SPLOM T2：Subset-Tracing 时间，PCP和Combined视图显著快于SPLOM 准确率，SPLOM显著最差 参与者更喜欢用PCP视图 眼追踪数据表明，Combined视图中，参与者花在PCP上的时间更多 T3：Object-Comparison 时间，3种技术无显著差异 准确率，SPLOM显著最差 参与者更喜欢用Combined视图 眼追踪数据表明，表明频繁切换视图策略最受欢迎，单独使用PCP与先使用PCP的比例相同，无人单独使用SPLOM T4: Outlier-Detection 时间，PCP显著多于SPLOM和Combined视图 准确率，SPLOM和Combined视图显著高于PCP 参与者更喜欢用Combined视图 眼追踪数据表明，频繁切换和先使用SPLOM最受欢迎 T5: Correlation-Estimation 时间，PCP显著多于SPLOM和Combined视图 准确率，所有技术之间都有显著性，SPLOM&gt;C&gt;PCP 参与者更喜欢用SPLOM视图 眼追踪数据表明，无人单独使用PCP，单独使用SPLOM的人最多 T6: Cluster-Identification 时间，SPLOM显著快于其余两个 准确率，Combined视图最高，其次是SPLOM，最后是PCP 参与者更喜欢用SPLOM视图 眼追踪数据表明，只使用SPLOM的人最多，无人只使用PCP 图5，图6，图7，图8展示了相关结果 图5 按任务分类的三种可视化技术中每一种的平均结果。黄色背景表明该值与相同指标下的其他值有显著差异；黑色加粗字体表明是该指标下的最优值。 图6 按任务分类的三种可视化技术中每一种的完成时间的平均值和标准偏差。 图7 按任务分类的三种可视化技术中每一种的准确率平均值和标准偏差。 图8 (a)每个任务下，参与者在Combined视图中的偏好选择；(b)每个任务下，参与者在Combined视图中使用的策略情况。 视图使用情况对比 组合视图中，参与者花费在每个视图中的时间对比图，如图9所示。 发现：在前3个任务中，多数参与者使用PCP 1st and Parallel Use两种策略；后3个任务，则多数参与者使用SPLOM 1st and Parallel Use 两种策略 图9 用于查看Combined视图中每个视图的相对时长（栏高度）；1-3是训练任务。 结论总结 提出PCP和SPLOM的组合表示，主要探讨并评估多维数据的感知互补视图的影响。 PCP和SPLOM视图在感知上是互补的。 SPLOM点可视化可以帮助参与者理解整体（后3个Tasks）,PCP线可视化帮助参与者识别单个观察点，两种结合既可以解决整体任务又可以解决细节任务。 并排放置感知互补的视图是获得两者优点的有效方法。 以往用复杂的设计和交互手段使两个技术结合成一种可视化方案并不是没有效果，但是这种效果普通参与者无法体验。更为直接的方法就是让两个视图并排放置在一起，本文的实验结果证明了这种做法是有效的。 基于眼动追踪数据，针对组合视图确定了五种常用视图使用策略。 有些人在没有意识的情况下使用互补视图的优势。 参与者给出的每个任务下对技术的主观偏好与眼动仪追踪数据分析的结果并不完全一致，使用组合视图时，参与者主观偏好某一种可视化技术，但是眼动仪数据表明他们使用组合策略时间更久。这说明参与者其实在没有意识的情况下已经体验了互补视图的优势。 当一个视图对于特定任务明显比另一视图更好时，使用组合视图会有更多的时间开销。 例如分析任务5，SPLOM视图明显比PCP更适合。当使用Combined视图时，会花费更多的时间，但是准确率仍然能够保证。 缺陷与未来工作 缺陷 尽管得出组合视图在6个任务中的5个上都表现好，但是标准偏差比较高。 整个实验使用的数据密度(100-200)太小，数据结构单一。意味着碰到数据密度很大，或者其他数据结构时，本文的实验结果不一定适用。 未来研究方向 研究互补性对其他类型数据集的影响（例如，对于分层数据和动态网络）。 哪些视图可以产生互补效应？有没有有效的分类？一个视图以数据项为中心（例如PCP和节点链接图）和一个视图以关系为中心（如SPLOM和邻接矩阵），这是不是一种有效的分类呢？还需要更多的研究。 可以进一步研究训练对人们使用组合图的有效性的影响。有经验的参与者知道利用Combined视图的优势，而无经验的用户则可能只会使用单个视图，说明训练对人们的策略使用是有影响的。还可以进一步确定组合图的使用是否有助于人们学习更有效地使用单个视图。]]></content>
      <categories>
        <category>可视化论文 summary</category>
      </categories>
      <tags>
        <tag>reading notes</tag>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flex布局]]></title>
    <url>%2F2018-05-13%2F</url>
    <content type="text"><![CDATA[最近在做一个关于区块链可视化的前端页面练习，尝试了flex布局，作个简单的summary。 网页布局 区块图展示部分刚开始使用的是布局的传统解决方案: 基于盒状模型，依赖display属性+position属性+float属性，后来发现Flex弹性布局更好用，垂直方向居中完全不是问题，盒子换行放置也很easy。 Flex布局即是一种弹性布局。任何一个容器都可以指定为Flex布局（display:flex），行内元素也可以使用（display:inline-flex）。 注意：设为Flex布局以后，子元素的float,clear,和vertical-align属性将失去作用。 基本概念 采用 Flex 布局的元素，称为 Flex 容器（flex container），简称&quot;容器&quot;。它的所有子元素自动成为容器成员，称为 Flex 项目（flex item），简称&quot;项目&quot;。 容器的属性 每个属性对应的含义是： • 项目排列方向；可取四个值： row | row-reverse | column | column-reverse • 项目是否换行，怎么换；可取三个值： nowrap | wrap | wrap-reverse • 前2个属性的简写; 取值： &lt;flex-direction&gt; || &lt;flex-wrap&gt; • 项目在主轴上的对齐方式，与主轴的方向有关；可取五个值： flex-start | flex-end | center | space-between | space-around • 项目在交叉轴上的对齐方式,与交叉轴的方向有关；可取五个值： flex-start | flex-end | center | baseline | stretch • 多根轴线的对齐方式，若项目只有一根轴线，该属性不起作用；可取六个值： flex-start | flex-end | center | space-between | space-around | stretch 项目的属性 每个属性对应的含义是： • 项目排列顺序，值越小，排列越靠前；默认为0 • 有多余空间时，项目是否放大；默认为0，不放大 • 空间不够时，项目是否缩小；默认为1，缩小 • 分配多余空间之前，项目占据的主轴空间；默认为auto,即原本大小 • flex-grow, flex-shrink 和 flex-basis属性的简写（优先使用这个属性，而不是单独写三个分离的属性，因为浏览器会推算相关值），常用的两个快捷值： auto(1 1 auto)和none(0 0 auto) • 允许单个项目有与其他项目不一样的对齐方式，可覆盖align-items属性；默认值为auto 我的布局 HTML结构 1 . 包含所有区块的大div -&gt;.d3layout（容器）； 2 . 大div里包含3个次大div -&gt;.block-row（既是容器又是项目）； 3 . 每个次大div里包含5个或者2个小div -&gt; .block/.block_0（项目）； CSS 第一层的大div .d3layout{ width: 935px; margin: 20px 0; height: 740px; /*flex布局容器*/ display: flex; flex-flow: row wrap; justify-content: flex-start; align-items: flex-start; align-content: space-around; } 第二层的次大div .rowTop,.rowMiddle,.rowBottom{ height: 202px; width: 935px; /*flex布局项目*/ flex: auto; /*（1，1，auto）*/ /*flex布局容器*/ display: flex; flex-flow: row wrap; justify-content: space-between; align-items: center; } 第三层的小div .block_0{ width: 703px; height: 180px; /*flex布局项目*/ flex: 0 1 auto; } .block{ width: 145px; height: 180px; /*flex布局项目*/ flex: 0 1 auto; } 布局效果 黑色为大div；红色为次大div；灰色为小div 最终在实践过程中，又添了一层，灰色小div.block里还包括上下两个div，仍然用的flex布局。 注：基本概念参考来源]]></content>
      <categories>
        <category>technique summary</category>
      </categories>
      <tags>
        <tag>javascript</tag>
        <tag>css</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Twice linear]]></title>
    <url>%2F2018-05-07%2F</url>
    <content type="text"><![CDATA[题目描述 Consider a sequence u where u is defined as follows: 1 . The number u(0) = 1 is the first one in u. 2 . For each x in u, then y = 2 * x + 1 and z = 3 * x + 1 must be in utoo. 3 . There are no other numbers in u. Ex: u = [1, 3, 4, 7, 9, 10, 13, 15, 19, 21, 22, 27, …] 1 gives 3 and 4, then 3 gives 7 and 10, 4 gives 9 and 13, then 7 gives 15 and 22 and so on… Task: Given parameter n the function dbl_linear (or dblLinear…) returns the element u(n) of the ordered (with &lt;) sequence u. Example: dbl_linear(10) should return 22 Note: Focus attention on efficiency 我的代码 function dblLinear(n) { // your code var res = [1]; var i=0,j=0; while(res.length &lt;= n){ var y = res[i]*2+1; var z = res[j]*3+1; if(y&lt;z){ res.push(y); i++; }else if(y==z){ res.push(y); i++; j++; }else{ res.push(z); j++; } } return res[n]; } Clever function dblLinear(n) { var ai = 0, bi = 0, eq = 0; var sequence = [1]; while (ai + bi &lt; n + eq) { var y = 2 * sequence[ai] + 1; var z = 3 * sequence[bi] + 1; if (y &lt; z) { sequence.push(y); ai++; } else if (y &gt; z) { sequence.push(z); bi++; } else { sequence.push(y); ai++; bi++; eq++; } } return sequence.pop(); } Key 考虑效率问题，就不能将所有值都放入数组，再来排序，去重；且这种解法，不好控制循环的次数； 想到在push元素进数组时就按从小到大的顺序放入，且遇到相同的元素就只push一次进数组 具体思路就是每次将y和z中较小的一个放入数组，同时其对应的计数器+1；若y和z相等，放任意一个进数组，两个计算器都+1]]></content>
      <categories>
        <category>technique</category>
      </categories>
      <tags>
        <tag>javascript</tag>
        <tag>codewars</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Adding Big Numbers]]></title>
    <url>%2F2018-05-06%2F</url>
    <content type="text"><![CDATA[题目描述 We need to sum big numbers and we require your help. Write a function that returns the sum of two numbers. The input numbers are strings and the function must return a string. Example add(“123”, “321”); -&gt; “444” add(“11”, “99”); -&gt; “110” Notes The input numbers are big. The input is a string of only digits The numbers are positives 我的代码 function add(a, b) { var arra =a.split(''); var arrb =b.split(''); var len = a.length &gt; b.length?a.length:b.length; var result = []; var count = 0; for(i=0; i&lt;len; i++){ var temp; if(i&gt;=a.length){ temp = Number(arrb.pop()) + count; }else if(i&gt;=b.length){ temp = Number(arra.pop()) + count; }else{ temp = (Number(arra.pop()) + Number(arrb.pop())) + count; } temp &gt;= 10?[temp,count]=[temp-10,1]:count=0; result.push(temp); } result.push(count); // console.log(result); return result.reverse().join('').replace(/^0+/,''); // return Number(a) + Number(b); // Fix this! } Clever function add (a, b) { var res = '', c = 0 ; a = a.split(''); b = b.split(''); while (a.length || b.length || c) { c += ~~a.pop() + ~~b.pop(); res = c % 10 + res; c = c &gt; 9; } return res; } Key 1 . 思路是将a和b的最后一位相加，如果相加的结果大于10，取个位数部分，进位值count+1；并将结果放入到数组中。 2 . 要一位一位相加，需要把a和b两个字符串转成字符串数组，用到.split()函数。 3 . 还要注意判断a串和b串哪个更长，最后循环完后需要将最后一次的进位值放入数组。 4 . 以上步骤得到的数组不是最终结果，还要先翻转，再将字符串数组变成字符串，最后用.replace()方法将首位的0去掉。]]></content>
      <categories>
        <category>technique</category>
      </categories>
      <tags>
        <tag>javascript</tag>
        <tag>codewars</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Find the missing term in an Arithmetic Progression]]></title>
    <url>%2F2018-05-02%2F</url>
    <content type="text"><![CDATA[题目描述 An Arithmetic Progression is defined as one in which there is a constant difference between the consecutive terms of a given series of numbers. You are provided with consecutive elements of an Arithmetic Progression. There is however one hitch: exactly one term from the original series is missing from the set of numbers which have been given to you. The rest of the given series is the same as the original AP. Find the missing term. You have to write the function findMissing(list), list will always be at least 3 numbers. The missing term will never be the first or last one. Example : findMissing([1,3,5,9,11]) == 7 PS: This is a sample question of the facebook engineer challenge on interviewstreet. I found it quite fun to solve on paper using math, derive the algo that way. 我的代码 var findMissing = function (list) { var len = list.length; var x = Math.abs((list[1]-list[0])) &lt; Math.abs((list[len-1]-list[len-2])) ? Math.abs((list[1]-list[0])):Math.abs((list[len-1]-list[len-2])); console.log(x); for(i=0; i&lt;len; i++){ if(list[len-1] &gt; list[0]){ if(list.indexOf(list[i]+x) == -1) return list[i]+x; }else if(list[len-1] &lt; list[0]){ if(list.indexOf(list[i]-x) == -1) return list[i]-x; } } } Clever var findMissing = function (list) { var step = (list[list.length - 1] - list[0]) / (list.length); return list.filter(function(val, index) { return val !== (list[0] + index * step); })[0] - step; } Key 关键点1要找到等差值，利用题目中给的信息：第一个和最后一个数不会缺失；将这两个数分别与它们的后一个和前一个数相减，小的那个差就是等差值； 关键点2要注意数列中可能有负数，按照关键点1求等差值的做法，则求解的时候要取绝对值；且需要判断数列是递增还是递减；]]></content>
      <categories>
        <category>technique</category>
      </categories>
      <tags>
        <tag>javascript</tag>
        <tag>codewars</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Range Extraction]]></title>
    <url>%2F2018-05-01%2F</url>
    <content type="text"><![CDATA[题目描述 A format for expressing an ordered list of integers is to use a comma separated list of either • individual integers • or a range of integers denoted by the starting integer separated from the end integer in the range by a dash, ‘-’. The range includes all integers in the interval including both endpoints. It is not considered a range unless it spans at least 3 numbers. For example (“12, 13, 15-17”) Complete the solution so that it takes a list of integers in increasing order and returns a correctly formatted string in the range format. Example: solution([-6, -3, -2, -1, 0, 1, 3, 4, 5, 7, 8, 9, 10, 11, 14, 15, 17, 18, 19, 20]); // returns “-6,-3-1,3-5,7-11,14,15,17-20” 我的代码 function solution(list){ // TODO: complete solution var str = []; for (i = 0; i &lt; list.length; i++) { if ((list[i]+1) !== list[i+1]) { //完全没有连续的情况 str += list[i].toString() + ','; } else if ((list[i]+1) === list[i+1] &amp;&amp; (list[i+1]+1) === list[i+2] &amp;&amp; (list[i-1]+1) !== list[i]) { //有2个及其以上连续的情况 str += list[i].toString() + '-'; } else if (str[str.length-1] === '-' &amp;&amp; (list[i]+1) !== list[i+1]) { //判断是否是range的最后一个元素 str += list[i].toString() + ','; } else if ((list[i]-1) === list[i-1] &amp;&amp; (list[i]+1) === list[i+1]) { //属于range中的元素 } else if ((list[i]-1) === list[i-1] &amp;&amp; (list[i-1]-1) === list[i-2]) { //只有2个连续的情况 str += list[i].toString() + ','; } else { //新range开头 str += list[i].toString() + ','; } } return str.slice(0, -1);//去掉最后的逗号 } Clever function solution(list){ for(var i = 0; i &lt; list.length; i++){ var j = i; while(list[j] - list[j+1] == -1) j++; if(j != i &amp;&amp; j-i&gt;1) list.splice(i, j-i+1, list[i] +'-'+list[j]); } return list.join(); } Key .slice() 方法可返回一个新的数组，包含从 start 到 end （不包括该元素）的 arrayObject 中的元素。（含头不含尾） 语法：arrayObject.slice(start,end) 用到了.splice()方法，替换数组中的内容。 参数包含:（起始下标，替换长度，替换内容） i记录range开始位置，j记录range结束位置。]]></content>
      <categories>
        <category>technique</category>
      </categories>
      <tags>
        <tag>javascript</tag>
        <tag>codewars</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sudoku Solution Validator]]></title>
    <url>%2F2018-04-30%2F</url>
    <content type="text"><![CDATA[题目描述 Sudoku Background： Sudoku is a game played on a 9x9 grid. The goal of the game is to fill all cells of the grid with digits from 1 to 9, so that each column, each row, and each of the nine 3x3 sub-grids (also known as blocks) contain all of the digits from 1 to 9. 我的代码 function validSolution(board){ //TODO var flag = 1; for(j=0; j&lt;9; j++){ var sumcol = 0; var sum = 0; // 计算列和是否为45 for(i=0; i&lt;9; i++){ sumcol += board[i][j]; } if(sumcol != 45){ flag = 0; break; } // 计算行和是否为45，利用数组的方法求和 board[j].some(function(item,i){ sum += item; }) if(sum != 45){ flag = 0; break; } } // 计算前9个宫格的和是否为45 var sumgrid = 0; for(k=0; k&lt;3; k++){ for(l=0; l&lt;3; l++){ sumgrid += board[k][l]; } } if(sumgrid != 45){ flag = 0; } if(flag){ return true; }else{ return false; } } Clever function equals45(n){ return n == 45; } function validSolution(board){ var sumh = [0,0,0,0,0,0,0,0,0]; var sumv = [0,0,0,0,0,0,0,0,0]; osums = [[0,0,0],[0,0,0],[0,0,0]]; for (var i=0;i&lt;9;i++){ for (var j=0;j&lt;9;j++){ sumh[i] += board[i][j]; sumv[j] += board[i][j]; // 这里我认为应该是board[j][i] osums[Math.floor(i/3)][Math.floor(j/3)] += board[i][j]; } } for (var i=0;i&lt;3;i++) if (!osums[i].every(equals45)) return false; return (sumh.every(equals45) &amp;&amp; sumv.every(equals45)); } Key every()方法只有数组中每一项执行回调函数结果合部为true才会返回，不然就会返回false。 常见的数组操作方法 -这个题的做法实质上是不能正确验证是否是数独解。因为若每个cell都是5，满足行、列、每个9宫格的和为45的条件，但不满足数独解的条件。]]></content>
      <categories>
        <category>technique</category>
      </categories>
      <tags>
        <tag>javascript</tag>
        <tag>codewars</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Directions Reduction]]></title>
    <url>%2F2018-04-28%2F</url>
    <content type="text"><![CDATA[题目描述 Write a function which will take an array of strings and return an array of strings with the needless directions removed (WEST and EAST, NORTH and SOUTH cancel each other out). If everything cancels out, return an empty array (stay in place); For example: dirReduc([“NORTH”, “SOUTH”, “SOUTH”, “EAST”, “WEST”, “NORTH”, “WEST”]) =&gt; [“WEST”] dirReduc([“NORTH”, “SOUTH”, “SOUTH”, “EAST”, “WEST”, “NORTH”]) =&gt; [] 我的代码 function dirReduc(arr){ // ... for(i=0; i&lt;arr.length; i++){ if(arr[i] == &quot;NORTH&quot;){ arr[i] = 1; }else if(arr[i] == &quot;SOUTH&quot;){ arr[i] = -1; }else if(arr[i] == &quot;EAST&quot;){ arr[i] = 2; }else if(arr[i] == &quot;WEST&quot;){ arr[i] = -2; } } // console.log(arr); for(i=0; i&lt;arr.length; i++){ if(arr[i] + arr[i+1] == 0){ arr.splice(i,2); i -=2; continue; } } for(i=0; i&lt;arr.length; i++){ if(arr[i] == 1){ arr[i] = &quot;NORTH&quot;; }else if(arr[i] == -1){ arr[i] = &quot;SOUTH&quot;; }else if(arr[i] == 2){ arr[i] = &quot;EAST&quot;; }else if(arr[i] == -2){ arr[i] = &quot;WEST&quot;; } } return arr; } Clever function dirReduc(arr) { var str = arr.join(''), pattern = /NORTHSOUTH|EASTWEST|SOUTHNORTH|WESTEAST/; while (pattern.test(str)) str = str.replace(pattern,''); return str.match(/(NORTH|SOUTH|EAST|WEST)/g)||[]; } Key 如何考虑重复判断数组arr里是否还有符合要求的字符对？ 解决办法：下标i回到0，跳出当前循环，又从第一个元素开始判断剩余数组中的内容，重复这个过程，直到没有符合条件的元素 删除的数组的某一项用：splice(index,len,[item]) 注：该方法会改变原始数组 splice有3个参数，它也可以用来替换/删除/添加数组内某一个或者几个值 index:数组开始下标（要删除的元素的下标） len: 替换/删除的长度 item:替换的值，删除操作的话 item为空]]></content>
      <categories>
        <category>technique</category>
      </categories>
      <tags>
        <tag>javascript</tag>
        <tag>codewars</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BuildTower]]></title>
    <url>%2F2018-04-27%2F</url>
    <content type="text"><![CDATA[题目描述 Build a tower like this: 我的代码 function towerBuilder(nFloors) { // build here var arr = []; for(var i=0; i&lt;=nFloors-1; i++){ var stars = i*2+1; var space = nFloors-1-i; arr.push(&quot; &quot;.repeat(space) + &quot;*&quot;.repeat(stars) + &quot; &quot;.repeat(space)); } return arr; } Clever 分别计算出空格个数和星星个数，将这些字符连接起来成串，放入字符串数组中。 Key 如何生成重复字符？ &quot;字符&quot;.repeat(n) 方法 如何将字符放入数组: 每次单考虑字符串数组中的一个元素，每个元素都是一串字符，用+连接形成字符串（元素），再使用arr.push() 方法将每个元素依次放入数组。]]></content>
      <categories>
        <category>technique</category>
      </categories>
      <tags>
        <tag>javascript</tag>
        <tag>codewars</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[统计数组中元素出现的次数]]></title>
    <url>%2F2018-04-26%2F</url>
    <content type="text"><![CDATA[题目描述 我的代码 function deleteNth(arr,n){ // ... var re = []; var arr1 = {}; for(i=0; i&lt;arr.length; i++){ !arr1[arr[i]] ? arr1[arr[i]] = 1 : arr1[arr[i]] += 1; if(arr1[arr[i]] &lt;= n) re.push(arr[i]); } return re; } Clever function deleteNth(arr,x){ var obj = {}; return arr.filter(function(number){ obj[number] = obj[number] ? obj[number] + 1 : 1 return obj[number] &lt;= x }); } Key 定义统计数组中每个元素出现的次数的变量应该是个对象：var arr1 = {}; 而不是一个数组 var arr1 = []; （这样每个元素对应一个出现次数，之后就可检索出，出现某个次数的元素） 给一个数组(var re = [];)赋值：不是var re = [0];而是应该用.push()方法，re.push(arr[i]);]]></content>
      <categories>
        <category>technique</category>
      </categories>
      <tags>
        <tag>javascript</tag>
        <tag>codewars</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文摘要写作思路实践]]></title>
    <url>%2F2018-04-23%2F</url>
    <content type="text"><![CDATA[原因 第一篇论文是一篇评估论文，在最后改稿阶段，重写了摘要。导师说摘要写得太平淡，读起来像流水账，没有达到发表的标准。 正好我那几天读了一篇教人如何写摘要的文章，其中说到摘要写作是有结构可遵循的。于是利用这个机会，就把看到的写作结构拿来实践一番。 思路 • 第一句：介绍论文的主题 • 第二句：总结以前的工作 • 第三句：指出目前的研究有哪些不足之处（没能解决的问题） 前4句介绍想法 • 第五句：详细说明研究，解释如何工作的，并讨论你应用它的各种方法 • 第六句：总结出为什么你的研究很重要 具体如下： 1 . 引入。思考文章的主题是什么？用一种读者能理解的方式来表达。 2 .陈述研究的问题。关键的研究问题是什么？同样，用一个句子表达。第一句话介绍了整个主题，所以现在可以在这个问题上继续讨论，集中在那个主题中的一个关键问题上。 3 .用一句话总结为什么至今还没有人能充分的回答/解决你要研究的问题。 记住，关键是不要列举人们尝试失败的各种方式，重点是要去说明有一种方法是其他人都没有尝试过的，而这正是你的研究所做的事情。 4 .用一句话来解释如何解决这个研究问题的。你在你的研究中采用了什么创新的方法？ 5 .再一句话说明，你是如何来根据你的想法来实施并完成这项研究的。你是通过科学的实验吗？还是构建了一个软件？或是进行案例研究？这句可能是整个摘要最长的句子。 6 .结尾句子，重点放在研究的主要影响是什么？为什么其他人要关心你的研究呢？他们能做什么来进一步研究你的工作。 我的实践 Abstract 1Fuzzy clustering assigns a probability of membership for a datum to a cluster, which veritably reflects real-world clustering scenarios but significantly increases the complexity of understanding fuzzy clusters. 2Many studies have demonstrated that visualization techniques for multi-dimensional data are beneficial to understand fuzzy clusters. 3However, no empirical evidence exists on the effectiveness and efficiency of these visualization techniques in solving analytical tasks featured by fuzzy clustering. 4In this paper, we conduct a controlled experiment to evaluate the ability of fuzzy cluster analysis to use four multi-dimensional visualization techniques, namely, parallel coordinate plot, scatterplot matrix, principal component analysis, and Radviz. 5First, we deﬁne the analytical tasks and their representative questions specific to fuzzy cluster analysis. Then, we design objective questionnaires to compare the accuracy, time, and satisfaction in using the four techniques to solve the questions. We also design subjective questionnaires to collect the experience of volunteers with the four techniques in terms of ease of use, informativeness, and helpfulness. 6 With a complete experiment process and a detailed result analysis, we test against four hypotheses that are formulated on the basis of our experience, and provide instructive guidance for analysts in selecting appropriate and efficient visualization techniques to analyze fuzzy clusters. 这个摘要基本达到了上面提到的6句话结构，除了第5部分，因为不能用一句话写完，因而用了连词衔接几句话。 PS: 这是论文摘要的最终版本。中途经历了很多次修改，自己改，导师也改，才写出不像流水账的感觉：）]]></content>
      <categories>
        <category>method</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文Latex排版-初入坑]]></title>
    <url>%2F2018-03-27%2F</url>
    <content type="text"><![CDATA[入坑缘由 在第一篇研究生投稿论文进入尾声之即，开始学习用Latex排版。在此之前，只会用word，别谈什么工具整理参考文献，一切全凭耐心，手动操作。 模板说明 由于准备投稿的会议对排版格式有要求，所以在官网下了参考模板。 准备 (La)Tex编辑器：Texworks 格式要求说明/参考模板 一篇paper 启动编辑器 我安装的是Tex Live套装，里面包含texworks。启动方法： 搜索应用程序TeXworks 或者 启动命令窗口cmd，输入TeXworks回车 使用texworks的原因及具体的texworks界面介绍请参见这里。 几乎所有设置都保持默认，只需根据自己需求在工具栏选择排版工具。我主要用到的排版工具是 pdfLaTeX 和 BibTeX。 遇到的问题 1 . 注释，引用，引号，斜体，加粗，换行，章节格式，Section简写，行内公式编辑等基本规则 2 . 图片和表格如何引用，单栏和双栏图片如何设置 3 . 表格绘制，版面占据调整，行距，居中怎么实现 4 . 参考文献排版 基本语法 章节和段落是 \section{} , \subsection{} , \paragraph {} 有参考模板可以直接将内容替换成自己的，格式就会是预设效果。花括号里写上要使用该格式的内容。 注释是%，引用\cite{}，加粗 \textbf{}，斜体 \emph{}，换行是双反斜杠 \\，引号包括前引号~和后引号 ''， 章节缩写，根据章节层级数不同而略有区别，例如：section3 ， 缩写引用格式为，\sectionautorefname{} ,花括号里写上具体章节数3 ，得到的最终效果为Sect.3。 再往下一级走，只需添加sub前缀。 此次论文只有行内公式，具体格式为双美元符内编辑公式$...$，至于公式符号及运算符号,可参考整理。 图片及表格引用，单双栏设置 不用手动给图片和表格编号，使用\aotoref{}标签即可自动编号，花括号内填label内自己定义的名称。以一个图为例： \begin{figure*}[tb] %[tb]表示表格放置在页面上的位置，需要特别注意这里的*号， %若没有它，那么图片会浮在文字上方。添加`*`号，使后面width设置成文本宽度生效 \centering % avoid the use of \begin{center}...\end{center} and use \centering instead (more compact) 水平居中设置 \includegraphics[scale=1,width=\textwidth]{Fig3.png} %scale设置缩放比例，width设置图片占据的宽度，这里设置成与文本同宽，即双栏；最后一个花括号写要加载的图片名称，默认路径是放在pictures文件夹 \caption{} %这里写图题 \label{fig:3} %这里自己定义，引用时用 \end{figure*} width的宽度还可以为\columnwidth，即为单栏宽度。表格引用与图片类似。 表格绘制 目前尝试过的方式有两种： 手动写LaTeX语法 线上生成LaTeX语法 所见即所得方式 对于手动方式绘制表格，即按照语法，在编辑框内添加。例如添加下图所示的一个三线表： 基本语法格式为： \begin{table}[tb] \renewcommand\arraystretch{1.5} %控制行距 \caption{Datasets used for evaluation.} \label{tab:tabele-2} \scriptsize% \centering% %下面一句是控制整个表格的位置，只占据单栏 \resizebox{\columnwidth}{!}{% \begin{tabu}{% l c c c } %控制单元格内容居中方式 \toprule \textbf{Datasets} &amp; \textbf{Data Items} &amp; \textbf{Dimensions} &amp; \textbf{Clusters}\\ \midrule Iris\cite{F1}(for training) &amp; 150 &amp; 4 &amp; 3 \\ Glass\cite{F1} &amp; 214 &amp; 9 &amp; 6 \\ Dermatology\cite{F1} &amp; 259 &amp; 34 &amp; 6 \\ Heart Disease\cite{F1}(for training) &amp; 303 &amp; 14 &amp; 5 \\ Synthetic\cite{F2} &amp; 750 &amp; 12 &amp; 4 \\ Concretec\cite{F1} &amp; 1030 &amp; 9 &amp; 4 \\ Pendigitsc\cite{F3} &amp; 2498 &amp; 63 &amp; 10 \\ \bottomrule \end{tabu}% } \end{table} 手动键入以上示例代码还能接受，当表格复杂时，手动绘制线条和控制跨行跨列就显得繁琐，也容易出错。于是有了第二种方式，线上生成LaTeX代码。使用的线上生成LaTeX表格的工具叫*Tables Generator*，界面简洁，操作方式简单。粘贴或上传表格数据后，表格样式设计与excel操作类似，设计完成后，下方会自动生成LaTeX代码。然后将代码粘贴到相应排版内容处就OK了。 排版“大表格”时出现了一些问题： 1 . 占据双栏，表格太大（宽），页面放不下，且浮在文字上方，用*也解决不了问题； 2 . 占据单栏，大表格变成了mini表格，整体缩小了一两倍； 解决办法： 在求助冬哥后，从他发给我的一篇帖子里找到灵感,问题在于一行文字太长没有换行 原贴参见这里 强制换行之后，表格基本合格了，但仍然存在问题： 按照模板预设的格式单元格内容应该垂直居中，实际效果没有； 单元格太窄，显得字体很挤 解决这两个问题的过程中，发现了一篇帖子，推“所见即所得”的插入表格法。于是有了下面第三种方式的尝试。主要步骤有3步： 1 . 用excel排版好表格，保存成.pdf格式 2 . 用pdfcrop工具（LaTeX工具包内）将刚保存的PDF文件中图片边上的白色剪裁掉。方法如下： 将要剪裁的pdf文件与pdfcrop.exe放在同一个文件夹下；然后打开cmd命令行，通过cd进入所在的文件夹；(快捷打开方式：删除当前目录路径，键入cmd，回车） 最后输入pdfcrop input.pdf output.pdf input.pdf 和 output.pdf 是输入和输出的pdf文件名，改为自己的文件名即可。点击回车之后，output.pdf就会出现在当前文件夹下。 3. 在latex中插入“表格”。具体而言，与插入图片的源码类似： \begin{table} \centering \includegraphics[width=0.75 \textwidth]{output.pdf} \caption{Table Caption} \label{tab:tab-for-table} \end{table} 采用第三种方式可以解决上面两个问题。但是排版出来的效果图不是很满意，绘制的线条从excel转成pdf时，样式发生了改变，线条看起来比较粗，跟预设效果不一致。 最终找到了两个问题的解决办法： 问题1，垂直居中问题主要出现在多行控制的单元格，于是在代码里改变了其所控制的行数，使其居中。后面的{*}控制宽度，*表示自适应 问题2，可以给表格设置行距解决： 参考文献排版 参考文献涉及到BibTeX工具。 操作流程及原理讲解请参见这里 说明 1 . 根据不同要求，采用的文献引用格式不同，\bibliographystyle{abbrv-doi} 2 . 文献数据库名自己定义，\bibliography{&lt;文献数据库名&gt;} 3 . 文献数据库是一个.bib文件，里面的内容，可以使用在线工具Crossref 生成 4 . 参考文献生成的编译顺序： pdfLaTeX -&gt; BibTeX -&gt; pdfLaTeX -&gt; pdfLaTeX]]></content>
      <categories>
        <category>method</category>
      </categories>
      <tags>
        <tag>tool</tag>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[表达自己的需求]]></title>
    <url>%2F2018-03-25%2F</url>
    <content type="text"><![CDATA[《人性的弱点》 我们要学会表达真实的自己，遇到不开心的事要主动跟朋友说；为难的工作要主动跟领导说；碍于情面不敢说出口的拒绝要勇敢说出来，只有这样，我们才能与他人建立真实的关系，交到真心的朋友。不然我们永远只能孤身一人扮演“热心肠好人”。 我们要在说话与行动前，多想想自己的需求，要问自己：“我这样说自己会快乐吗？”“我这么做自己会幸福吗？” 在我们以往的人生中，把太多注意力放在了别人身上，现在我们要重新聚焦于自己，爱自己、关心自己，为此 我们必须建立一个个原则，要有原则地拒绝和坚持，而不是无底线地迎合和付出。 只有这样，我们才能让别人看到自己的底线，清楚自己的原则，才能从人情世故中获得真正的解脱。]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>reading notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[黑客与画家]]></title>
    <url>%2F2018-02-27%2F</url>
    <content type="text"><![CDATA[以下内容是阅读《黑客与画家》时，摘录的关于计算机的一些基础知识。 低级语言 什么叫机器语言 计算机和其他机器一样，也有一张操作命令清单。比如，可以命令计算机把两个数相加。这种操作命令的总和就是计算机的机器语言（machine language）。 什么叫汇编语言 命令清单还是一样，只是将语言换成更人性化的表达，如计算机中的加，用机器语言表达就是11001101，汇编中就是add。 机器语言和汇编语言的共同问题就是，只能让大多数计算机做一些很简单的事情。 为什么使用低级语言 当考虑效率问题时。 如果你非常关注运行速度，那么最好使用接近机器的语言。 比如说，C 语言就是一种低层次语言，很接近硬件，几乎堪称可移植的汇编语言。大多数操作系统都是用C语言写的。 那么问题来了，硬件速度越来越快了，底层次语言的作用越来越小，为什么不抛弃C语言呢？ 原因：人们可能想保留缓存区溢出攻击，使得程序员保持警惕！ 缓存区溢出攻击 当你在C语言中为输入的内容分配出一片内存（缓存）时，它会被分配在当前运行代码的返回地址旁边。 返回地址：指的是一块特定内存，当前代码运行完毕以后，就要运行这块内存中包含的代码。 缓冲区(buffer）: 一个内存区域，用来保存程序需要的输入数据，或者将程序的输出数据累积起来，到一定数量后再输出。 假定有人打算入侵你的计算机，他们猜出你会为某种输入分配256字节的缓存，于是他们就提交多于256字节的内容，目的是覆盖旁边的返回地址。那么，当前代码运行完毕之后，程序的控制权就交给了他们指定的内存地址。这个地址通常是缓存的首地址，当中放的是入侵者事前编好的机器码。于是，入侵者的程序就运行在你的计算机上了。 在C语言中，一旦接受用户输入的时候没有检查输入长度，就创造出了一个安全漏洞。利用这种漏洞的攻击行为就被称为缓存区溢出攻击。 如果使用更抽象的高级语言，上面的事情是不可能发生的。 高级语言,与低级语言的关系 简便方式书写程序所用的语言就是高级语言，其优点是使得程序更具有可移植性，而不同计算机的机器语言是不一样的。 高级语言（简便方式书写的程序，就像一行if 语句）可以通过编译器转变为低级语言（硬件可以理解的语言）。 编译器与解释器区别 编译器 不是低级语言唯一的实现方法，另一种方法是使用解释器。 解释器： 实时地将代码解释为相应的机器语言，然后一行行运行。 编译器： 先将整个程序全部翻译成机器语言，然后再运行。 当编程语言变得很抽象时，就可完全脱离硬件，但是问题是太抽象的话，能解决的问题太少。]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>reading notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[黑客与画家]]></title>
    <url>%2F2018-01-19%2F</url>
    <content type="text"><![CDATA[以下内容是阅读《黑客与画家》时，我摘录的一些个人觉得有意思和值得回味的观点。 Popular 的解释 “受欢迎”的英语单词是popular，这个词还有另一个意思，“大众化的，多数人的”，比如popular support（民意的支持）。此处使用了双关语，作者既是说青少年的行为目的是为了得到同伴的关注和称赞，也是说青少年这样做是为了与群体保持一致。 老成 的真实意思 成年人使用“老成”（tact）这个词，含义似乎就是“闭上嘴巴，不要说话”。我以为它与“缄默”（tacit）和“不苟言笑”（taciturn）有着相同的词根，字面意思就是安静。我就对自己发誓，我绝不要变成“老成”的人，没有人能够让我闭上嘴巴。可是事实上，这个词的词根与“触觉”（tactile）相同，它真正的意思是熟练的碰触。“老成”的反义词是“笨拙”（clumsy）。 创造 优美的事物 与其说优秀的软件设计师是工程师，还不如说是建筑师[插图]在英语中，“建筑师”（architect）和“架构师”（architect）是同一个词，所以这里用的是双关语，意思是优秀程序员不仅负责建造，还负责架构。后一句中的“建筑学”（architecture）也是这种双关用法，同时指“架构学”（architecture）。 优美的软件并不总是论文的合适题材。 1 .首先，科学研究必须具有原创性。 2 .其次，科学研究必须是能够产生大量成果的，而那些不成熟的、障碍重重的领域最容易写出许多篇论文，因为你可以写那些为了完成工作、你不得不克服的障碍。没有什么比一个错误的前提更容易产生大量待解决的问题了。 创造优美事物的方式往往不是从头做起，而是在现有成果的基础上做一些小小的调整，或者将已有的观点用比较新的方式组合起来。这种类型的工作很难用研究性的论文表达。那么，为什么大学和实验室还把论文数量作为考核黑客工作的指标呢？这种事情其实在日常生活中普遍存在，比如，我们使用代码的行数考核程序员的工作效率。。这样的考核容易实施，而容易实施的考核总是首先被采用。 把控细节 他(达芬奇)对作品每一部分的认真程度完全不取决于预料中会不会有人仔细看这个部分。他就像篮球巨星迈克尔·乔丹（Michael Jordan），每一球都一丝不苟，绝不降低对自己的要求。 坚持一丝不苟，就能取得优秀的成果。因为那些看不见的细节累加起来，就变得可见了。 (这个原则跟上次的反思里，学习学姐的做事态度是一样的，放在自身就是对待学术，写论文要一丝不苟，把细节做好，不断拓宽思路。在写程序上还处于初步阶段，更应该一丝不苟，细节处理到位才行，每一次的修订都是进步，长久积累下来就是看得见的长进。) 思考“不能说的话” 有人可能会问，为什么要去找出“不能说的话”？为什么要故意打探那些龌龊的、见不得人的思想观点？你明知那里有挡住去路的石头，为什么还要把它们翻过来看个究竟呢？ 1 .首先，我这样做与小孩子翻石头是出于同样的原因：纯粹的好奇心。我对任何被禁止的东西都有特别强烈的好奇心。我要亲眼看一下，然后自己做决定。 其次，我这样做是因为我不喜欢犯错。如果像其他时代一样，那些我们自以为正确的事情将来会被证明是荒谬可笑的，我希望自己能够知道是哪些事情，这样可以使我不会上当。 2 .再次，我这样做，是因为这是很好的脑力训练。想要做出优秀作品，你需要一个什么问题都能思考的大脑。尤其是那些似乎不应该思考的问题，你的大脑也要养成思考它们的习惯。 我认为这样做不可取，更好的方法是在思想和言论之间划一条明确的界线。在心里无所不想，但是不一定要说出来。我就鼓励自己在心里默默思考那些最无法无天的想法。你的思想是一个地下组织，绝不要把那里发生的事情一股脑说给外人听。 “元标签”（meta-label） 所谓“元标签”，就是对某个标签的抽象描述。如果人们开始讨论元标签，那么原来的标签反而不会受到注意了。举例来说，“政治正确”（political correctness）就是一个“元标签”，是许多特定现象的总称。这个词现在被广泛使用，其实这恰恰意味着“政治正确”的时代正在开始消亡，因为它使得你可以从总体上攻击这个现象，而不会受到指控，不会被说成支持某一种特定的“政治不正确”现象。 如果一个命题不是错的，却被加上各种标签，进行压制和批判，那就有问题。因为只要不是错的观点，就不应该被压制讨论。所以每当你看到有些话被攻击为出自××分子或××主义，这就是一个明确的信号，表明背后有问题。不管在1630年还是在2030年，都是如此。当你听到有人在用这样的标签，就要问为什么。]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>reading notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反思]]></title>
    <url>%2F2018-01-17%2F</url>
    <content type="text"><![CDATA[1月16日，老师在群里怼学姐，当时大家都很懵，也很无语。但是通过他们的对话，我觉得我应该学习学姐的优点并反思自己工作中的不足。 对我来说： 1 .CMH学姐的细心和细节思考的态度和做论文的方式值得我学习。 2 .SYM学姐的论文整理、每日进度汇报方式值得借鉴，特别注意借鉴好的便利的工具能取得较好的效果。 3 .有自己的做事风格，对于老师交代的事要完成并且要有自己的思考，去细化和充实内容，这样才能让他人感受到你在认真对待手中的事，在用脑子去做，而不是为了应付任务。 4 .下图是老师发飙后第二天，走路时想到的就记在了手机便签上。]]></content>
      <categories>
        <category>work</category>
      </categories>
      <tags>
        <tag>essay</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[看在出门前]]></title>
    <url>%2F2017-07-31%2F</url>
    <content type="text"><![CDATA[《人性的弱点》 今天我要让自己很开心。 林肯说过“多半的人都可以决定自己拥有更多的快乐。”快乐来源于内心，并非外来之物 今天我要调适自己。 而不是调整世界来配合我。我要让自己配合自己的家庭、事业与机遇。 今天我要照顾自己的身体。 我要运动，关心它、滋养它、不滥用它、不忽略它，让它变成我心灵的殿堂。 今天我要强化心灵。 我要学习，不让心灵闲置，阅读需要聚精会神才能读懂的读物。 今天我要让自己怡人。 使自己看起来愉悦，穿着合宜，轻声慢语，举止恰当，多予赞赏，少作批评，不找任何事的毛病，也不挑任何人的缺点。 今天要全心全意只活这一天，不去想自己的整个人生。 一天工作12小时固然很好，如果想到一辈子都要如此，可能会先吓坏自己。 今天我要制定计划。 计划今天要做的事。可能不能完全遵行，但还是要计划，为的是避免仓促及犹豫不决。 今天我要给自己保留一点轻松时间。 用这些时间思考，想想自己的人生远景。 今天我将无所畏惧，我不怕更快乐、更享受人生的美好；也不怕失去爱人，相信爱我的人亦爱我。]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>reading notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2016.1.5]]></title>
    <url>%2F2016-01-05%2F</url>
    <content type="text"><![CDATA[我喜欢那个能够按时按计划按想象去成为的我，也喜欢现在的这个自己。 生活本来就是个最具变量的东西，没有任何人可以确定自己的明天，明天想要的会不会跟今天一样，现在视若珍宝的，是否转眼就会弃如敝履。可是换取的，永远跟失去的一样多。而那些不曾预料的获得，比胸有成竹要更喜出望外。 我知道我终将成为更好的人。]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>essay</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[岁月神偷]]></title>
    <url>%2F2015-12-29%2F</url>
    <content type="text"><![CDATA[《岁月神偷》片中任达华饰演倔强又好强的父亲，无论是猛烈台风来临时，还是儿子被疾病缠身时，他总是倾尽所有来关爱亲人。最令人印象深刻的是任达华通过“手”的细节所展现的演技。父亲为了患癌儿子能输上新鲜血液，当掉戒指露出让人心疼的深深戒痕；为了在儿子墓前栽下可以遮荫的树，父亲的手被划伤满是触目惊心的血痕；而吴君如在戏里更是颠覆了大笑姑婆的本色，但是却保有了她的伶牙俐齿和积极的心态 一步难，一步佳，难一步，佳一步，难又一步，佳又一步，日子总要信，总要过。最好的时光里过着苦大于甜的生活。]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>movie</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何阅读一本书]]></title>
    <url>%2F2015-11-26%2F</url>
    <content type="text"><![CDATA[四个阶段： 基础阅读 （大部分已具备，就是一些识字能力） 检视阅读一 （即有系统的略读或粗读） 如何做： 1 .先看书名页 2 .研究目录页，基本框架做概括性理解 3 .有索引要检阅，有关键词目 4 .新书，可阅读出版社信息 5 .对目录有大概概括后挑选几个跟主题相关的章节来读 6 .打开书东翻下西翻下，随时寻找主要论点的讯号，最后两三页不要忽略 这是一种非常主动的阅读，一个人若不够灵活就没法集中精力来阅读，就没法进行检视阅读。跟随步骤做就不会读着读着就走神进入白日梦状态。 检视阅读二 （粗浅的阅读） 头一次面对难读得书时，从头到尾先读一遍，碰到不懂的不要停下来查询或思索。理解能理解的部分，不懂的可以略读跳过，在查询思考前至少保证已扫读过一遍。 这个规则同样适用于论说性作品。 检视阅读：快速阅读，应该要掌握什么时候用什么样的阅读速度去读，一本好书总有部分值得放慢阅读速度 想要读得好，一个主动、自我要求的读者，就得采取一些行动。任何一个超越基础阅读的层次其核心就是要努力提出问题（然后尽可能找出答案），这是绝不可或忘的原则。 如何做一个自我要求的读者 提出四个问题： 1 .这本书到底在说什么？ 2 .作者细部说了什么，怎么说的？ 3 .说得有道理吗？ 4 .这本书跟你有什么关系？若是得到了资讯，有什么意义？若还有启示，就必须找出更深的建议。 如何让一本书真正属于自己 做笔记不可缺，课可采用的方法： 1 .勾画 2 .在栏外再加注强调，或是某段太长，可在段外加记号 3 .记号强调重要的声明或段落即可 4 .编号，某个论点发出一串重要陈述 5 .参照标记，Cf，不同地方阐述同一点，有助于集中想法 6 .空白页对书的问题或答案记录，最后一页写作者的主要观点。]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>reading notes</tag>
      </tags>
  </entry>
</search>
